[{"path":"index.html","id":"acerca-de-este-curso","chapter":"Acerca de este curso","heading":"Acerca de este curso","text":"","code":""},{"path":"index.html","id":"requerimiento-y-competencias","chapter":"Acerca de este curso","heading":"Requerimiento y competencias","text":"se requieren conocimiento previos de programación para trabajar en este curso.\nEs solamente necesario un repaso de algunos conceptos de estadística y algo de experiencia en gestión de la cadena de suministrosEste curso tendrá más valor agragado para ti si en logar de utilizar los datos que te entregamos utilizas tus propios datos. Esto es lo que llamaremos datasets. En general tus datos pueden venir de hojas de cálculo, pero la tecnología que utilizaremos puede manejar volúnemes de información cientos o miles de veces más extensas que tu hoja de excel más grande que puedas concebir.","code":""},{"path":"index.html","id":"software-utilizado","chapter":"Acerca de este curso","heading":"Software utilizado","text":"Utilizaremos R-CranQue puedes bajarlo de este sitio:Descargar R-Cran\nHay versiones para Mac y LinuxComplementaremos el trabajo con R-Studio (hoy llamado Posit)Finalmente es conveniente (absolutamente necesario) que sepas trabajar en github.render example PDF bookdown::pdf_book, ’ll need install XeLaTeX. recommended install TinyTeX (includes XeLaTeX): https://yihui.org/tinytex/.","code":"\nbookdown::render_book()"},{"path":"index.html","id":"preview-book","chapter":"Acerca de este curso","heading":"Preview book","text":"work, may start local server live preview HTML book. preview update edit book save individual .Rmd files. can start server work session using RStudio add-“Preview book”, R console:","code":"\nbookdown::serve_book()"},{"path":"análisis-exploratorio.html","id":"análisis-exploratorio","chapter":"Capítulo 1 Análisis Exploratorio","heading":"Capítulo 1 Análisis Exploratorio","text":"","code":""},{"path":"análisis-exploratorio.html","id":"introducción","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.1 Introducción","text":"El objetivo de realizar un análisis exploratorio consiste en examinar los datos en base su distribución, características antes de realizar alguna técnica estadística.El objetivo principal es entender los datos y sus variables antes de acometer algún tipo de análisis más detallado.Detectar fallas en el diseño de los datos o en la recolección y aplicado datos univariantes o multivariantes.El proceso de obtención de información partir de los datos involucra una serie de procesos asociados.","code":""},{"path":"análisis-exploratorio.html","id":"premisas-industriales","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.2 Premisas Industriales","text":"Siempre que se aprende algo se avanza desde lo que conozco hacia lo que\ndesconozco. Sea que estés en una actividad industrial, social o\nacadémica, una de las premisas para aprender implica soltar lo que se\ntiene y ha tenido por años como cierto o verdadero para pasar trabajar\ncomo lo hacen los jueces y los abogados. daremos nada por cierto\nhasta que veamos datos y pruebas que lo demuestren.El análisis exploratorio de datos será la herramienta que utilizaremos\npara comprobar nuestras conjeturas e hipótesis, pero esto requiere\npasión por aprender. Ese avalente gesto de dar nada por cierto es lo\nque algunos filósofos como Descartes nos proponen bajo la consigna de\n“la duda metódica” como única vía para llegas la verdad (yo agregaría\n“sin morir en el intento”).En el terreno industrial existen muchos mitos o verdades reveladas. \nmodo de ejemplo citaría la mala interpretación que los economistas\nargentinos hicieron en la década del 80 del siglo XX respecto las\ntécnicas de manágement japonés y el paradigma de la calidad. Para un\neconomista de entonces las ideas puestas en práctica sobre “inventario\ncero” sonaban una cosa muy plausible, sobre todo la hora de pagar\nimpuestos sobre los activos. Créanme jamás vi tanta convulsión en las\ncadenas de suministros locales ni tantos quebrantos de empresas como en\nesa época.Como primer actividad realizaremos un análisis exploratorio de datos en\nR-Cran sin adentrarnos mucho en las estructuras y tipos de datos o sobre\ncomo importar datos de fuentes como SAP, hojas de cálculo e incluso la\nmisma web con herramientas como curl.Sin lugar dudas el recurso más preciado para cualquier sistema de\nmanufactura o producción de servicios es su recurso humano. Algunos\nautores en los últimos 10 años han cambiado esta palabra elevando la\nimportancia con el cambio de nombre de la gerencia que los administra\npasando de Gerentes de Recursos Humanos Gerencia de Gestión del\nTalento.Comenzaremos nuestro introducción al tema entonces con este tipo de\ninformación utilizadon un dataset que viene cargado en una de la\nbibliotecas (o paquetes) de R.","code":""},{"path":"análisis-exploratorio.html","id":"kdd","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.3 KDD","text":"Knowledge Discovery Databases es un proceso orientado la identificación de patrones y al descubrimientos de patrones nuevos mas entendibles.Knowledge Discovery implica la evaluación e interpretación de patrones y modelos para tomar decisiones con respecto lo que constituye conocimiento y lo que lo es. Por lo tanto, el KDD requiere de un amplio y profundo conocimiento sobre tu área de estudio.(Gentileza de MESRK Containers Statistics)KDD requiere un mayor conocimiento acerca del área de estudio que el Data Mining.","code":""},{"path":"análisis-exploratorio.html","id":"comprension-del-dominio-de-estudio","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.4 Comprension del dominio de estudio","text":"Como en cualquier tipo de investigación, es fundamental tener muy claros los límites y objetivos de lo que pretendemos. Es muy fácil perder el rumbo en el océano infinito de datos nuestra disposición.Desarrollo de un entendimiento sobre el dominio\nDescubrimiento de conocimiento previo que sea relevante\nDefinición del objetivo del KDD\nEn este paso es cuando reconocemos las fuentes de información más importantes y quienes tienen control sobre ellas. También es relevante incluir toda la metadata relacionada, dimensionar la cantidad de datos, y formatos.Se recomienda que toda la información más importante que se encuentre solamente en medios físicos sea digitalizada, previo iniciar las actividades de KDD.El KDD se refiere al proceso general de descubrir conocimientos útiles partir de datos, y la minería de datos se refiere un paso particular en este proceso. La minería de datos es la aplicación de algoritmos específicos para extraer patrones partir de los datos. La distinción entre el proceso KDD y el paso de minería de datos (dentro del proceso) es un punto central de este artículo. Los pasos adicionales en el proceso KDD, tales como preparación de datos, selección de datos, limpieza de datos, incorporación de conocimiento previo apropiado e interpretación apropiada de los resultados de la minería, son esenciales para asegurar que el conocimiento útil se derive de los datos. La aplicación ciega de métodos de minería de datos (correctamente criticados como dragado de datos en la literatura estadística) puede ser una actividad peligrosa, que fácilmente conduce al descubrimiento de patrones sin sentido e inválidos.KDD ha evolucionado y sigue evolucionando desde la intersección de campos de investigación como el aprendizaje de máquinas, reconocimiento de patrones, bases de datos, estadísticas, IA, adquisición de conocimientos para sistemas expertos, visualización de datos y computación de alto rendimiento. La meta unificadora es extraer conocimiento de alto nivel desde datos de bajo nivel en el contexto de grandes conjuntos de datos.El proceso KDD puede ser visto como una actividad multidisciplinaria que abarca técnicas fuera del alcance de cualquier disciplina en particular, como el aprendizaje automático o machine learning. En este contexto, existen oportunidades claras para que otros campos de IA (aparte del aprendizaje automático) contribuyan al KDD. El KDD pone un énfasis especial en encontrar patrones comprensibles que se pueden interpretar como conocimiento útil o interesante. Así, por ejemplo, las redes neuronales, aunque son una potente herramienta de modelado, son relativamente difíciles de entender en comparación con los árboles de decisión. El KDD también hace hincapié en las propiedades de escalado y robustez de los algoritmos de modelado para grandes conjuntos de datos ruidosos.","code":""},{"path":"análisis-exploratorio.html","id":"limpieza-y-procesamiento-de-datos","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.5 Limpieza y procesamiento de datos","text":"Los datasets disponibles en la actualidad usualmente están incompletos (valores de atributos faltantes), tienen ruido (errores y datos aislados o outliers), o presentan inconsistencias (discrepancias en los datos recolectados).Eliminación de ruido y datos aislados o outliers.\nUso del conocimiento previo para eliminar las inconsistencias y los duplicados.\nSelección y uso de estrategias para manejar la información faltante en los datasets.\nEstos “datos sucios” pueden confundir el proceso de minería y conducir resultados inválidos o poco confiables.El preprocesamiento y la limpieza tienen el objetivo de mejorar la calidad de los datos y los resultados de la minería. Recuerda que la implementación de análisis complejos y el minado de grandes cantidades de datos puede tomar mucho tiempo, así que lo que podamos hacer para acortar ese tiempo será siempre de provecho.","code":""},{"path":"análisis-exploratorio.html","id":"minería-de-datos","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.6 Minería de datos","text":"La minería es una exploración. Nos adentramos en la inmensidad de los datos y descubrimos poco poco los patrones o modelos presentes en ellos; las relaciones.Y en esta exploración, una de nuestras herramientas más útiles son los algoritmos.¿Qué es un algoritmo? Básicamente, un algoritmo es una serie de instrucciones o reglas establecidas en un programa informático que nos permiten llegar un resultado o solución.En el caso de la minería de datos, un algoritmo nos permite procesar un set de datos para obtener nueva información sobre ese mismo dataset.En general, la minería de datos comprende tres pasos: la selección de la tarea, la selección del algoritmo (o algoritmos) y su uso.El algoritmo busca patrones y modelos que nos interesen, siguiendo sus reglas preestablecidas, que pueden incluir árboles de clasificación, modelos de regresión, clusters, modelos mixtos, entre otros.La mayoría de los métodos de minería de datos se basan en técnicas comprobadas de aprendizaje automático o machine learning, reconocimiento de patrones, y estadísticas: clasificación, agrupación, regresión, etc. La formación de diferentes algoritmos bajo cada uno de estos encabezados menudo puede ser desconcertante para el analista de datos novato y también para el experto.Debe hacerse hincapié en que, de los muchos métodos de minería de datos anunciados en la literatura, en realidad sólo hay unas pocas técnicas fundamentales.","code":""},{"path":"análisis-exploratorio.html","id":"interpretación-de-patrones-minados","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.7 Interpretación de patrones minados","text":"Es importante que comprendamos la diferencia entre dos términos clave: patrones y modelos.Patrones: son estructuras locales que hacen declaraciones sólo sobre un espacio restringido por variables. Esto tiene aplicaciones importantes en detección de anomalías como la detección de faltas en procesos industriales o de fraudes en el sistema bancario.\nModelos: son estructuras globales que hacen declaraciones sobre cualquier punto en el espacio de medición. Por ejemplo, los modelos pueden predecir el valor de alguna otra variable.\nEn la etapa de interpretación, hallamos los patrones y modelos en los datos analizados.Los resultados deben presentarse en un formato entendible. Por esta razón las técnicas de visualización son importantes para que los resultados sean útiles, dado que los modelos matemáticos o descripciones en formato de texto pueden ser difíciles de interpretar para los usuarios finales.Desde este punto del proceso es posible regresar cualquiera de los pasos anteriores.","code":""},{"path":"análisis-exploratorio.html","id":"imputación-de-datos","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.8 Imputación de datos","text":"La falta de datos en el proceso de captura puede deberse diversas situaciones que van desde:fatiga del informanterechazo informarcalidad de la muestraformación del encuestadorLos valores NA (available) pueden ser comparados ni se puede realizar operaciones sobre ellos.","code":""},{"path":"análisis-exploratorio.html","id":"detección-en-r","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.9 Detección en R","text":"La elección de datos ausente se debe verificar previo la realización de ciertas calculas para entender su resultado, que puede verse afectados por la aparición de datos nulos.Uno de los aspectos más interesantes de R-Cran es la facilidad que tienen para generar datasets en forma sensilla, lo que lo hace especialmente atractivo para desarrollar y entrenar modelos de simulaciónVeamos un ejemplo de generación de un dataset fincticio llamado “valores y comencemos ha hacer el análisis primero en forma manual.Devuelve TRUE si el objeto contiene un valor Nulo o ausente. Se aplica cada elemento del objeto.Se utiliza para detener la ejecución de u scrip o programa si el objeto contiene algún valor ausente.Devolverá TRUE si existe al menos un valor ausente dentro de objeto.","code":"\n# Crear un set de datos de 50 datos ficticios con 5 valores con NA.\nvalores <- as.integer(runif(50,1,10))\nindices <- as.integer(runif(5,1,50))\nvalores[indices] <- NA\nvalores\n#>  [1]  7  9  5  4  1  8  9  8  7  5 NA  5  8  2  9  7  2  4\n#> [19]  7  8 NA  5  5 NA  2  6  2  5  4  7  9  4  9  2  8  3\n#> [37]  3  5  5  9  9 NA  4  9  8  8  8  3  2  9\nis.na(valores)\n#>  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#> [10] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#> [19] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE\n#> [28] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#> [37] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n#> [46] FALSE FALSE FALSE FALSE FALSE  na.fail(objeto)any(is.na(objeto))\nany(is.na(valores))\n#> [1] TRUE"},{"path":"análisis-exploratorio.html","id":"eliminar-datos-ausentes","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.9.1 Eliminar datos ausentes","text":"La eliminación de datos ausentes es una medida que de realizarse en forma previa cualquier análisis posterior del conjunto de datos.Elimina del objeto cualquier dato ausente existenteSe utiliza en dataframes y verifica que ninguna de las columnas de cada fila contenga valores NA. Devuelve un vector, con valor de TRUE en las filas sin valores ausente (completas) y FALSE en el resto. este vector puede utilizarse para seleccionar filas de interés.Existen una cantidad considerable de dataset de prueba en R-Cran que puedes utilizar para dar tus primeros pasos (e incluso en experimentos tan básicos). Todos ellos aparecen en la bibliografía de estadística o tienen papers que explicna las técnicas de recolección. Es recomendable espolrar las fuentes que aparecen en las ayuda para que en caso de utilizarlo en tus artículos científicos puedas citar correctamente.Para poder verlos tipea en la consolaRecuerda que habitualmente cada biblioteca que cargas con install.packages() agrega otros datasets esta colección\nSi quieres ver todos los datasets tipea:Usaremos el dataset de Calidad de Aire de la Ciudad de New Yorkpara visualizar el dataset tipeaTambién es posible ver esto más claramente con strAlgunas funciones de R pueden funcionar con valores NA, ingresando un parámetro adicional de tipo na.rm o action","code":"na.omit(objeto)\nna.omit(valores)\n#>  [1] 7 9 5 4 1 8 9 8 7 5 5 8 2 9 7 2 4 7 8 5 5 2 6 2 5 4 7 9\n#> [29] 4 9 2 8 3 3 5 5 9 9 4 9 8 8 8 3 2 9\n#> attr(,\"na.action\")\n#> [1] 11 21 24 42\n#> attr(,\"class\")\n#> [1] \"omit\"complete.cases(objeto)\ncomplete.cases(valores)\n#>  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n#> [10]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n#> [19]  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n#> [28]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n#> [37]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n#> [46]  TRUE  TRUE  TRUE  TRUE  TRUEdata()  data(package = .packages(all.available = TRUE)\nairquality\n#>     Ozone Solar.R Wind Temp Month Day\n#> 1      41     190  7.4   67     5   1\n#> 2      36     118  8.0   72     5   2\n#> 3      12     149 12.6   74     5   3\n#> 4      18     313 11.5   62     5   4\n#> 5      NA      NA 14.3   56     5   5\n#> 6      28      NA 14.9   66     5   6\n#> 7      23     299  8.6   65     5   7\n#> 8      19      99 13.8   59     5   8\n#> 9       8      19 20.1   61     5   9\n#> 10     NA     194  8.6   69     5  10\n#> 11      7      NA  6.9   74     5  11\n#> 12     16     256  9.7   69     5  12\n#> 13     11     290  9.2   66     5  13\n#> 14     14     274 10.9   68     5  14\n#> 15     18      65 13.2   58     5  15\n#> 16     14     334 11.5   64     5  16\n#> 17     34     307 12.0   66     5  17\n#> 18      6      78 18.4   57     5  18\n#> 19     30     322 11.5   68     5  19\n#> 20     11      44  9.7   62     5  20\n#> 21      1       8  9.7   59     5  21\n#> 22     11     320 16.6   73     5  22\n#> 23      4      25  9.7   61     5  23\n#> 24     32      92 12.0   61     5  24\n#> 25     NA      66 16.6   57     5  25\n#> 26     NA     266 14.9   58     5  26\n#> 27     NA      NA  8.0   57     5  27\n#> 28     23      13 12.0   67     5  28\n#> 29     45     252 14.9   81     5  29\n#> 30    115     223  5.7   79     5  30\n#> 31     37     279  7.4   76     5  31\n#> 32     NA     286  8.6   78     6   1\n#> 33     NA     287  9.7   74     6   2\n#> 34     NA     242 16.1   67     6   3\n#> 35     NA     186  9.2   84     6   4\n#> 36     NA     220  8.6   85     6   5\n#> 37     NA     264 14.3   79     6   6\n#> 38     29     127  9.7   82     6   7\n#> 39     NA     273  6.9   87     6   8\n#> 40     71     291 13.8   90     6   9\n#> 41     39     323 11.5   87     6  10\n#> 42     NA     259 10.9   93     6  11\n#> 43     NA     250  9.2   92     6  12\n#> 44     23     148  8.0   82     6  13\n#> 45     NA     332 13.8   80     6  14\n#> 46     NA     322 11.5   79     6  15\n#> 47     21     191 14.9   77     6  16\n#> 48     37     284 20.7   72     6  17\n#> 49     20      37  9.2   65     6  18\n#> 50     12     120 11.5   73     6  19\n#> 51     13     137 10.3   76     6  20\n#> 52     NA     150  6.3   77     6  21\n#> 53     NA      59  1.7   76     6  22\n#> 54     NA      91  4.6   76     6  23\n#> 55     NA     250  6.3   76     6  24\n#> 56     NA     135  8.0   75     6  25\n#> 57     NA     127  8.0   78     6  26\n#> 58     NA      47 10.3   73     6  27\n#> 59     NA      98 11.5   80     6  28\n#> 60     NA      31 14.9   77     6  29\n#> 61     NA     138  8.0   83     6  30\n#> 62    135     269  4.1   84     7   1\n#> 63     49     248  9.2   85     7   2\n#> 64     32     236  9.2   81     7   3\n#> 65     NA     101 10.9   84     7   4\n#> 66     64     175  4.6   83     7   5\n#> 67     40     314 10.9   83     7   6\n#> 68     77     276  5.1   88     7   7\n#> 69     97     267  6.3   92     7   8\n#> 70     97     272  5.7   92     7   9\n#> 71     85     175  7.4   89     7  10\n#> 72     NA     139  8.6   82     7  11\n#> 73     10     264 14.3   73     7  12\n#> 74     27     175 14.9   81     7  13\n#> 75     NA     291 14.9   91     7  14\n#> 76      7      48 14.3   80     7  15\n#> 77     48     260  6.9   81     7  16\n#> 78     35     274 10.3   82     7  17\n#> 79     61     285  6.3   84     7  18\n#> 80     79     187  5.1   87     7  19\n#> 81     63     220 11.5   85     7  20\n#> 82     16       7  6.9   74     7  21\n#> 83     NA     258  9.7   81     7  22\n#> 84     NA     295 11.5   82     7  23\n#> 85     80     294  8.6   86     7  24\n#> 86    108     223  8.0   85     7  25\n#> 87     20      81  8.6   82     7  26\n#> 88     52      82 12.0   86     7  27\n#> 89     82     213  7.4   88     7  28\n#> 90     50     275  7.4   86     7  29\n#> 91     64     253  7.4   83     7  30\n#> 92     59     254  9.2   81     7  31\n#> 93     39      83  6.9   81     8   1\n#> 94      9      24 13.8   81     8   2\n#> 95     16      77  7.4   82     8   3\n#> 96     78      NA  6.9   86     8   4\n#> 97     35      NA  7.4   85     8   5\n#> 98     66      NA  4.6   87     8   6\n#> 99    122     255  4.0   89     8   7\n#> 100    89     229 10.3   90     8   8\n#> 101   110     207  8.0   90     8   9\n#> 102    NA     222  8.6   92     8  10\n#> 103    NA     137 11.5   86     8  11\n#> 104    44     192 11.5   86     8  12\n#> 105    28     273 11.5   82     8  13\n#> 106    65     157  9.7   80     8  14\n#> 107    NA      64 11.5   79     8  15\n#> 108    22      71 10.3   77     8  16\n#> 109    59      51  6.3   79     8  17\n#> 110    23     115  7.4   76     8  18\n#> 111    31     244 10.9   78     8  19\n#> 112    44     190 10.3   78     8  20\n#> 113    21     259 15.5   77     8  21\n#> 114     9      36 14.3   72     8  22\n#> 115    NA     255 12.6   75     8  23\n#> 116    45     212  9.7   79     8  24\n#> 117   168     238  3.4   81     8  25\n#> 118    73     215  8.0   86     8  26\n#> 119    NA     153  5.7   88     8  27\n#> 120    76     203  9.7   97     8  28\n#> 121   118     225  2.3   94     8  29\n#> 122    84     237  6.3   96     8  30\n#> 123    85     188  6.3   94     8  31\n#> 124    96     167  6.9   91     9   1\n#> 125    78     197  5.1   92     9   2\n#> 126    73     183  2.8   93     9   3\n#> 127    91     189  4.6   93     9   4\n#> 128    47      95  7.4   87     9   5\n#> 129    32      92 15.5   84     9   6\n#> 130    20     252 10.9   80     9   7\n#> 131    23     220 10.3   78     9   8\n#> 132    21     230 10.9   75     9   9\n#> 133    24     259  9.7   73     9  10\n#> 134    44     236 14.9   81     9  11\n#> 135    21     259 15.5   76     9  12\n#> 136    28     238  6.3   77     9  13\n#> 137     9      24 10.9   71     9  14\n#> 138    13     112 11.5   71     9  15\n#> 139    46     237  6.9   78     9  16\n#> 140    18     224 13.8   67     9  17\n#> 141    13      27 10.3   76     9  18\n#> 142    24     238 10.3   68     9  19\n#> 143    16     201  8.0   82     9  20\n#> 144    13     238 12.6   64     9  21\n#> 145    23      14  9.2   71     9  22\n#> 146    36     139 10.3   81     9  23\n#> 147     7      49 10.3   69     9  24\n#> 148    14      20 16.6   63     9  25\n#> 149    30     193  6.9   70     9  26\n#> 150    NA     145 13.2   77     9  27\n#> 151    14     191 14.3   75     9  28\n#> 152    18     131  8.0   76     9  29\n#> 153    20     223 11.5   68     9  30\nstr(airquality)\n#> 'data.frame':    153 obs. of  6 variables:\n#>  $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n#>  $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n#>  $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n#>  $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n#>  $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n#>  $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...  total de filas completas, sin valores nulos o ausentes\nnrow(na.omit(airquality))\n#> [1] 111  total de filas completas, sin valores nulos o ausentes\nnrow(airquality[complete.cases(airquality),])\n#> [1] 111\npromedio <- mean(valores, na.rm=TRUE)\npromedio\n#> [1] 5.826087"},{"path":"análisis-exploratorio.html","id":"uso-de-na.action-com-parametro-de-function-modelo-lineal","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.9.2 Uso de na.action com parametro de function modelo lineal","text":"","code":"\nlm(Solar.R ~ Temp, airquality, na.action=na.omit)\n#> \n#> Call:\n#> lm(formula = Solar.R ~ Temp, data = airquality, na.action = na.omit)\n#> \n#> Coefficients:\n#> (Intercept)         Temp  \n#>     -24.431        2.693"},{"path":"análisis-exploratorio.html","id":"limpieza-de-datos","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.10 Limpieza de datos","text":"Los valores ausentes son relativamente comunes en los set de datos y se deben muchas causas: fallas en la captura de datos o en la transcripción de datos.","code":""},{"path":"análisis-exploratorio.html","id":"missing-completely-at-random-mcar","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.11 Missing Completely at Random (MCAR)","text":"Los valores perdidos se relacionan con las variables en el set de datos. Esta función elimina las filas que tengan 100% de datos perdidos.Como dice, los valores faltan aleatoriamente de su conjunto de datos. Los valores de los datos que faltan se relacionan con ningún otro dato del conjunto de datos y existe un patrón para los valores reales de los datos que faltan.Por ejemplo, cuando el estatus de fumador se registra en un subconjunto aleatorio de pacientes. Esto es fácil de manejar, pero desafortunadamente, los datos casi nunca faltan completamente al azar.","code":""},{"path":"análisis-exploratorio.html","id":"missing-at-random-mar","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.12 Missing at Random (MAR)","text":"Esto es confuso y sería mejor decir que falta condicionalmente al azar. Aquí, los datos que faltan tienen una relación con otras variables en el set de datos. Sin embargo, los valores reales que faltan son aleatorios.Por ejemplo, el estatus de fumador está documentado en pacientes mujeres porque el médico era demasiado tímido para preguntar.","code":""},{"path":"análisis-exploratorio.html","id":"not-missing-at-random-nmar","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.13 Not Missing at Random (NMAR)","text":"El patrón de falta de datos está relacionado con otras variables del conjunto de datos, pero además, los valores de los datos que faltan son aleatorios.Por ejemplo, cuando se registra el estado de tabaquismo en los pacientes ingresados como una emergencia, que también son más propensos tener peores resultados de la cirugía.La falta de datos aleatorios es importante, puede alterar sus conclusiones y es la más difícil de diagnosticar y manejar. Sólo pueden detectarse recogiendo y examinando algunos de los datos que faltan. Esto es menudo difícil o imposible de hacer.La forma de tratar los datos que faltan depende del tipo de falta.","code":""},{"path":"análisis-exploratorio.html","id":"análisis-univariado","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.14 Análisis Univariado","text":"","code":""},{"path":"análisis-exploratorio.html","id":"qué-es-un-análisis-exploratorio","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.14.1 Qué es un análisis exploratorio","text":"Un análisis exploratorio de datos, o AED (en inglés, EDA -\nExploratory Data Analysis), es una fase inicial y fundamental en\ncualquier proceso de analítica de datos o proyecto de ciencia de datos.\nSu propósito es generar conclusiones definitivas, sino comprender\nla estructura, las relaciones y las anomalías de los datos antes de\naplicar modelos estadísticos o de aprendizaje automático más complejos.Piense en el AED como la fase de “investigación detectivesca” de\nsus datos. Es el momento de hacerse preguntas como:¿Cómo se distribuyen mis variables?¿Hay valores atípicos (outliers) que podrían sesgar mis resultados?¿Hay valores faltantes y, de ser así, cómo los manejo?¿Existen correlaciones o patrones interesantes entre las variables?","code":""},{"path":"análisis-exploratorio.html","id":"por-qué-es-crucial","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.14.2 ¿Por qué es crucial?","text":"El AED es vital porque le permite identificar problemas y\noportunidades que, de otra forma, podrían pasar desapercibidos. Por\nejemplo, si un valor atípico representa un error de entrada de datos, \ncorregirlo podría llevar que su modelo predictivo sea completamente\ninútil.En un contexto de posgrado, solo esperamos que se sepa cómo realizar\nun AED, sino que entienda el razonamiento detrás de cada paso. Un\nAED bien ejecutado puede guiar la selección de la metodología analítica\nmás apropiada y, en última instancia, mejorar la calidad y la\nfiabilidad de sus resultados.","code":""},{"path":"análisis-exploratorio.html","id":"técnicas-comunes-en-el-aed","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.15 Técnicas comunes en el AED","text":"Para llevar cabo un AED, se utilizan diversas técnicas, que se pueden\nclasificar en dos grandes grupos:Paciencia y relfexión: AED es preparen, fuego, apunten; sino\ntodo lo contrario. Es importante que se sepa que este análisis\nprobablemente te de respuestas. En su lugar te conforntará con\nnuevas preguntas que te llevarán la causa raíz del problema.Paciencia y relfexión: AED es preparen, fuego, apunten; sino\ntodo lo contrario. Es importante que se sepa que este análisis\nprobablemente te de respuestas. En su lugar te conforntará con\nnuevas preguntas que te llevarán la causa raíz del problema.Técnicas Visuales: Se utilizan gráficos para inspeccionar los\ndatos. Algunos ejemplos incluyen:Técnicas Visuales: Se utilizan gráficos para inspeccionar los\ndatos. Algunos ejemplos incluyen:Histogramas: Para ver la distribución de una variable numérica.Histogramas: Para ver la distribución de una variable numérica.Diagramas de caja (Box plots): Para identificar la dispersión,\nla mediana y los posibles valores atípicos.Diagramas de caja (Box plots): Para identificar la dispersión,\nla mediana y los posibles valores atípicos.Diagramas de dispersión (Scatter plots): Para visualizar la\nrelación entre dos variables.Diagramas de dispersión (Scatter plots): Para visualizar la\nrelación entre dos variables.Mapas de calor (Heatmaps): Para visualizar correlaciones entre\nmúltiples variables.Mapas de calor (Heatmaps): Para visualizar correlaciones entre\nmúltiples variables.Técnicas Numéricas/Estadísticas: Se utilizan resúmenes numéricos\npara describir los datos. Esto incluye:Técnicas Numéricas/Estadísticas: Se utilizan resúmenes numéricos\npara describir los datos. Esto incluye:Estadísticas descriptivas: Calcular la media, mediana, moda,\ndesviación estándar y rango.Estadísticas descriptivas: Calcular la media, mediana, moda,\ndesviación estándar y rango.Matrices de correlación: Para cuantificar la fuerza de la\nrelación entre variables.Matrices de correlación: Para cuantificar la fuerza de la\nrelación entre variables.Tablas de frecuencia: Para contar la ocurrencia de cada\ncategoría en una variable cualitativa.Tablas de frecuencia: Para contar la ocurrencia de cada\ncategoría en una variable cualitativa.En resumen, el AED es la primera conversación que tiene con sus\ndatos. Es una etapa iterativa y flexible que le permite entender fondo\nlo que tiene entre manos, formular hipótesis y preparar el\ncamino para el análisis formal que vendrá después. Es la base de un\ntrabajo de investigación o de un proyecto de ciencia de datos serio y\nrobusto.","code":""},{"path":"análisis-exploratorio.html","id":"insatalar-paquetes-y-datasets","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.16 Insatalar Paquetes y Datasets","text":"Es lo rpimero que se tiene que hacer, contar con un dataset cargado en\nel memoria o área de trabajo de R. Lo más importante es conocer y\nrevalidar los datos.Uno de lo paquetes que podemos utilizar es AER , que tienen cientos de\ndatasets muy conocidos en conometría, algunos de ellos preferidos como\nconsutores en economía industrial tales como.1Recuerda que antes de poder utilizar un paquete en R, debes descargarlo\nutilizando:Biblioteca AER2","code":"-> Tools -> install.packeges y elegir en el cuadro de diálogo.\nlibrary(AER)\n#> Loading required package: car\n#> Loading required package: carData\n#> Loading required package: lmtest\n#> Loading required package: zoo\n#> \n#> Attaching package: 'zoo'\n#> The following objects are masked from 'package:base':\n#> \n#>     as.Date, as.Date.numeric\n#> Loading required package: sandwich\n#> Loading required package: survival"},{"path":"análisis-exploratorio.html","id":"listado-de-datasets","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.16.1 Listado de Datasets","text":"Para visualizar los datasets que tiene incluidos AER usamos el\nsiguiente código.verás el detalle en RMarkdownVamos uilizar CPS1985 son parte de AER y haremos algunos pasos\npara explorarlos.Observa que hay una tabla igual pero con los datos de 1988","code":"\ndata(package=\"AER\")nota las comillas dobles en el nombre del paquete. verás la salida en la consola de RStudio"},{"path":"análisis-exploratorio.html","id":"visualización-de-las-estructura-del-dataset","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.16.2 Visualización de las estructura del dataset","text":"Para poder utilizarlo debemos invocar al datset y podrás ver parte de su\nmínima expresión de datos en la ventana superior derecha de R-Studio.\nUtilizarEsto crea una variable que se llama igual que el datset que tiene 534\ninstancias o casos observados en una estructura de 11 columnas.Para explorar la cabecera de datos utilizarPara ver los datos finales de la tabla utilizar:","code":"\ndata(CPS1985)\nhead(CPS1985)\n#>       wage education experience age ethnicity region gender\n#> 1     5.10         8         21  35  hispanic  other female\n#> 1100  4.95         9         42  57      cauc  other female\n#> 2     6.67        12          1  19      cauc  other   male\n#> 3     4.00        12          4  22      cauc  other   male\n#> 4     7.50        12         17  35      cauc  other   male\n#> 5    13.07        13          9  28      cauc  other   male\n#>      occupation        sector union married\n#> 1        worker manufacturing    no     yes\n#> 1100     worker manufacturing    no     yes\n#> 2        worker manufacturing    no      no\n#> 3        worker         other    no      no\n#> 4        worker         other    no     yes\n#> 5        worker         other   yes      no\ntail(CPS1985)\n#>      wage education experience age ethnicity region gender\n#> 528 11.79        16          6  28      cauc  other female\n#> 529 11.36        18          5  29      cauc  other   male\n#> 530  6.10        12         33  51     other  other female\n#> 531 23.25        17         25  48     other  other female\n#> 532 19.88        12         13  31      cauc  south   male\n#> 533 15.38        16         33  55      cauc  other   male\n#>     occupation        sector union married\n#> 528  technical         other   yes      no\n#> 529  technical         other    no      no\n#> 530  technical         other    no     yes\n#> 531  technical         other   yes     yes\n#> 532  technical         other   yes     yes\n#> 533  technical manufacturing    no     yes"},{"path":"análisis-exploratorio.html","id":"exploración-de-la-clase","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.16.3 Exploración de la clase","text":"Como R es un lenguaje orientado objetos, opera con el concepto de\nclases. Si conoces que es esto, simplemene observa la información que\nobtienes del comando str.Ahora nos queda más claro que los datos que nos ofrecía data() habla\nsobre:Sobre una columna llamada Salarios (WAGES) y como podemos explicarla en\nfunción de los datos vinculados en otras columna.la columna Wages se la llama columnas de etiquetas del dataset y es\nvital para hacer más tarde aprendizaje supervisado de máquina machine\nlearning.Vemos también que el dataset tiene columnas que son de tipo numérico\ny alfabético, pero existen además otras que se denominan de tipo\ncategórico con instancias bien explicadas.Existen además otros tipos tales como binarias y otras que veremos\nmás adelante.","code":"\nstr(CPS1985)\n#> 'data.frame':    534 obs. of  11 variables:\n#>  $ wage      : num  5.1 4.95 6.67 4 7.5 ...\n#>  $ education : num  8 9 12 12 12 13 10 12 16 12 ...\n#>  $ experience: num  21 42 1 4 17 9 27 9 11 9 ...\n#>  $ age       : num  35 57 19 22 35 28 43 27 33 27 ...\n#>  $ ethnicity : Factor w/ 3 levels \"cauc\",\"hispanic\",..: 2 1 1 1 1 1 1 1 1 1 ...\n#>  $ region    : Factor w/ 2 levels \"south\",\"other\": 2 2 2 2 2 2 1 2 2 2 ...\n#>  $ gender    : Factor w/ 2 levels \"male\",\"female\": 2 2 1 1 1 1 1 1 1 1 ...\n#>  $ occupation: Factor w/ 6 levels \"worker\",\"technical\",..: 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ sector    : Factor w/ 3 levels \"manufacturing\",..: 1 1 1 3 3 3 3 3 1 3 ...\n#>  $ union     : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 2 1 1 1 1 ...\n#>  $ married   : Factor w/ 2 levels \"no\",\"yes\": 2 2 1 1 2 1 1 1 2 1 ...CPS1985  Determinants of Wages Data (CPS 1985)"},{"path":"análisis-exploratorio.html","id":"acceso-a-datos-individuales-filas-y-columnas","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.17 Acceso a datos individuales, filas y columnas","text":"Veamos la columna de educaciónExaminemos la claseLas sintaxis nombre de tabla $ nombre de columna nos muestra\nlos valores","code":"\nclass(CPS1985$education)\n#> [1] \"numeric\"\nCPS1985$education\n#>   [1]  8  9 12 12 12 13 10 12 16 12 12 12  8  9  9 12  7 12\n#>  [19] 11 12 12  7 12 11 12  6 12 10 12 12 14 12  8 17 12 12\n#>  [37] 12 12 12 14 12 14 12  9 13  7 16 10  8 12 12 16 12 12\n#>  [55] 13 12 13 10 12 12 11 12  3 12 12 10 12 12 12 10 11 14\n#>  [73] 10  8  8  6 11 12 12 12 14 12 10 16 13 12 11 12 12 11\n#>  [91] 12 12 12 12 12 12 15 12 12 13 11 10 12 12 14 12 14  5\n#> [109] 12  8 13 12 12 12 11 16 11 12  8 12 12 12 12 12 12 12\n#> [127]  9 13 12 12 12 12 12 14  9 10 12  7  9 12 12 12 11 11\n#> [145] 10 12 12 12 12 13  6 14 12 14 12 12 16 14 14 12 12 13\n#> [163] 17 12 14 16 16 15 17 12 14 12 16 12 14 16 18 15 12 12\n#> [181] 18 16 18 16 16 16 17 12 14 18 12 12 12 18 10 16 16 16\n#> [199] 13 12 14 16 17 12 16 12 15 13 14 16 18 13 12 18 14 14\n#> [217] 12 12 12 16 12 11 12 14 12 16 16 11 13 12 14 14 12 12\n#> [235] 13 14 14 12 14 12 12 12 14 12 12 15 12 13 17 14 13 16\n#> [253] 12 12 16 16 13 14 18 14 12 12 12 12 12 11 16 12 12 12\n#> [271] 12 18 11 12 16 13 16 13 12 12 12 12 12 13 14 14 12 12\n#> [289] 17 16 12 12 13 12 13 12 12 14 12 12 12 12 12 13 12  9\n#> [307] 13 12 13 12 13 12 13 16 12 12 12 12 12 12 12 17 14 12\n#> [325] 14 15 12 12 11 12 12 16 13 12 12 12 12 12 12 12 12 12\n#> [343] 12 17 14 12  4 14  8 15  2  8 11 10  8  9 12  8 14 12\n#> [361] 12 16 13 16 14 12 11 13 12 10  8 12 12 12 12 12 12 12\n#> [379] 12 13 14 12 12 11 11 12 12 12 10 12 12 12 14 14 12 13\n#> [397] 12 13 12 12 10 12 14 11 11  8 16 10 16 14 14 11 12  9\n#> [415] 11 13 14 12 14 12 12 12 12 11 12  8  9  7 11 13 18 17\n#> [433] 16 14 12 18 18 17 13 16 14 15 18 16 16 18 16 17 16 17\n#> [451] 15 18 17 16 18 18 14 16 18 18 16 12 16 16 16 12 12 12\n#> [469] 18 12 16 15 18 16 18 16 12 18 12 16 14 18 16 18 17 18\n#> [487] 14 14 16 16 14 17 16 16 16 15 18 17 16 18 17 12 17 12\n#> [505] 17 16 16 18 16 16 17 16 18 12 16 14 16 12 14 16 17 16\n#> [523] 16 17  9 15 15 12 16 18 12 17 12 16"},{"path":"análisis-exploratorio.html","id":"datos-ordenados","chapter":"Capítulo 1 Análisis Exploratorio","heading":"1.17.1 Datos ordenados","text":"","code":"\nsort(CPS1985$education)\n#>   [1]  2  3  4  5  6  6  6  7  7  7  7  7  8  8  8  8  8  8\n#>  [19]  8  8  8  8  8  8  8  8  8  9  9  9  9  9  9  9  9  9\n#>  [37]  9  9  9 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n#>  [55] 10 10 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n#>  [73] 11 11 11 11 11 11 11 11 11 11 11 12 12 12 12 12 12 12\n#>  [91] 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n#> [109] 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n#> [127] 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n#> [145] 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n#> [163] 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n#> [181] 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n#> [199] 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n#> [217] 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n#> [235] 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n#> [253] 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n#> [271] 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n#> [289] 12 12 12 12 12 12 12 12 12 12 12 12 12 12 13 13 13 13\n#> [307] 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13\n#> [325] 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 14 14 14\n#> [343] 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n#> [361] 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n#> [379] 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 15\n#> [397] 15 15 15 15 15 15 15 15 15 15 15 15 16 16 16 16 16 16\n#> [415] 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n#> [433] 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n#> [451] 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n#> [469] 16 16 16 16 16 16 16 16 16 16 16 17 17 17 17 17 17 17\n#> [487] 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 18\n#> [505] 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18\n#> [523] 18 18 18 18 18 18 18 18 18 18 18 18"},{"path":"identificación-de-valores-mínimos-y-máximos-conteos-query.html","id":"identificación-de-valores-mínimos-y-máximos-conteos-query","chapter":"Capítulo 2 Identificación de valores mínimos y máximos, conteos (QUERY)","heading":"Capítulo 2 Identificación de valores mínimos y máximos, conteos (QUERY)","text":"MínimosDebes entender que esto nos dice que el valor que está en la fila\nindicada tiene el primer valor mínimoPara ver qué valor es el mínimo ejecutamosMáximosDebes entender que esto nos dice que el valor que está en la fila\nindicada tiene el primer valor mínimoPara ver qué valor es el mínimo ejecutamos","code":"\nwhich.min(CPS1985$education)\n#> [1] 351\nCPS1985$education[351]\n#> [1] 2\nwhich.max(CPS1985$education)\n#> [1] 177\nCPS1985$education[177]\n#> [1] 18"},{"path":"identificación-de-valores-mínimos-y-máximos-conteos-query.html","id":"conteos","chapter":"Capítulo 2 Identificación de valores mínimos y máximos, conteos (QUERY)","heading":"2.1 Conteos","text":"Tenemos una muestra de 534 casos.","code":"\nlength(CPS1985$education)\n#> [1] 534"},{"path":"identificación-de-valores-mínimos-y-máximos-conteos-query.html","id":"cuántos-valores-hay-mayores-que-16-en-educación","chapter":"Capítulo 2 Identificación de valores mínimos y máximos, conteos (QUERY)","heading":"2.1.1 ¿Cuántos valores hay mayores que 16 en educación?","text":"El vector mayores_16 nos muestra el índice que contiene los casos de\neducación > que 16Para ver los valores ejecutamos¿Cuántos casos hay en la muestra que tengan educación mayor que 16.","code":"\nmayores_16 <- which(CPS1985$education > 16)\n\nmayores_16\n#>  [1]  34 163 169 177 181 183 187 190 194 203 211 214 249 259\n#> [15] 272 289 322 344 431 432 436 437 438 443 446 448 450 452\n#> [29] 453 455 456 459 460 469 473 475 478 482 484 485 486 492\n#> [43] 497 498 500 501 503 505 508 511 513 521 524 530 532\nCPS1985$education[mayores_16]\n#>  [1] 17 17 17 18 18 18 17 18 18 17 18 18 17 18 18 17 17 17\n#> [19] 18 17 18 18 17 18 18 17 17 18 17 18 18 18 18 18 18 18\n#> [37] 18 18 18 17 18 17 18 17 18 17 17 17 18 17 18 17 17 18\n#> [55] 17\nlength(mayores_16)\n#> [1] 55"},{"path":"ploteos-y-gráficas.html","id":"ploteos-y-gráficas","chapter":"Capítulo 3 Ploteos y Gráficas","heading":"Capítulo 3 Ploteos y Gráficas","text":"La función clásica del paquete base par plotear es plot(). Ella se\nadapta automáticamente la geometría del objeto (en este caso dataset)\ngraficar.Podemos ordenar los datos para tener mayor claridad y etiquetar los ejes\ncartecianos, así como agrevar color y tipo de línea","code":"\nplot(CPS1985$education)consulta desde la consola ?plot()\nplot(sort(CPS1985$education), main= \"Cuatrimestres de educación del personal\", ylab=\"Cuatrimestre\", xlab=\"Individuo (caso muestral)\",col= \"red\")"},{"path":"ploteos-y-gráficas.html","id":"gráficos-de-caja-y-bigote","chapter":"Capítulo 3 Ploteos y Gráficas","heading":"3.1 Gráficos de caja y bigote","text":"Si quieres ignorar (contemplar) los outilers usa el parámetro\n“outline = FALSE”En este caso los valores que difieren en más de 3 sigma son tenidos\nen cuenta para la media y el desvío estandard.","code":"\nboxplot(CPS1985$education, col=\"blue\", main=\"Gráficas para 1,2y3 Sigma con outilers - Nivel de educación\")\nboxplot(CPS1985$education, outline = FALSE  ,col=\"green\", main=\"Gráficas para 1,2y3 Sigma sin outilers - Nivel de educación\")"},{"path":"ploteos-y-gráficas.html","id":"histogramas","chapter":"Capítulo 3 Ploteos y Gráficas","heading":"3.2 Histogramas","text":"Histogramas dicotómicosClasificador en rangosHistograma de 50 cajas","code":"\nhist(CPS1985$education,breaks = 2, main=\"clasificador en dos submuestras de igual rango\", ylab=\"Frecuencai relativa o conteo\" )\nhist(CPS1985$education, col=\"orange\")\nhist(CPS1985$education,breaks = 50, main=\"clasificador de 50 cajas\", col=\"pink\" )"},{"path":"ploteos-y-gráficas.html","id":"funciones-de-densidad","chapter":"Capítulo 3 Ploteos y Gráficas","heading":"3.3 Funciones de densidad","text":"La función density() en R calcula la estimación de la densidad\nkernel de una variable numérica univariada, lo que permite visualizar la\ndistribución de los datos través de una curva continua en lugar de\nbarras. Esta función genera un conjunto de pares de puntos (x, y) que\nrepresentan la curva de densidad, la cual muestra la probabilidad de que\nlos datos se encuentren en un determinado rango, y es útil para entender\nla forma y dispersión de una distribución.Para una muestra de datos numéricos, density() estima cómo se\ndistribuyen los datos.Genera una curva suave: diferencia de un histograma, que usa\nbarras, density() crea una curva suave y continua, facilitando la\ninterpretación visual de los patrones. Produce puntos (x, y):La salida de la función es un objeto que contiene las coordenadas x e y\nde la curva de densidad estimada, permitiendo su graficación o análisis\nposterior.Utiliza estimación kernel:La función se basa en la técnica de estimación de densidad kernel, que\ncalcula la densidad de probabilidad de la variable.¿Para qué sirve?Visualización de datos:Es una herramienta fundamental para crear gráficos de densidad, que\nofrecen una representación fluida y continua de la distribución de una\nvariable.Análisis de la forma de los datos:Permite observar la forma general de la distribución, como la presencia\nde picos (modas) o el sesgo.Comparación de distribuciones:Facilita la comparación visual de la distribución de diferentes grupos\nde datos cuando se grafican varias curvas de densidad en el mismo\ngráfico.","code":"En otras palabras ella estima la distribución de probabilidad:\ndensidad <- density(CPS1985$education)\nprint(densidad)\n#> \n#> Call:\n#>  density.default(x = CPS1985$education)\n#> \n#> Data: CPS1985$education (534 obs.);  Bandwidth 'bw' = 0.5738\n#> \n#>        x                 y            \n#>  Min.   : 0.2786   Min.   :1.476e-05  \n#>  1st Qu.: 5.1393   1st Qu.:2.171e-03  \n#>  Median :10.0000   Median :2.513e-02  \n#>  Mean   :10.0000   Mean   :5.138e-02  \n#>  3rd Qu.:14.8607   3rd Qu.:7.301e-02  \n#>  Max.   :19.7214   Max.   :3.036e-01\nplot(density(CPS1985$education), col=\"violet\")"},{"path":"ploteos-y-gráficas.html","id":"comparación-de-dos-kernels","chapter":"Capítulo 3 Ploteos y Gráficas","heading":"3.4 comparación de dos kernels","text":"Generación de Vectores compararutilizaremos la función lines para agregar líneas una gáfico ya\nexistente y la función legend para etiquetar las gráficas","code":"\nlibrary(AER)\ndata(CPS1988)\n# Genero vectores\nVector_85 <- CPS1985$education\nVector_88 <- CPS1988$education\n\n# Cantidad de casos en las muestras\n\nlength(Vector_85)\n#> [1] 534\nlength(Vector_88)\n#> [1] 28155\n# Gráfica primera función\nplot(density(Vector_88),col=\"blue\", main=\"Graficas de Densidad Comparativa\", ylab=\"Densidad\")\n\n#Gráfica segunda función\nlines(density(Vector_85), col = \"violet\")\n\n# Caja de leyendas\nlegend(\"topright\", legend = c(\"Vector 1988\", \"Vector 1985\"), col = c(\"blue\", \"violet\"), lty = 1)"},{"path":"ploteos-y-gráficas.html","id":"alternativas","chapter":"Capítulo 3 Ploteos y Gráficas","heading":"3.5 Alternativas","text":"Como siempre en R, existen alternativas que perteneces bibliotecas\n(libraries) que hacen trabajo más vistoso con menos código","code":"\n\n# install.packages(\"remotes\")\n# remotes::install_github(\"R-CoderDotCom/ridgeline@main\")\n\nlibrary(ridgeline)\n\nridgeline(chickwts$weight, chickwts$feed,\n          mode = TRUE)"},{"path":"estadísticos-univariados-normales.html","id":"estadísticos-univariados-normales","chapter":"Capítulo 4 Estadísticos univariados normales","heading":"Capítulo 4 Estadísticos univariados normales","text":"","code":""},{"path":"estadísticos-univariados-normales.html","id":"cálculo-de-la-media","chapter":"Capítulo 4 Estadísticos univariados normales","heading":"4.1 Cálculo de la media","text":"La media significa el promedio o la suma de todos los elementos\ndivididos entre el total de la muestra, o lo que es lo mismo es un\npromedio de todos los elementos.La media proporciona una medida de localización central de los datos. Si\nlos datos son datos de una muestra, la media se denota x¯ ; si los datos\nson datos de una población, la media se denota con la letra griega μ ..3","code":""},{"path":"estadísticos-univariados-normales.html","id":"fórmula-de-la-media","chapter":"Capítulo 4 Estadísticos univariados normales","heading":"4.1.1 Fórmula de la media","text":"En las fórmulas estadísticas se usa identificar el valor de la primera\nobservación de la variable \\(x\\) con \\(x_1\\) , el valor de la segunda\nobservación de la variable \\(x\\) con \\(x_2\\) y así con lo siguiente.En general, el valor de la -ésima observación de la variable \\(x\\) se\ndenota \\(x_i\\) hasta la posición final del conjunto de datos \\(x_n\\) .La media se representa como \\(\\bar{x}\\) para la muestra o \\(\\hat{x}\\) para\nla población.Aquí la fórmula para le media.\\[\\bar{x}=  \\sum_{=1}^{n}{\\frac{x_i}{n}}  = \\frac{x_1+x_2+x_3+ \\ldots  x_n}{n}  \\]Se podría calcular manoPero lo habitual es calcularlo con la función integrada","code":"\nsuma_de_x <- sum(Vector_85)\nn <- length(Vector_85)\npromedio_aritmetico <- suma_de_x/n\npromedio_aritmetico\n#> [1] 13.01873\nmean(Vector_85)\n#> [1] 13.01873"},{"path":"estadísticos-univariados-normales.html","id":"cálculo-de-la-mediana","chapter":"Capítulo 4 Estadísticos univariados normales","heading":"4.2 Cálculo de la Mediana","text":"La mediana refleja el valor central de los datos. Como lo dice Lind\n(2015) es el punto medio de los valores una vez que se han sido\nordenados de menor mayor o de mayor menor..4Algunas veces, posiblemente cuando se detectan valores atípicos o\noutliers de la muestra, es necesario pensar en utilizar un concepto\nllamado media recortada la cual se calcula nuevamente pero habiendo\nquitando cierto porcentaje de los valores mayores y menores del\nconjunto.La función subset() filtra bajo una expresión o condición un conjunto de\ndatos que pueden ser vectores o data.frames. En este caso el resultado\nes un vector en R llamado datos.reducido.El propósito de la mediana es reflejar la tendencia central de la\nmuestra, de manera que esté influida por los valores extremos. Dado\nque las observaciones en una muestra son \\(x1,x2,...,xn\\) , acomodados en\norden de magnitud creciente, es decir, ordenados ascendentemente, la\nmediana de los datos estará dada por alguna de las maneras dependiendo\nsi el número de elementos es par o es impar:","code":"\nmedian(Vector_85)\n#> [1] 12"},{"path":"estadísticos-univariados-normales.html","id":"cálculo-del-desvío-estandard","chapter":"Capítulo 4 Estadísticos univariados normales","heading":"4.2.1 Cálculo del desvío estandard","text":"","code":"\nsd(Vector_85)\n#> [1] 2.615373"},{"path":"datos-categóricos.html","id":"datos-categóricos","chapter":"Capítulo 5 Datos Categóricos","heading":"Capítulo 5 Datos Categóricos","text":"Entendemos por datos categóricos aquellos que definen la pertenencia\nde un objeto estadístico una categoría o clase de acuerdo alguno de\nsus atributos. De acuerdo con el nivel de medición los datos categóricos\ncorresponden las variables medidas en escalas nominales u ordinales,\naunque aquí nos centraremos en las nominales. Una variable nominal\nasigna pertenencia una categoría excluyente, es decir, señala una\nigualdad o diferencia. En términos lógicos la única operación que\npodemos realizar es \\(=b\\) o \\(\\ne  b\\). En el caso de las ordinales\nexiste una magnitud, algunas categorías tienen más y otras tienen menos\nde algún atributo, por lo que es posible hacer operaciones como\n\\(\\gt  b\\) o \\(\\lt b\\). Sin embargo esa magnitud expresa una distancia\ndesconocida.","code":""},{"path":"datos-categóricos.html","id":"tipos-de-análisis-categóricos","chapter":"Capítulo 5 Datos Categóricos","heading":"5.1 Tipos de Análisis Categóricos","text":"Veamos algunos ejemplos de tratamiento de estos datos. En nuestra tabla\ncolumnas como “gendre” , “ethnicity” u “ocupatión” son variables\nFactoriales o Categóricas.Para trabajarlas debemos construir clases que se adapten este tipo de\ndatos.Ploteandolas tenemosPodemos usar la biblioteca kable para analizarlas como tablaTable 5.1: Tabla de conteos de la variable GéneroVariable OcupaciónTable 5.2: Tabla de conteos de la variable GéneroGráfica de pastel","code":"\ngenero <- table (CPS1985$gender)\nclass(genero)\n#> [1] \"table\"\ngenero\n#> \n#>   male female \n#>    289    245\nplot(genero)\nlibrary(kableExtra)\n genero %>%   kable(., caption=\"Tabla de conteos de la variable Género\")\n#> Warning: 'xfun::attr()' is deprecated.\n#> Use 'xfun::attr2()' instead.\n#> See help(\"Deprecated\")\n\n#> Warning: 'xfun::attr()' is deprecated.\n#> Use 'xfun::attr2()' instead.\n#> See help(\"Deprecated\")\nocupacion <- table (CPS1985$occupation)\nprop.table(ocupacion)\n#> \n#>     worker  technical   services     office      sales \n#> 0.29213483 0.19662921 0.15543071 0.18164794 0.07116105 \n#> management \n#> 0.10299625\nkable( ocupacion , caption=\"Tabla de conteos de la variable Género\")\n#> Warning: 'xfun::attr()' is deprecated.\n#> Use 'xfun::attr2()' instead.\n#> See help(\"Deprecated\")\n\n#> Warning: 'xfun::attr()' is deprecated.\n#> Use 'xfun::attr2()' instead.\n#> See help(\"Deprecated\")\npie(ocupacion)"},{"path":"datos-categóricos.html","id":"tablas-de-contingencia-bidimensionales","chapter":"Capítulo 5 Datos Categóricos","heading":"5.2 Tablas de Contingencia Bidimensionales","text":"Las tablas de una sola variable son útiles para conocer la distribución\nunivariada de una variable categórica, expresada en cantidades o\nproporciones. Sin embargo el análisis de datos categóricos generalmente\nimplica trabajar con más de una variable. Con más de una variable\ntambién podemos trabajar con tablas, las que llamaremos tablas de\ncontingencia. En función de la cantidad de variables que contemos en una\ntabla de contingencia definiremos el número de dimensiones de la tabla.\nAsí, la anterior es una tabla de una dimensión. Una tabla de\ncontingencia tiene 2 o más dimensiones. Es importante confundir el\nnúmero de categorías de las variables con el número de dimensiones de la\ntabla.Para tablas de contingencia bidimensionales visualmente las dimensiones\nse nos presentan como filas y columnas. En la intersección de cada fila\ny columna hay una celda que registra el conteo de las observaciones que\npertenecen ambas categorías. En términos de conjuntos, el conteo de\nobjetos en la intersección de y B. Nótese que la tabla debe incluir\ntodas las intersecciones posibles, dadas por las categorías de la\nvariable, aun cuando haya observación en ese conjunto de datos en\nparticular. Se les asigna el conteo correspondiente: 0.","code":"\nequidada <- cbind(genero,ocupacion)\nequidada\n#>            genero ocupacion\n#> worker        289       156\n#> technical     245       105\n#> services      289        83\n#> office        245        97\n#> sales         289        38\n#> management    245        55\nlibrary(vcd)\n#> Loading required package: grid\nmosaicplot(equidada)\n# mosaic(equidada)\nlibrary(vcd)\n\nmosaic(prop.table(equidada),panel = )\n\nmosaic(prop.table(equidada), \n       shade=T, #Fortifico con el modelo de independencia. \n       main=\"Equidad en ingresos\", #Pongo títulos con main=\"\"\n       sub = \"Mosaico\", las=3)  "},{"path":"datos-categóricos.html","id":"el-modelo-de-independencia.","chapter":"Capítulo 5 Datos Categóricos","heading":"5.3 El modelo de independencia.","text":"","code":""},{"path":"datos-categóricos.html","id":"historia-mitológica","chapter":"Capítulo 5 Datos Categóricos","heading":"5.3.1 Historia mitológica","text":"Acompáñeme un situación hipotética. Rexthor, dog-bearer5\nhttps://xkcd.com/1725/, una criatura mitológica y malvada que acecha \nlos analistas cualitativos nos ha robado una tabla de contingencia, pero\nha olvidado llevarse las sumas marginales. Empecemos por agradecer que\nfue Rexthor y \\\\\\\\\\…Preguntémonos cómo podríamos reconstruir la tabla original con esos\ndatos. La respuesta es: ninguna manera. Sin embargo podríamos crear una\ntabla completa en la que se respeten las sumas marginales y se aproxime\n-más o menos, nunca lo sabremos– la tabla original.","code":""},{"path":"datos-categóricos.html","id":"proporciones-y-sumas-marginales-para-una-tabla-de-contingencia.","chapter":"Capítulo 5 Datos Categóricos","heading":"5.3.2 Proporciones y sumas marginales para una tabla de contingencia.","text":"Cuando analizamos tablas de contingencia las proporciones marginales\ntotales, de fila o columna facilitan la visualización de las diferencias. La función prop.table() se encarga de calcularlos. Esta función recibe como input una tabla y regresa otra, con las proporciones en lugar de los conteos.La sintaxis de prop.table() es prop.table(x, márgen), donde:\\(x\\) es un objeto de la clase table y\\(x\\) es un objeto de la clase table y\\(margen\\) es un número del 0 al 2 que indica el margen sobre el que se computará la proporción:\n0 para el total de la tabla,\n1 para las filas y\n2 para la columnas.\nPara tablas de más de dos dimensiones podemos usar números mayores.\n\\(margen\\) es un número del 0 al 2 que indica el margen sobre el que se computará la proporción:0 para el total de la tabla,0 para el total de la tabla,1 para las filas y1 para las filas y2 para la columnas.2 para la columnas.Para tablas de más de dos dimensiones podemos usar números mayores.Para tablas de más de dos dimensiones podemos usar números mayores.Si queremos obtener los perfiles de fila o columna podemos usar\nmargin.table(), que regresa un vector con los perfiles marginales. La sintaxis de margin.table() es\n\\[margin.table(x, márgen)\\], donde\\(x\\) es un objeto de la clase table y\\(x\\) es un objeto de la clase table y\\(margen\\) un número del 1 al 2:\n1 para las sumas marginales de las filas y\n2 para las columnas. Nos regresa un vector.\n\\(margen\\) un número del 1 al 2:1 para las sumas marginales de las filas y2 para las columnas. Nos regresa un vector.Si queremos agregar la tabla las sumas marginales usamos la funciónaddmargins(), con la sintaxis\\[addmargins(x, c(margen, margen))\\].El vector de márgenes creado con c() observa las mismas reglas ya expuestas. Nos regresa un objeto de la clase table con las mismas dimensiones del original al que ha agregado una fila y/o una columna para las sumas marginales.En tanto que el conteo para normas ambieltales será","code":"\n#Dos vectores de: variables categóricas.\\\nISO_9000 <- c(\"Sí\", \"No\", \"Sí\", \"No\", \"No\", \"Sí\", \"No\", \"No\", \"No\", \"No\")\nISO_14000 <- c(\"No\", \"No\", \"Sí\", \"Sí\", \"Sí\", \"No\", \"Sí\", \"Sí\", \"Sí\", \"Sí\")\n\n#Tablas univariadas (conteos).\nISO_9000\n#>  [1] \"Sí\" \"No\" \"Sí\" \"No\" \"No\" \"Sí\" \"No\" \"No\" \"No\" \"No\"\ntable(ISO_9000)\n#> ISO_9000\n#> No Sí \n#>  7  3\nISO_14000\n#>  [1] \"No\" \"No\" \"Sí\" \"Sí\" \"Sí\" \"No\" \"Sí\" \"Sí\" \"Sí\" \"Sí\"\ntable(ISO_14000)\n#> ISO_14000\n#> No Sí \n#>  3  7"},{"path":"datos-categóricos.html","id":"tabla-de-contigencia-por-filas","chapter":"Capítulo 5 Datos Categóricos","heading":"5.3.3 Tabla de contigencia por filas","text":"Proporciones de filasProporciones porcentuales de columnas.","code":"\nx <- table(ISO_9000, ISO_14000)\nprop.table(x)\n#>         ISO_14000\n#> ISO_9000  No  Sí\n#>       No 0.1 0.6\n#>       Sí 0.2 0.1\nprop.table(x,1)\n#>         ISO_14000\n#> ISO_9000        No        Sí\n#>       No 0.1428571 0.8571429\n#>       Sí 0.6666667 0.3333333\nprop.table(x, 2)*100\n#>         ISO_14000\n#> ISO_9000       No       Sí\n#>       No 33.33333 85.71429\n#>       Sí 66.66667 14.28571"},{"path":"datos-categóricos.html","id":"sumas-marginales","chapter":"Capítulo 5 Datos Categóricos","heading":"5.4 Sumas marginales","text":"","code":"\nmargin.table(x, 1)      #Filas\n#> ISO_9000\n#> No Sí \n#>  7  3\nmargin.table(x, 2)      #Columnas\n#> ISO_14000\n#> No Sí \n#>  3  7"},{"path":"datos-categóricos.html","id":"tablas-adosadas","chapter":"Capítulo 5 Datos Categóricos","heading":"5.4.1 Tablas adosadas","text":"","code":"\n#Tabla con marginales adosados. \n\naddmargins(x, c(1, 2))\n#>         ISO_14000\n#> ISO_9000 No Sí Sum\n#>      No   1  6   7\n#>      Sí   2  1   3\n#>      Sum  3  7  10"},{"path":"datos-categóricos.html","id":"prueba-de-hipótesis-de-independencia-estadística-para-tablas-de-contingencia.","chapter":"Capítulo 5 Datos Categóricos","heading":"5.5 Prueba de hipótesis de independencia estadística para tablas de contingencia.","text":"Como vimos en las tablas del ejemplo anterior los certificados de ISO9000 e ISO14000 parecen ser las mismas personas: quién acreditan Calidad tiene una probabilidad baja de acreditar Ambiental y viceversa. Si ISO_9000 y ISI_14000 fueran variables medidas en la escala razón podríamos estimar el coeficiente de correlación, pero como son variables categóricas es posible. Lo que sí podemos es verificar si hay alguna tipo de relación entre esas variables comparando los datos observados con una tabla ideal que presenta la distribución que recíproca que tendrían esas variables si estuvieran distribuidas al azar. Es decir, si certificar Ambiental afectara la probabilidad de certifical Calidad y viceversa. Esa tabla ideal existe, pero podemos crearla y llamarla modelo de independencia: nos indica la forma que tendrían los conteos si las variables fueran independientes.\nSi la tabla observada tiene una gran divergencia con respecto al modelo de independencia podríamos afirmar que hay independencia entre filas y columnas: hay algo que es el azar que está incidiendo en su distribución.","code":""},{"path":"datos-categóricos.html","id":"conteos-esperados.","chapter":"Capítulo 5 Datos Categóricos","heading":"5.6 Conteos esperados.","text":"Los conteos esperados se obtienen multiplicando las suma marginales\ncorrespondientes una celda y dividiéndola por la n.\\[ E= \\frac {\\sum(fila) \\sum(columna)}{n} \\]¿Cómo lo hacemos? La forma más simple es multiplicar los conteos marginales para cada celda y luego dividirlos por la n, el total de observaciones. Las proporciones marginales indican la probabilidad de que un objeto en el sentido estadístico pertenezca cierta categoría. En cada celda ubicamos el conteo según la probabilidad conjunta de la categoría de las filas y de la de las columnas. Por las proporciones marginales de la tabla robada sabemos sabemos que una observación tiene una probabilidad de \\(P=0.6\\) de certificar ISO9000 y $ P = 0.3$\nde certificar ISO14000. Es decir, una probabilidad conjunta de \\(0.18\\) obtenida al multiplica \\(0.6 y 0.3.\\) Podemos describir ese valor como\\[ P( ISO9000_{} | ISO14000_{})\\]\nla probabilidad de certificar ISO9000 ni ISO14000.\nla probabilidad de certificar ISO9000 ni ISO14000.En esta tabla las probabilidades marginales son conocidas, pero las centrales . Estamos usando un criterio para construirla: que las probabilidades marginales tienen información suficiente para crear la tabla y que cualquier otra variación es producto del azar. Según nuestro supuesto las únicas variables que afectan la probabilidad de cada celda son ellas mismas, el resto es azar. Este modelo reconstruye los conteos asumiendo que esas variables son independientes en su distribución, es decir, dependen solamente de ellas mismas. Por eso lo llamamos modelo de independencia: son la reconstrucción ideal de una distribución de probabilidades conjuntas en el que ambas variables son independientes.Veamos, pues, que conteos produce el modelo de independencia para la tabla robada.Agradezcamos la amabilidad de Rexthor, dog-bearer, que nos ha regresado la tabla original. Ahora podemos comparar cuan buena era la predicción de nuestro modelo. Directamente computaremos la diferencia entre los valores centrales de la tabla –los marginales ya sabemos que son iguales.Table 5.3: Residuales crudos","code":"\n\n#Función para estimar valores esperados. \nesperados <- function (x) {\n  res = matrix(rep(0, length(x)), ncol=ncol(x)) \nfor(i in 1:nrow(x)) {\n  for(j in 1:ncol(x)) {\n    res[i,j] = (margin.table(x, 1)[i] * margin.table(x, 2)[j])/sum(x) }}\n  return(res)}\n\n#Código para ejemplo: Ya no lo voy usar.\n#addmargins(x, c(1,2)) -> rexthor\n#rexthor[1:2, 1:2] <- 0\n\n#ftable(rexthor, main=\"La tabla robada\")\nespe <- esperados(x)\nftable(espe, main=\"Frecuencias esperadas bajo el supuesto de independencia estadística\")\n#>      A   B\n#>           \n#> A  2.1 4.9\n#> B  0.9 2.1\nresiduos_crudos <- x-esperados(x)\nkable(residuos_crudos, caption=\"Residuales crudos\")\n#> Warning: 'xfun::attr()' is deprecated.\n#> Use 'xfun::attr2()' instead.\n#> See help(\"Deprecated\")\n\n#> Warning: 'xfun::attr()' is deprecated.\n#> Use 'xfun::attr2()' instead.\n#> See help(\"Deprecated\")\nlibrary(vcd)\nmosaicplot(x)\nmosaic(x, \n       shade=T, #Fortifico con el modelo de independencia. \n       main=\"Asociación entre certificación Calidad vs Ambiental\", \n       sub = \"Mosaico\")  \nassoc(x)    #Análisis de los residuales de Pearson. \nassoc(x, \n      shade=T, \n      main=\"Asociación entre Certificación Calidad vs Ambiental\", \n      sub=\"Residuales de Pearson\")"},{"path":"análisis-multivariado.html","id":"análisis-multivariado","chapter":"Capítulo 6 Análisis Multivariado","heading":"Capítulo 6 Análisis Multivariado","text":"","code":""},{"path":"análisis-multivariado.html","id":"la-magia-de-r-cran","chapter":"Capítulo 6 Análisis Multivariado","heading":"6.1 La Magia de R-cran","text":"Realizar un análisis multivariado con R-CRAN ofrece numerosas ventajas, que lo convierten en una herramienta muy popular en el ámbito de la estadística, la ciencia de datos y la investigación. continuación, se detallan las principales:Potencia y Flexibilidad:Gran variedad de paquetes: R-CRAN cuenta con una enorme colección de paquetes (librerías) que extienden su funcionalidad. Para el análisis multivariado, existen paquetes especializados para técnicas como:Análisis de Componentes Principales (PCA)Análisis Factorial Exploratorio (EFA) y Confirmatorio (CFA)Análisis de Clúster (K-means, jerárquico)Modelos de Ecuaciones Estructurales (SEM)Análisis de Correspondencias Múltiples (MCA)Análisis DiscriminanteY muchos más…Capacidad de personalización: diferencia de muchos programas de “apuntar y hacer clic”, R permite los usuarios escribir su propio código, lo que brinda un control total sobre el análisis. Esto es ideal para resolver problemas específicos, crear funciones personalizadas y automatizar tareas repetitivas.Naturaleza Gratuita y de Código Abierto:Sin costo de licencia: R es un software de código abierto, lo que significa que es completamente gratuito. Esto lo hace accesible para estudiantes, investigadores, pequeñas empresas y cualquier persona que pueda permitirse software comercial costoso.Comunidad activa y soporte: La gran comunidad de usuarios de R contribuye la creación de nuevos paquetes, la documentación y la resolución de problemas. Puedes encontrar una gran cantidad de foros, blogs, tutoriales y recursos en línea para ayudarte en tu camino.Reproducibilidad y Transparencia:Scripting: Al trabajar con R, se escribe un script o código que documenta cada paso del análisis. Esto hace que el proceso sea reproducible, lo que es fundamental en la investigación científica. Cualquiera puede ejecutar el mismo código y obtener los mismos resultados, lo que aumenta la transparencia y la confianza en los hallazgos.Flujo de trabajo documentado: El código actúa como una bitácora del análisis. Esto facilita la colaboración con otros investigadores, la revisión de un proyecto y la identificación de posibles errores.Visualización de Datos de Alta Calidad:Gráficos avanzados: R es reconocido por su capacidad para crear visualizaciones de datos de gran calidad. Paquetes como ggplot2 permiten generar gráficos muy personalizables, informativos y estéticamente atractivos.Visualización de resultados multivariados: Para el análisis multivariado, la visualización es crucial. R facilita la creación de gráficos de dispersión, biplots, dendrogramas y otras representaciones visuales que ayudan interpretar los resultados complejos.Integración con otros lenguajes y herramientas:Interoperabilidad: R puede conectarse y trabajar con otros lenguajes de programación como Python, así como con bases de datos y diversas fuentes de datos (archivos de texto, CSV, Excel, etc.).Reportes automáticos: Se puede integrar R con herramientas como R Markdown para generar informes automáticos y dinámicos que combinan el código, los resultados del análisis y el texto explicativo.","code":""},{"path":"análisis-multivariado.html","id":"matrices-de-correlación","chapter":"Capítulo 6 Análisis Multivariado","heading":"6.2 Matrices de correlación","text":"Una matriz de correlación multivariada es una tabla cuadrada y simétrica que muestra los coeficientes de correlación de Pearson (o de otro tipo) para cada par de variables en un conjunto de datos. En otras palabras, es una herramienta clave en el análisis multivariado que resume la fuerza y la dirección de la relación lineal entre múltiples variables simultáneamente.","code":""},{"path":"análisis-multivariado.html","id":"interpretación","chapter":"Capítulo 6 Análisis Multivariado","heading":"6.2.1 Interpretación","text":"Estructura: La matriz tiene el mismo número de filas y columnas que el número de variables que se están analizando.Estructura: La matriz tiene el mismo número de filas y columnas que el número de variables que se están analizando.Diagonal principal: La diagonal principal de la matriz siempre contiene valores de 1. Esto se debe que cada variable está perfectamente correlacionada consigo misma.Diagonal principal: La diagonal principal de la matriz siempre contiene valores de 1. Esto se debe que cada variable está perfectamente correlacionada consigo misma.Simetría: La matriz es simétrica. El valor de la correlación entre la variable y la variable B es el mismo que entre la variable B y la variable . Por lo tanto, la información en la parte superior e inferior de la diagonal es un espejo.Simetría: La matriz es simétrica. El valor de la correlación entre la variable y la variable B es el mismo que entre la variable B y la variable . Por lo tanto, la información en la parte superior e inferior de la diagonal es un espejo.Coeficientes de correlación: Los valores que están en la diagonal son los coeficientes de correlación entre los pares de variables. Estos valores oscilan entre -1 y 1.\nValores cercanos 1: Indican una correlación positiva fuerte. Cuando una variable aumenta, la otra también tiende aumentar.\nValores cercanos -1: Indican una correlación negativa fuerte. Cuando una variable aumenta, la otra tiende disminuir.\nValores cercanos 0: Indican que existe una relación lineal fuerte entre las variables. Es importante recordar que esto significa que haya ninguna relación, sino que hay una relación lineal.\nCoeficientes de correlación: Los valores que están en la diagonal son los coeficientes de correlación entre los pares de variables. Estos valores oscilan entre -1 y 1.Valores cercanos 1: Indican una correlación positiva fuerte. Cuando una variable aumenta, la otra también tiende aumentar.Valores cercanos 1: Indican una correlación positiva fuerte. Cuando una variable aumenta, la otra también tiende aumentar.Valores cercanos -1: Indican una correlación negativa fuerte. Cuando una variable aumenta, la otra tiende disminuir.Valores cercanos -1: Indican una correlación negativa fuerte. Cuando una variable aumenta, la otra tiende disminuir.Valores cercanos 0: Indican que existe una relación lineal fuerte entre las variables. Es importante recordar que esto significa que haya ninguna relación, sino que hay una relación lineal.Valores cercanos 0: Indican que existe una relación lineal fuerte entre las variables. Es importante recordar que esto significa que haya ninguna relación, sino que hay una relación lineal.Notemos como el paradigma de objetos nos permite utilizar el comano plot que se adapta al objeto “matriz multivariada”.","code":"\nplot(CPS1985)\n\n#install.packages(\"corrplot\")\nlibrary(corrplot)\n#> corrplot 0.95 loaded\nmatrizCPS1985 <- cor(CPS1985[ ,1:4])\ncorrplot(matrizCPS1985)\npalette = colorRampPalette(c(\"green\", \"white\", \"red\")) (20)\nheatmap(x = matrizCPS1985, col = palette, symm = TRUE)"},{"path":"análisis-multivariado.html","id":"fumar-y-productividad","chapter":"Capítulo 6 Análisis Multivariado","heading":"6.3 Fumar y productividad","text":"matriz de correlación","code":"\ndata(\"CigarettesSW\")\ndatos_cigarros <- as.data.frame(CigarettesSW[ , - c(1,2)])\nstr(datos_cigarros)\n#> 'data.frame':    96 obs. of  7 variables:\n#>  $ cpi       : num  1.08 1.08 1.08 1.08 1.08 ...\n#>  $ population: num  3973000 2327000 3184000 26444000 3209000 ...\n#>  $ packs     : num  116 129 105 100 113 ...\n#>  $ income    : num  4.60e+07 2.62e+07 4.40e+07 4.47e+08 4.95e+07 ...\n#>  $ tax       : num  32.5 37 31 26 31 ...\n#>  $ price     : num  102.2 101.5 108.6 107.8 94.3 ...\n#>  $ taxs      : num  33.3 37 36.2 32.1 31 ...\ncorelacion_cigarros <- cor(datos_cigarros)\ncorelacion_cigarros\n#>                    cpi  population      packs     income\n#> cpi         1.00000000  0.04758017 -0.4994643  0.2317893\n#> population  0.04758017  1.00000000 -0.2112834  0.9573113\n#> packs      -0.49946432 -0.21128337  1.0000000 -0.3317847\n#> income      0.23178932  0.95731126 -0.3317847  1.0000000\n#> tax         0.68571446  0.16598557 -0.6421176  0.3372751\n#> price       0.91165558  0.14586043 -0.6524732  0.3375339\n#> taxs        0.70412144  0.18891721 -0.6574167  0.3582307\n#>                   tax      price       taxs\n#> cpi         0.6857145  0.9116556  0.7041214\n#> population  0.1659856  0.1458604  0.1889172\n#> packs      -0.6421176 -0.6524732 -0.6574167\n#> income      0.3372751  0.3375339  0.3582307\n#> tax         1.0000000  0.8993727  0.9853330\n#> price       0.8993727  1.0000000  0.9203278\n#> taxs        0.9853330  0.9203278  1.0000000\nlibrary(psych)\n#> \n#> Attaching package: 'psych'\n#> The following object is masked from 'package:car':\n#> \n#>     logit\ncorPlot(corelacion_cigarros)\npairs.panels(corelacion_cigarros,\n             smooth = TRUE,      # Si TRUE, dibuja ajuste suavizados de tipo loess\n             scale = FALSE,      # Si TRUE, escala la fuente al grado de correlación\n             density = TRUE,     # Si TRUE, añade histogramas y curvas de densidad\n             ellipses = TRUE,    # Si TRUE, dibuja elipses\n             method = \"pearson\", # Método de correlación (también \"spearman\" o \"kendall\")\n             pch = 21,           # Símbolo pch\n             lm = FALSE,         # Si TRUE, dibuja un ajuste lineal en lugar de un ajuste LOESS\n             cor = TRUE,         # Si TRUE, agrega correlaciones\n             jiggle = FALSE,     # Si TRUE, se añade ruido a los datos\n             factor = 2,         # Nivel de ruido añadido a los datos\n             hist.col = 4,       # Color de los histogramas\n             stars = TRUE,       # Si TRUE, agrega el nivel de significación con estrellas\n             ci = TRUE)          # Si TRUE, añade intervalos de confianza a los ajustes"},{"path":"análisis-multivariado.html","id":"herramientas-exploratorias-semi-automáticas","chapter":"Capítulo 6 Análisis Multivariado","heading":"6.4 Herramientas exploratorias semi automáticas","text":"Dataset esoph tiene datos referidos correlación entre consumo de alcohol y tabaco en forma conjunta con test por contraste (casos de control) y entre casos independientes y muestra selectiva en blanco.Utilizaremos la biblioteka skimrTable 6.1: Data summaryVariable type: factorVariable type: numeric","code":"\nlibrary(skimr)\nskim(esoph)\n#> Warning: 'xfun::attr()' is deprecated.\n#> Use 'xfun::attr2()' instead.\n#> See help(\"Deprecated\")\n\n#> Warning: 'xfun::attr()' is deprecated.\n#> Use 'xfun::attr2()' instead.\n#> See help(\"Deprecated\")\n\n#> Warning: 'xfun::attr()' is deprecated.\n#> Use 'xfun::attr2()' instead.\n#> See help(\"Deprecated\")"},{"path":"análisis-multivariado.html","id":"gráficos-especiales","chapter":"Capítulo 6 Análisis Multivariado","heading":"6.5 Gráficos Especiales","text":"Algunas funciones para dibujar algunos diagramas especiales: la función ‘bagplot’ traza un diagrama de bolsa, ‘faces’ traza caras de Chernoff, ‘iconplot’ traza una representación de una tabla de frecuencia o una matriz de datos, ‘plothulls’ traza cascos de un conjunto de datos bivariados, ‘plotsummary’ traza un resumen gráfico de un conjunto de datos, ‘puticon’ agrega iconos un gráfico, ‘skyline.hist’ combina varios histogramas de un conjunto de datos unidimensional en un gráfico, las funciones ‘slider’ admiten algunos gráficos interactivos, ‘spin3R’ ayuda inspeccionar una nube de puntos de 3 dim, ‘stem.leaf’ traza un gráfico de tallo y hoja, ‘stem.leaf.backback’ traza versiones consecutivas del gráfico de tallo y hoja.La biblioteca aplpack generará rostros según la familia de flores del dataset iris.\nDada la enorme capacidad que nuestro cerebro tiene para percibir aún variaciones pequeñas entre rostros, esta técnica permite de un sólo vistazo indagar sobre un dataset completo.","code":"\nlibrary(\"aplpack\")\niris_sample<-iris[sample(1:dim(iris)[1],size=16,replace=F),]\n\niris_sample\n#>     Sepal.Length Sepal.Width Petal.Length Petal.Width\n#> 39           4.4         3.0          1.3         0.2\n#> 90           5.5         2.5          4.0         1.3\n#> 136          7.7         3.0          6.1         2.3\n#> 54           5.5         2.3          4.0         1.3\n#> 62           5.9         3.0          4.2         1.5\n#> 31           4.8         3.1          1.6         0.2\n#> 106          7.6         3.0          6.6         2.1\n#> 150          5.9         3.0          5.1         1.8\n#> 45           5.1         3.8          1.9         0.4\n#> 134          6.3         2.8          5.1         1.5\n#> 79           6.0         2.9          4.5         1.5\n#> 78           6.7         3.0          5.0         1.7\n#> 96           5.7         3.0          4.2         1.2\n#> 3            4.7         3.2          1.3         0.2\n#> 61           5.0         2.0          3.5         1.0\n#> 65           5.6         2.9          3.6         1.3\n#>        Species\n#> 39      setosa\n#> 90  versicolor\n#> 136  virginica\n#> 54  versicolor\n#> 62  versicolor\n#> 31      setosa\n#> 106  virginica\n#> 150  virginica\n#> 45      setosa\n#> 134  virginica\n#> 79  versicolor\n#> 78  versicolor\n#> 96  versicolor\n#> 3       setosa\n#> 61  versicolor\n#> 65  versicolor\n\nfaces(iris_sample[1:4],face.type=1,labels=iris_sample$Species)#> effect of variables:\n#>  modified item       Var           \n#>  \"height of face   \" \"Sepal.Length\"\n#>  \"width of face    \" \"Sepal.Width\" \n#>  \"structure of face\" \"Petal.Length\"\n#>  \"height of mouth  \" \"Petal.Width\" \n#>  \"width of mouth   \" \"Sepal.Length\"\n#>  \"smiling          \" \"Sepal.Width\" \n#>  \"height of eyes   \" \"Petal.Length\"\n#>  \"width of eyes    \" \"Petal.Width\" \n#>  \"height of hair   \" \"Sepal.Length\"\n#>  \"width of hair   \"  \"Sepal.Width\" \n#>  \"style of hair   \"  \"Petal.Length\"\n#>  \"height of nose  \"  \"Petal.Width\" \n#>  \"width of nose   \"  \"Sepal.Length\"\n#>  \"width of ear    \"  \"Sepal.Width\" \n#>  \"height of ear   \"  \"Petal.Length\""},{"path":"análisis-multivariado.html","id":"análisis-exploratŕio-automático","chapter":"Capítulo 6 Análisis Multivariado","heading":"6.6 Análisis Exploratŕio Automático","text":"","code":""},{"path":"análisis-multivariado.html","id":"dataexplorer","chapter":"Capítulo 6 Análisis Multivariado","heading":"6.6.1 DataExplorer","text":"El paquete DataExplorer() es capaz de tomar un dataframe y generarnos un informe completo en formato html mediante el comando create_report. Lamentablemente el informe es interactivo y podemos incrustarlo en este artículo. Por ello te pedimos que ejecutes desde la consola.Este generará un exahustivo análisis de la planilla oficial de sobrevivientes del Titanic, indicando sesgos varianzas, promedios, Existencias de NA, co-varianzas, etc.","code":"library(DataExplorer)\ncreate_report(mtcars)"},{"path":"análisis-multivariado.html","id":"ggally","chapter":"Capítulo 6 Análisis Multivariado","heading":"6.6.2 GGally","text":"Otra herramienta muy utilizada es GGally, tiene una enorme cantidad de gráficos que se adaptan data.frames categoricos , lógicos, numericos, etc.","code":"\nlibrary(GGally)\n#> Loading required package: ggplot2\n#> \n#> Attaching package: 'ggplot2'\n#> The following objects are masked from 'package:psych':\n#> \n#>     %+%, alpha\nggpairs(mtcars)\ndf <- mtcars[, c(1,3,4,5,6,7)]\n\nggcorr(df, palette = \"RdBu\", label = TRUE)\nrequire(\"survival\")\n# Fit survival functions\nsurv <- survfit(Surv(time, status) ~ sex, data = lung)\n# Plot survival curves\nsurv.p <- ggsurv(surv)\nsurv.p"},{"path":"data_analitics.html","id":"data_analitics","chapter":"Capítulo 7 Data_Analitics","heading":"Capítulo 7 Data_Analitics","text":"lo largo de este curso desarrollaremos una serie de tópicos que son novedosos y omnipresentes en la Ingeniería Industrial 4.0. Muchos de ellos han estado presente en el pasado, tales como los métodos estadísticos. Pero solamente han capturado la atención de gobiernos, academia y empresas cuando las arquitecturas del hardware, más los avances de datos disponibles en la nube han logrado superar las barreras de los estancos que los límites que el papel nos imponía. Gran parte de los cimientos de la inteligencia artificial se remontan los años 50 del siglo XX, autores como GEORGE BOOLE en 1854 sentaron las bases de esta disciplina.\nEn 1950 ALAN TURING Propone en su ensayo Computing Machinery Intelligence el Test de Turing, una prueba de comunicación verbal hombre-máquina que evalúa la capacidad de las segundas de hacerse pasar por humanos.Luego en 1956 JOHN MCCARTHY El informático acuña por primera vez el término “Inteligencia Artificial”, durante la Conferencia de Darmouth, evento considerado el germen de la disciplina, para finalizar con MARVIN MINSKY que en 1969 escribe el libro perceptores, el trabajo fundamental del análisis de las redes neuronales artificiales.En estos días hay dos lenguajes de propósito general que se están imponiendo en el terreno de abordar y aplicar los conceptos de Analítica de Datos.El primero de ellos es Python, que sin dudas veras que ha tenido repercución y éxito, aún cuando ha llegado ser parte del Sylabus de todas las carreras de ingeniería industrial.El otro lengunaje, que mi leal entender es más rápidamente asimilable con los ingenieros industriales es R-Cran.","code":""},{"path":"data_analitics.html","id":"bibliografía-recomendada","chapter":"Capítulo 7 Data_Analitics","heading":"7.1 Bibliografía Recomendada","text":"","code":""},{"path":"data_analitics.html","id":"desambiguación-de-conceptos","chapter":"Capítulo 7 Data_Analitics","heading":"7.2 Desambiguación de conceptos","text":"Se suelen utilizar como sinónimos, pero son diferentes conceptosAnalítica de DatosMachine LearningDeep LearningBusiness IntelligenceMinería de DatosInteligencia ArtificialInteligencia Artificial GenerativaTodos ellos tienen un factor en común, el uso del un dataset, hardware y software, pero si bien tienen similitudes son bastantes distintos.Veamos una aproximación al problema partiendo de los modelos estadísticos.Cualquier estadístico que usemos (T-Studen, Distribución Normal, Criterio de máxima verosimilitud, etc) puede ser obordado con el uso de computadora tomando datos etiquetados (que tienen una columna que muestra de que tipo son), luego con un algoritmo apropiado podemos construir un modelo.En analítica de datos invertimos el proceso.\nTomando datos sin etiquetar los sometemos al análisis de un modelo y predecimos que resultado de obtendría en un mundo físico partir de los datos que son abstracción de ese mundo.Así:Predecir cuando se romperá un ejeEncontrar en que categoría de la flota sería conveniente incorporar un camión nuevoDeterminar antes de cerrar una transacción electrónica virtual la probabilidad de fraudeEtc.Son ejemplos de lo que se puede hacer con analítica de datos.@ehrlinger_treating_2018En inteligencia artificial el proceso es muy semejante. diferencia del caso anterior tenemos un algorítmo que puede crear el modelo en base las etiquetas de los datos. En este caso se opera en forma supervisada. para crear un modelo.\nEl resultado de la inteligencia artificial es un modelo que nos entrega datos y partir de datos que el tomó para entrenarse y nos muestra patrones de semejanza entre los datos de entrenamiento y los de prueba.Es una práctica habitual crear modelos de IA utilizando un dataset y remuestrear (sampling) usando el 25% de los datos para entrenar y el 75% para predecir y evaluar la calidad del modelo.Ejemplos de este tipo de modelos son los árboles de decisión y su entrenamientoPor último tenemos el abordaje de machine learning.6","code":""},{"path":"data_analitics.html","id":"data-analitycs","chapter":"Capítulo 7 Data_Analitics","heading":"7.3 Data Analitycs","text":"La analítica de datos permite las organizaciones analizar todos sus datos (en tiempo real, históricos, estructurados, estructurados, cualitativos) para identificar patrones y generar conocimientos para informar y, en algunos casos, automatizar decisiones, conectando la inteligencia y la acción. Las mejores soluciones actuales respaldan el proceso analítico de un extremo otro, desde el acceso, la preparación y la analítica de datos hasta la operatividad de los análisis y el seguimiento de los resultados.Según TIBCO, la analítica de datos permite las organizaciones transformar digitalmente su empresa y su cultura, volviéndose más innovadoras y con visión de futuro en la toma de decisiones. Más allá del monitoreo y la generación de informes tradicionales de KPI para encontrar patrones ocultos en los datos, las organizaciones potenciadas por algoritmos son los nuevos innovadores y líderes empresariales.Al cambiar el paradigma más allá de los datos para conectar los conocimientos con la acción, las empresas podrán crear experiencias personalizadas para los clientes, crear productos digitales conectados, optimizar las operaciones y aumentar la productividad de los empleados, pero el tratamiento que asegure la calidad de la informació nes y será aprtir de este momento un factor clave de éxito.7Con la analítica colaborativa de datos, las empresas permiten que todos contribuyan al éxito empresarial, desde ingenieros de datos y científicos de datos, hasta desarrolladores y analistas empresariales, e incluso profesionales y líderes empresariales. La analítica colaborativa de datos también incentiva quienes están dentro y fuera de una organización conectarse y colaborar. Por ejemplo, los científicos de datos pueden trabajar en estrecha colaboración con un cliente para ayudarlo resolver sus problemas en tiempo real utilizando la interfaz de usuario altamente colaborativa de la analítica moderna del mundo de hoy.La analítica de datos impulsa las empresas avanzar mediante la introducción de algoritmos en todas partes para optimizar los momentos comerciales críticos, como un cliente que ingresa su tienda, un equipo punto de fallar u otros eventos que podrían significar la diferencia entre ganar o perder negocios. La analítica de datos se aplica todas las industrias, incluidas las de servicios financieros y seguros, fabricación, energía, transporte, viajes y logística, atención médica y otras. La analítica de datos puede ayudar predecir y manejar interrupciones, optimizar rutas, brindar un servicio proactivo al cliente, realizar ofertas inteligentes de venta cruzada, predecir fallas inminentes de equipos, administrar el inventario en tiempo real, optimizar los precios y prevenir el fraude.","code":""},{"path":"data_analitics.html","id":"qué-dimensiones-engloba-el-concepto-de-ad","chapter":"Capítulo 7 Data_Analitics","heading":"7.4 ¿Qué dimensiones engloba el concepto de AD?","text":"Automatización de ReportesInteligencia de NegociosPreparción de datos (Data Wrangling)Visualización de Datos y ConocimientoAnalítica Geoespacial (GIS)Analítica productiva (predictiva y prescriptiva)Machine LearningAnalítica en tiempo real","code":""},{"path":"data_analitics.html","id":"cómo-utilizar-la-analítica-de-datos-el-proceso-analítico","chapter":"Capítulo 7 Data_Analitics","heading":"7.5 ¿Cómo utilizar la analítica de datos: el proceso analítico?","text":"Comprenda el problema empresarial.Recopile/identifique datos relevantes para el problema.Prepare los datos para el análisis.Analice los datos para generar conocimientos.Implemente/ponga en funcionamiento los análisis y los modelos.Supervise y optimice el rendimiento.","code":""},{"path":"data_analitics.html","id":"dataset-o-información-base","chapter":"Capítulo 7 Data_Analitics","heading":"7.5.1 Dataset o Información base","text":"Tomaremos un dataset simple, que nos muestra la cantidad de semanas que un pack de baterias de un autoelevador fue capaz de tomar carga como para resistir las operaciones durante toda una jornada de trabajo sin necesidad de recargarse o ser cambiado. Fuente [apte_data_1997]Trucos:Puedes cargar los datos en un vecto con el comando Edad_75 <- scan() . tipea uno uno los valores de edad y luego dos veces seguidas Enter para finalizar el proceso.En los siguientes capítulos veremos como capturar de fuentes de datos como .xls","code":""},{"path":"data_analitics.html","id":"tratamiento-y-captura-de-la-información","chapter":"Capítulo 7 Data_Analitics","heading":"7.6 Tratamiento y captura de la información","text":"Obtener el la edad de la batería con nro de inventario 3Nos interesa saber si la vida en semanas de nuestras baterias compradas coincide con los valores publicados en el sitio web del fabricantes. Se considera que una batería tiene una vida últil mientras al aplicarle una carga completa ella pueda resistir todo el turno de 8 horas sin necesidad de ser reemplazada.Obtener las edades de la muestraPromedio de vida","code":"\ndatos_75 <- c(1 , 19 , 0,\n2 , 18 , 0,\n3 , 22 , 0,\n4 , 25 , 0,\n5 , 17 , 0,\n6 , 30 , 0,\n7 , 29 , 0,\n8 , 32 , 0,\n9 , 31 , 0,\n10, 33 , 0,\n11, 38 , 0,\n12, 36 , 0,\n13, 40 , 1,\n14, 40 , 0,\n15, 42 , 0,\n16, 45 , 0,\n17, 47 , 0,\n18, 49 , 0,\n19, 55 , 0,\n20, 58 , 1,\n21, 57 , 1,\n22, 63 , 1,\n23, 65 , 1,\n24, 65 , 1,\n25, 66 , 1)\nMuestra_75 <- matrix(datos_75, ncol = 3, byrow = TRUE)\nMuestra_75\n#>       [,1] [,2] [,3]\n#>  [1,]    1   19    0\n#>  [2,]    2   18    0\n#>  [3,]    3   22    0\n#>  [4,]    4   25    0\n#>  [5,]    5   17    0\n#>  [6,]    6   30    0\n#>  [7,]    7   29    0\n#>  [8,]    8   32    0\n#>  [9,]    9   31    0\n#> [10,]   10   33    0\n#> [11,]   11   38    0\n#> [12,]   12   36    0\n#> [13,]   13   40    1\n#> [14,]   14   40    0\n#> [15,]   15   42    0\n#> [16,]   16   45    0\n#> [17,]   17   47    0\n#> [18,]   18   49    0\n#> [19,]   19   55    0\n#> [20,]   20   58    1\n#> [21,]   21   57    1\n#> [22,]   22   63    1\n#> [23,]   23   65    1\n#> [24,]   24   65    1\n#> [25,]   25   66    1\nMuestra_75[3,2]\n#> [1] 22\nplot ((Muestra_75[ ,2]), main= \"Vida en Semanas\",xlab=\"Ficha Taller Batería con falla\", ylab= \"Semanas\")\nvida_media_muestra <- mean(Muestra_75[,2])\nvida_media_muestra\n#> [1] 40.88\nplot (sort(Muestra_75[ ,2]), main= \"Vida en Semanas\",xlab=\"Batería\", ylab= \"Semanas\")\nabline(h=vida_media_muestra)\nabline(h=40,col=\"red\")\nabline(h=50,col=\"green\")"},{"path":"data_analitics.html","id":"muestra-de-historial-de-carga","chapter":"Capítulo 7 Data_Analitics","heading":"7.6.1 Muestra de Historial de Carga","text":"","code":"\nplot(Muestra_75[ , 1:2],main = \"Resumen de Casos\",xlab = \"Ficha Bateria\",ylab=\"Edad\", type=\"b\", col=\"RED\")"},{"path":"data_analitics.html","id":"histogramas-de-edades","chapter":"Capítulo 7 Data_Analitics","heading":"7.7 Histogramas de Edades","text":"","code":"\nhist(Muestra_75[ ,2],breaks = 10, main = \"Histogramas de edades\")"},{"path":"data_analitics.html","id":"gráficos-de-densidad","chapter":"Capítulo 7 Data_Analitics","heading":"7.8 Gráficos de Densidad","text":"También conocido como gráfico de densidad de Kernel y gráfico de densidad de traza.Un gráfico de densidad visualiza la distribución de datos en un intervalo o período de tiempo continuo. Este gráfico es una variación de un Histograma que usa el suavizado de cerner para trazar valores, permitiendo distribuciones más suaves al suavizar el ruido. Los picos de un gráfico de densidad ayudan mostrar dónde los valores se concentran en el intervalo.Una ventaja de los gráficos de densidad sobre los histogramas es que son mejores para determinar la forma de distribución porque se ven afectados por el número de contenedores utilizados (cada barra utilizada en un histograma típico). Un histograma que consta de solo 4 compartimientos producirá una forma de distribución lo suficientemente distinguible como lo haría un histograma de 20 compartimientos. Sin embargo, con los gráficos de densidad esto es un problema.La función de densidad puede calcularse fácilmene en R-Cran con el siguiente comandoDado que es un objeto de R partir del resultado de los cuantiles es posible plotear directamente la función invocada","code":"\ndensity(Muestra_75[ ,2])\n#> \n#> Call:\n#>  density.default(x = Muestra_75[, 2])\n#> \n#> Data: Muestra_75[, 2] (25 obs.); Bandwidth 'bw' = 7.394\n#> \n#>        x                y            \n#>  Min.   :-5.183   Min.   :5.383e-05  \n#>  1st Qu.:18.159   1st Qu.:2.474e-03  \n#>  Median :41.500   Median :1.246e-02  \n#>  Mean   :41.500   Mean   :1.070e-02  \n#>  3rd Qu.:64.841   3rd Qu.:1.698e-02  \n#>  Max.   :88.183   Max.   :2.186e-02\nplot(density(Muestra_75[ ,2]), main = \"Gráfico de Densidad\", ylab=\"Cantidad relativa de muestras\",xlab=\"Edad\")"},{"path":"data_analitics.html","id":"gráfica-conjunta-de-histograma-y-densidad","chapter":"Capítulo 7 Data_Analitics","heading":"7.8.1 Gráfica conjunta de Histograma y Densidad","text":"","code":"\nhist(Muestra_75[ ,2], # histogram\n    breaks = 3,\n     col=\"peachpuff\", # column color\n border=\"black\",\n prob = TRUE, # show densities instead of frequencies\n xlab = \"Edad\",\n main = \"Distribución Edades de la Muestra\")\nlines(density(Muestra_75[ ,2]), # density plot\n lwd = 2, # thickness of line\n col = \"chocolate3\")"},{"path":"data_analitics.html","id":"varianza-de-muestra-y-población","chapter":"Capítulo 7 Data_Analitics","heading":"7.9 Varianza de Muestra y Población","text":"La varianza de una población está expresada por la ecuación\\[\\sigma^2 = \\sum_{=1}^{n} \\frac {(x_i – \\mu)^2} {N}   \\]La varianza (de la muestra puede ser calculada con el comandoDato que la muestra siempre tiene un número considerablemente menor de individuos que la población la expresión que se usa para calcular la varianza de la muestra es distinta.Varianza de la muestra es\\[ s^2 = \\sum_{=1}^{n} \\frac {(x_i – \\bar{x})^2} {n-1}  \\]Puede encontrarse la varianza de la población sustituyendo los valores como se indica continuación:Con esto valores podemos calcular la varianza de la población con la siguiente codificación\\[ \\sigma = \\frac {S} {n} \\]El desvio estandar de la meustra esEn tanto que el desvió estándar de la población esUna forma más elegante de presentar estos resultados parciales puede conseguirse con el uso de la función *sprintf()Un valor que es interesante calcular para comparar con la muestra es el desvío porcentual calculado como \\(dsp_{\\%}= \\frac{dsp}{n}\\)","code":"  var(Muestra_75[ ,2])\nvar_muestra <- var(Muestra_75 [ ,2])\nvar_muestra\n#> [1] 244.61\nn <- length(Muestra_75[ ,2])\nn\n#> [1] 25\nn_over_n_1 <- n/(n-1)\nn_over_n_1\n#> [1] 1.041667\nvar_poblacion <- var(Muestra_75[ ,2]) / n\nvar_poblacion\n#> [1] 9.7844\ndsm <- sd({Muestra_75[ ,2]})\ndsm\n#> [1] 15.64001\ndsp <- sqrt(var_poblacion)\ndsp\n#> [1] 3.128003\nsprintf(\"Varianza muestral =%s, Varianza poblacional = %s\", var_muestra, var_poblacion )\n#> [1] \"Varianza muestral =244.61, Varianza poblacional = 9.7844\"\ndspp_75 <- dsp/n\ndspp_75\n#> [1] 0.1251201"},{"path":"data_analitics.html","id":"repetir-en-análisis-para-la-muestra-siemens-1005","chapter":"Capítulo 7 Data_Analitics","heading":"7.10 Repetir en análisis para la muestra Siemens 1005","text":"Tarea !Siempre comenzaremos nuestro trabajo con un análisis exploratorio básico, tal como hemos señalado.\nEn este caso hemos descartado ningún individuo de la muestra, pero sería conveniente hacerlo.\nEl comando boxplot() te permitirá hacer una inspección rápida.\nAdisionalmente prueba el comando summary() para analizar la distribución de cunatiles y datos faltantes.","code":""},{"path":"data_analitics.html","id":"construcción-de-un-modelo","chapter":"Capítulo 7 Data_Analitics","heading":"7.11 Construcción de un modelo","text":"Nota!En esta parte comenzaremos desarrollar un modelo.En la ficha del fabricante de baterías, se indica que el hecho de sustituir el electrolíto original por otro que sea el oficial y que en general contiene antimonio puede ser en primera instancia considerado como un factor que prolonga la vida útil de la batería y amplía el rango de su ciclo de carga/descarga.Sin embrago el fabricante advierte del peligro potencial de cáncer de próstata que el antimonio presenta para el personal técnico que opera las baterías y para el conductor del vehículo.Vale recordar que las placas de plomo tienen en su aleación antimonio y medida que se van envejeciendo liberan antimonio al electrolito, que aún cuando sea en concentraciones bajas, son capaces de afctar la salud. Por este motivo podríamos inferir que hay una correlación entre la vida útil de las baterías y la aparición de síntomas de cáncer de próstata.Utilizaremos una hipótesis simple. El modelo de regresión lineal podría ayudarnos predecir para esta población que edad deberíamos comenzar el diagnóstico de antígeno prostático en el personal afectado las tareas que se relacionan con las baterías.También realizaremos una comprobación basada en el modelo para saber si ha habido cambios significativos en los síntomas atribuibles al uso del antimonio.Como se puede ver, recurrimos aún la medicina, pero nos planteamos una hipótesis sobre las implicancias del antimonio y la salud y el objetivo es otorgarle al médico una recomendación para que tenga una alerta temprana, sin que esto afirme o desmienta hechos científicos sobre los que tenemos capacidad para diagnostiras enfermedades y nos limitamos prevenirlas.","code":""},{"path":"data_analitics.html","id":"hipótesis-o-pregunta-de-investigación","chapter":"Capítulo 7 Data_Analitics","heading":"7.12 Hipótesis o pregunta de investigación","text":"El proceso de investigación científica repetible parte de este tipo de premisa. Es un proceso iterativo de aproximación y revelación.8 Comenzaremos con una pregunta de investigación. ¿Será posible utilizar un modelo de regresión lineal para entender el comportamiento de estos datos?. Esta pregunta con el tiempo podrá ser formulada como una afirmación. Este proceso debe tener una investigación preliminar. De esteo modo podríamos expresar algo como El modelo de regresión lineal, que es muy utilizado en inteligencia artificial podrá darme respuestas dos preguntas clave de este problema1- El modelo regresión lineal puede predecir la probabilidad de aparición temprana de cáncer.\n2- Es posible utilizar el modelo construido para comparar dos poblaciones.","code":""},{"path":"data_analitics.html","id":"bases-de-un-modelo-ia-de-regresión-lienal","chapter":"Capítulo 7 Data_Analitics","heading":"7.13 Bases de un modelo IA de regresión lienal","text":"Para trabajar en este sentido R-Cran tiene posibilidades de desarrollar modelos de regresión lineal en un sólo comando. pesar de ello construiremos un modelo desde CERO para entender como se procede en Inteligencia Artificial.9Todo modelo de regresión se basa en hallar los parámetros \\(\\) y \\(b\\) de una ecuación de una recta.\\[ y = *x + b \\]Donde: y es un número que varía entre 1 y 01 señala que se trata de un caso probable positivo, en tanto que0 señala que es poco probable la aparición de cancer.Para poder hallar los valores de estas variable o parámetros del modelo deberíamos utilizar los datos. En realidad con solamente dos datos de la muestra podríamos encontrar una recta que pase por estos dos puntos. Lo más aconsejable es utilizar sólo dos, sino todos los puntos.\nEsto implica plantear un problema de dos ecuaciones con dos incógnitas y en rigor el método de los mínimos cuadrados sería lo más indicado.Siguiendo con la idea de la construcción artesanal del modelo podemos decir que encontrando un punto significativo del modelo por el que pase la recta podremos agregar alguna estrategia para hallar la pendiente y con esos datos calcular \\(\\) y \\(b\\).","code":""},{"path":"data_analitics.html","id":"puntos-significativos-de-modelo","chapter":"Capítulo 7 Data_Analitics","heading":"7.14 Puntos significativos de modelo","text":"Un punto significativo podría ser la moda de las edades y la moda de la variable categórica [1,0].\nSe puede demostrar que en el modelo de regresión de los mínimos cuadrados este punto es el promedio de \\(\\overline{x}\\) y el promedio de \\(\\overline{y}\\).\nPero existen otros valores verosímiles (Ver criterios de máxima verosimilitud de Ronald Fisher).","code":""},{"path":"data_analitics.html","id":"cácluo-de-x-e-y-significativos","chapter":"Capítulo 7 Data_Analitics","heading":"7.14.1 Cácluo de x e y significativos","text":"Valor de y verosimilValor de x verosimilEl punto de coordenadas [24.5 , 0.5] es un punto por el que debe pasar la recta que construiremos.","code":"\nindex_ymax <- which.max(Muestra_75[ ,3])\nymax <- Muestra_75[index_ymax,3]\n\nindex_ymin <- which.min(Muestra_75[ ,3])\nymin <- Muestra_75[index_ymin,3]\n\ny_sig <- (ymax-ymin) /2\n\ny_sig \n#> [1] 0.5\nindex_xmax <- which.max(Muestra_75[ ,2])\nxmax <- Muestra_75[index_xmax,2]\nxmax\n#> [1] 66\nindex_xmin <- which.min(Muestra_75[ ,2])\nxmin <- Muestra_75[index_xmin,2]\nxmin\n#> [1] 17\n\nx_sig <- (xmax-xmin)\n\nx_sig \n#> [1] 49"},{"path":"data_analitics.html","id":"cáclulo-de-la-pendiente","chapter":"Capítulo 7 Data_Analitics","heading":"7.14.2 Cáclulo de la pendiente","text":"em valor \\(\\) representa la pendiente de la recta. De modo que con los \\(\\Delta{x}\\) y \\(\\Delta{y}\\) podremos calcularlo facilmente\\(\\) = valor de la pendiente es 0.02040816Pendiente en grados","code":"\na <- ((ymax-ymin)/(xmax-xmin))\na\n#> [1] 0.02040816\natan(a)\n#> [1] 0.02040533"},{"path":"data_analitics.html","id":"cáclulo-de-la-ordenada-al-origen.","chapter":"Capítulo 7 Data_Analitics","heading":"7.14.3 Cáclulo de la ordenada al origen.","text":"Para calcular la ordenada al origen basta con poner \\(x=\\overline{x}\\) del punto significativo que hemos adoptado en la formula de la recta usando el \\(\\) calculado y despejar \\(b\\)\\[ y = *x +b\\]\n\\[ b = y-*x\\]\nCáclulo de b","code":"\ny_sig\n#> [1] 0.5\nx_sig\n#> [1] 49\na\n#> [1] 0.02040816\n\nb <- y_sig-(a*x_sig)\nb\n#> [1] -0.5"},{"path":"data_analitics.html","id":"gráfica","chapter":"Capítulo 7 Data_Analitics","heading":"7.14.4 Gráfica","text":"","code":"\nplot(Muestra_75[ ,2],Muestra_75[ ,3], main = \"Dignóstico Preliminar\", ylab=\"Diagnóstico + -\", xlab=\"Edad\")\nabline(h=y_sig, col=\"red\")\nabline(v=x_sig,col=\"red\")\nabline(b,a)"},{"path":"data_analitics.html","id":"modelo-de-regresión-lineal-generalizado","chapter":"Capítulo 7 Data_Analitics","heading":"7.15 Modelo de regresión lineal generalizado","text":"En realidad este trabajo que hemos hecho puede ser realizado con solamente un comando en R-Cran.Construiremos el datasetConstrucción del modelo linealComo vemos nos dice que la ordenada al origen es -0.610 y la pendiente es 0.021 muy parecidas los valores \\(\\) y \\(b\\) que obtuvimos antes.","code":"\nx_muestra_75 <- Muestra_75[ ,2]\ny_muestra_75 <- Muestra_75[ ,3]\nmrlg <- as.data.frame( cbind(x_muestra_75,y_muestra_75))\nmi_formula <- y_muestra_75 ~ x_muestra_75\nmodelo_2 <-  lm (mi_formula, data=mrlg)\nmodelo_2\n#> \n#> Call:\n#> lm(formula = mi_formula, data = mrlg)\n#> \n#> Coefficients:\n#>  (Intercept)  x_muestra_75  \n#>     -0.61021       0.02178\nmodelo_2$coefficients[2]\n#> x_muestra_75 \n#>   0.02177616\nmodelo_2$coefficients[1]\n#> (Intercept) \n#>  -0.6102094\nplot(mrlg)\nabline(modelo_2,col=\"blue\")"},{"path":"data_analitics.html","id":"modelo-de-regresión-logística","chapter":"Capítulo 7 Data_Analitics","heading":"7.16 Modelo de regresión logística","text":"Una de las cosas destacables del uso de IA es que puedo recurrir muchos modelos para interpretar o predecir el comportamiento de los datos. Puedo generar (entrenar) el modelo o modelos y utilizar algún mecanismo de medición de la calidad de cada modelo y en base ello utilizar IA para seleccionar el que más se adapta mis datos.Hay una distribución de probabilidades llamada Distribución Logística que puede utilizarse (del mismo modo que la distribución lineal se usa para el modelo de regresión lineal generalizado) para armar una regresión lineal logística.La forma típica de la distribución logística normalizada tiene la siguiente ecuación paramétrica y luce como se ve en el ejemplo.\\[ \\sigma(x)= \\frac{e^x}{(e^x+1)} = \\frac{1}{1+e^{-x}}\\]Como pude verse el modelo de regresión logística es mas sensible los datos y proporcional resultados más verosímiles, pero aún mantienen error admisible. De todos modos esta análisis visual nos indica que ya tenemos una alternativa que es mejor que nuestro modelo lineal inicial.La función de distribución de probabilidad acumulada de la regresión logística es:\\[g(F(x))=  ln \\left[ \\frac{F(x)}{1-F(x)} \\right] = \\beta^0 + \\beta_1*x \\]\nAl igual que en la regresión lineal es posible (aplicando logaritmos) encontrar los parámetros \\(\\beta_0\\) y \\(\\beta_1\\) con solamente cuatro datos en la muestra. El error será inadmisible, pero es algo parecido armar una regresión lineal con sólo dos muestras.","code":"\ncurve((1/(1+exp(-x))),-10,10,col =\"red\")\nabline(0.5,0.05,col=\"green\")\ntext(-8,0.2,\"Rergresion Lineal\",col=\"green\")\ntext(6,0.8,\"Rergresion Logística\",col=\"red\")"},{"path":"patrones-de-asociación.html","id":"patrones-de-asociación","chapter":"Capítulo 8 Patrones de asociación","heading":"Capítulo 8 Patrones de asociación","text":"","code":""},{"path":"patrones-de-asociación.html","id":"introducción-1","chapter":"Capítulo 8 Patrones de asociación","heading":"8.1 Introducción","text":"Los algoritmos de reglas de asociación tienen como objetivo encontrar relaciones dentro un conjunto de transacciones, en concreto, items o atributos que tienden ocurrir de forma conjunta. En este contexto, el término transacción hace referencia cada grupo de eventos que están asociados de alguna forma, por ejemplo:El pedido de la compra en un supermercado.El pedido de la compra en un supermercado.Los libros que compra un cliente en una librería.Los libros que compra un cliente en una librería.Las páginas web visitadas por un usuario.Las páginas web visitadas por un usuario.Las características que aparecen de forma conjunta.Las características que aparecen de forma conjunta.Cómo ordenar los repuestos en el almacén de un taller de mantenimientoCómo ordenar los repuestos en el almacén de un taller de mantenimientoDisposición de insumos para la ayuda humanitariaDisposición de insumos para la ayuda humanitariaEl protfolio de iniciativas de innovación (Metodo TRIZ)El protfolio de iniciativas de innovación (Metodo TRIZ)","code":""},{"path":"patrones-de-asociación.html","id":"formatos-del-dataset","chapter":"Capítulo 8 Patrones de asociación","heading":"8.2 Formatos del dataset","text":"Los datos que se necesitan pueden venir especificados en dos formatos denominados lista corta o lista larga (matricial)","code":""},{"path":"patrones-de-asociación.html","id":"lista-corta","chapter":"Capítulo 8 Patrones de asociación","heading":"8.2.1 Lista corta","text":"Este el el formato utilizado habitualmente por las bases de datos de salida de almacenes, especialmente utilizado en los archivos csv de SAP.","code":"\ndatos_crudo <- c(1,1,1,2,2,3,3,\"5W40\",\"Filtro Aceite\",\"Filtro Aire\",\"Correa Distr\",\"Bomba de Agua\",\"Filtro Nafta\",\"Limpia Inyectores\")\nlista_corta <- matrix(datos_crudo,ncol=2, byrow=FALSE)\ndimnames(lista_corta) <- list(salida = c(1,2,3,4,5,6,7) , producto = c(\"ticket\",\"producto\") )\nlista_corta\n#>       producto\n#> salida ticket producto           \n#>      1 \"1\"    \"5W40\"             \n#>      2 \"1\"    \"Filtro Aceite\"    \n#>      3 \"1\"    \"Filtro Aire\"      \n#>      4 \"2\"    \"Correa Distr\"     \n#>      5 \"2\"    \"Bomba de Agua\"    \n#>      6 \"3\"    \"Filtro Nafta\"     \n#>      7 \"3\"    \"Limpia Inyectores\""},{"path":"patrones-de-asociación.html","id":"lista-larga","chapter":"Capítulo 8 Patrones de asociación","heading":"8.2.2 Lista larga","text":"cada uno de los eventos o elementos que forman parte de una transacción (ticket) se le conoce como item y un conjunto de ellos itemset. Una transacción puede estar formada por uno o varios items, en el caso de ser varios, cada posible subconjunto de ellos es un itemset distinto. Por ejemplo, la transacción \\(T = {,B,C}\\) está formada por 3 items (, B y C) y sus posibles itemsets son: {,B,C}, {,B}, {B,C}, {,C}, {}, {B} y {C}.Una regla de asociación se define como una implicación del tipo “si \\(X entonces Y” (X⇒Y\n)\\), donde \\(X e Y\\) son itemsets o items individuales. El lado izquierdo de la regla recibe el nombre de antecedente o lenft-hand-side (LHS) y el lado derecho el nombre de consecuente o right-hand-side (RHS). Por ejemplo, la regla {,B} => {C} significa que, cuando ocurren y B, también ocurre C.Existen varios algoritmos diseñados para identificar itemsets frecuentes y reglas de asociación. continuación, se describen algunos de los más utilizados.","code":"\ndatos_m <- c(1,1,1,0,0,0,0, 0,0,0,1,1,0,0, 0,0,0,0,0,1,1)\nlista_larga <- matrix(datos_m,nrow=3,byrow=TRUE)\nlista_larga\n#>      [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n#> [1,]    1    1    1    0    0    0    0\n#> [2,]    0    0    0    1    1    0    0\n#> [3,]    0    0    0    0    0    1    1\ndimnames(lista_larga) <- list(ticket = c(1,2,3) , producto = c(\"5W40\",\"Filtro Aceite\",\"Filtro Aire\",\"Correa Distr\",\"Bomba de Agua\",\"Filtro Nafta\",\"Limpia Inyectores\"))\n\nlista_larga\n#>       producto\n#> ticket 5W40 Filtro Aceite Filtro Aire Correa Distr\n#>      1    1             1           1            0\n#>      2    0             0           0            1\n#>      3    0             0           0            0\n#>       producto\n#> ticket Bomba de Agua Filtro Nafta Limpia Inyectores\n#>      1             0            0                 0\n#>      2             1            0                 0\n#>      3             0            1                 1"},{"path":"patrones-de-asociación.html","id":"ejecutando-la-magia","chapter":"Capítulo 8 Patrones de asociación","heading":"8.2.3 Ejecutando la magia","text":"Inspeccionaremos las transacciones, que como es de esperar tendrán una enorme cantidad de reglas descubiertas redundantes.Luego explicaremos que quiere decir “support” y “confidence”Con esto podemos graficar","code":"\nlibrary(arules)\n#> Loading required package: Matrix\n#> \n#> Attaching package: 'arules'\n#> The following objects are masked from 'package:base':\n#> \n#>     abbreviate, write\napriori(lista_larga)\n#> Warning: 'as(<matrix>, \"ngCMatrix\")' is deprecated.\n#> Use 'as(as(as(., \"nMatrix\"), \"generalMatrix\"), \"CsparseMatrix\")' instead.\n#> See help(\"Deprecated\") and help(\"Matrix-deprecated\").\n#> Apriori\n#> \n#> Parameter specification:\n#>  confidence minval smax arem  aval originalSupport maxtime\n#>         0.8    0.1    1 none FALSE            TRUE       5\n#>  support minlen maxlen target  ext\n#>      0.1      1     10  rules TRUE\n#> \n#> Algorithmic control:\n#>  filter tree heap memopt load sort verbose\n#>     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n#> \n#> Absolute minimum support count: 0 \n#> \n#> set item appearances ...[0 item(s)] done [0.00s].\n#> set transactions ...[7 item(s), 3 transaction(s)] done [0.00s].\n#> sorting and recoding items ... [7 item(s)] done [0.00s].\n#> creating transaction tree ... done [0.00s].\n#> checking subsets of size 1 2 3 done [0.00s].\n#> writing ... [13 rule(s)] done [0.00s].\n#> creating S4 object  ... done [0.00s].\n#> set of 13 rules\ntransacciones_1 <- apriori(lista_larga)\n#> Apriori\n#> \n#> Parameter specification:\n#>  confidence minval smax arem  aval originalSupport maxtime\n#>         0.8    0.1    1 none FALSE            TRUE       5\n#>  support minlen maxlen target  ext\n#>      0.1      1     10  rules TRUE\n#> \n#> Algorithmic control:\n#>  filter tree heap memopt load sort verbose\n#>     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n#> \n#> Absolute minimum support count: 0 \n#> \n#> set item appearances ...[0 item(s)] done [0.00s].\n#> set transactions ...[7 item(s), 3 transaction(s)] done [0.00s].\n#> sorting and recoding items ... [7 item(s)] done [0.00s].\n#> creating transaction tree ... done [0.00s].\n#> checking subsets of size 1 2 3 done [0.00s].\n#> writing ... [13 rule(s)] done [0.00s].\n#> creating S4 object  ... done [0.00s].\n\nreglas_a <- inspect(transacciones_1)\n#>      lhs                            rhs                \n#> [1]  {Correa Distr}              => {Bomba de Agua}    \n#> [2]  {Bomba de Agua}             => {Correa Distr}     \n#> [3]  {Filtro Nafta}              => {Limpia Inyectores}\n#> [4]  {Limpia Inyectores}         => {Filtro Nafta}     \n#> [5]  {Filtro Aceite}             => {Filtro Aire}      \n#> [6]  {Filtro Aire}               => {Filtro Aceite}    \n#> [7]  {Filtro Aceite}             => {5W40}             \n#> [8]  {5W40}                      => {Filtro Aceite}    \n#> [9]  {Filtro Aire}               => {5W40}             \n#> [10] {5W40}                      => {Filtro Aire}      \n#> [11] {Filtro Aceite,Filtro Aire} => {5W40}             \n#> [12] {5W40,Filtro Aceite}        => {Filtro Aire}      \n#> [13] {5W40,Filtro Aire}          => {Filtro Aceite}    \n#>      support   confidence coverage  lift count\n#> [1]  0.3333333 1          0.3333333 3    1    \n#> [2]  0.3333333 1          0.3333333 3    1    \n#> [3]  0.3333333 1          0.3333333 3    1    \n#> [4]  0.3333333 1          0.3333333 3    1    \n#> [5]  0.3333333 1          0.3333333 3    1    \n#> [6]  0.3333333 1          0.3333333 3    1    \n#> [7]  0.3333333 1          0.3333333 3    1    \n#> [8]  0.3333333 1          0.3333333 3    1    \n#> [9]  0.3333333 1          0.3333333 3    1    \n#> [10] 0.3333333 1          0.3333333 3    1    \n#> [11] 0.3333333 1          0.3333333 3    1    \n#> [12] 0.3333333 1          0.3333333 3    1    \n#> [13] 0.3333333 1          0.3333333 3    1\nreglas_a\n#>                              lhs                    rhs\n#> [1]               {Correa Distr} =>     {Bomba de Agua}\n#> [2]              {Bomba de Agua} =>      {Correa Distr}\n#> [3]               {Filtro Nafta} => {Limpia Inyectores}\n#> [4]          {Limpia Inyectores} =>      {Filtro Nafta}\n#> [5]              {Filtro Aceite} =>       {Filtro Aire}\n#> [6]                {Filtro Aire} =>     {Filtro Aceite}\n#> [7]              {Filtro Aceite} =>              {5W40}\n#> [8]                       {5W40} =>     {Filtro Aceite}\n#> [9]                {Filtro Aire} =>              {5W40}\n#> [10]                      {5W40} =>       {Filtro Aire}\n#> [11] {Filtro Aceite,Filtro Aire} =>              {5W40}\n#> [12]        {5W40,Filtro Aceite} =>       {Filtro Aire}\n#> [13]          {5W40,Filtro Aire} =>     {Filtro Aceite}\n#>        support confidence  coverage lift count\n#> [1]  0.3333333          1 0.3333333    3     1\n#> [2]  0.3333333          1 0.3333333    3     1\n#> [3]  0.3333333          1 0.3333333    3     1\n#> [4]  0.3333333          1 0.3333333    3     1\n#> [5]  0.3333333          1 0.3333333    3     1\n#> [6]  0.3333333          1 0.3333333    3     1\n#> [7]  0.3333333          1 0.3333333    3     1\n#> [8]  0.3333333          1 0.3333333    3     1\n#> [9]  0.3333333          1 0.3333333    3     1\n#> [10] 0.3333333          1 0.3333333    3     1\n#> [11] 0.3333333          1 0.3333333    3     1\n#> [12] 0.3333333          1 0.3333333    3     1\n#> [13] 0.3333333          1 0.3333333    3     1\nlibrary(arulesViz)\nplot(transacciones_1, method = \"graph\")"},{"path":"patrones-de-asociación.html","id":"apriori","chapter":"Capítulo 8 Patrones de asociación","heading":"8.3 Apriori","text":"Apriori fue uno de los primeros algoritmos desarrollados para la búsqueda de reglas de asociación y sigue siendo uno de los más empleados, tiene dos etapas:Identificar todos los itemsets que ocurren con una frecuencia por encima de un determinado límite (itemsets frecuentes).Convertir esos itemsets frecuentes en reglas de asociación.Con la finalidad de ilustrar el funcionamiento del algoritmo, se emplea un ejemplo sencillo. Supóngase la siguiente base de datos de un centro comercial en la que cada fila es una transacción. En este caso, el término transacción hace referencia todos los productos comprados bajo un mismo ticket (misma cesta de la compra). Las letras , B, C y D hacen referencia 4 productos (items) distintos.","code":""},{"path":"patrones-de-asociación.html","id":"transacción","chapter":"Capítulo 8 Patrones de asociación","heading":"8.4 Transacción","text":"{, B, C, D}{, B, D}{, B}{B, C, D}{B, C}{C, D}{B, D}Antes de entrar en los detalles del algoritmo, conviene definir una serie de conceptos:Soporte: El soporte del item o itemset X es el número de transacciones que contienen X dividido entre el total de transacciones.\\[ Sop_x = \\frac{itemset_x}{itemset_{total}}\\]Confianza: La confianza de una regla “Si X entonces Y” se define acorde la ecuación\\[confianza(X=>Y)=\\frac{soporte(unión(X,Y))}{soporte(X)},\\]\ndonde unión(XY) es el itemset que contienen todos los items de X y de Y. La confianza se interpreta como la probabilidad \\(P(Y|X)\\), es decir, la probabilidad de que una transacción que contiene los items de \\(X\\), también contenga los items de \\(Y\\).Volviendo al ejemplo del centro comercial, puede observarse que, el artículo , aparece en 3 de las 7 transacciones, el artículo B en 6 y ambos artículos juntos en 3. El soporte del item {} es por lo tanto del 43%, el del item {B} del 86% y del itemset {, B} del 43%. De las 3 transacciones que incluyen , las 3 incluyen B, por lo tanto, la regla “clientes que compran el artículo también compran B”, se cumple, acorde los datos, un 100% de las veces. Esto significa que la confianza de la regla {=> B} es del 100%.Encontrar itemsets frecuentes (itemsets con una frecuencia mayor o igual un determinado soporte mínimo) es un proceso trivial debido la explosión combinatoria de posibilidades, sin embargo, una vez identificados, es relativamente directo generar reglas de asociación que presenten una confianza mínima. El algoritmo Apriori hace una búsqueda exhaustiva por niveles de complejidad (de menor mayor tamaño de itemsets). Para reducir el espacio de búsqueda aplica la norma de “si un itemset es frecuente, ninguno de sus supersets (itemsets de mayor tamaño que contengan al primero) puede ser frecuente”. Visto de otra forma, si un conjunto es infrecuente, entonces, todos los conjuntos donde este último se encuentre, también son infrecuentes. Por ejemplo, si el itemset {,B} es infrecuente, entonces, {,B,C} y {,B,E} también son infrecuentes ya que todos ellos contienen {,B}.El funcionamiento del algoritmo es sencillo, se inicia identificando los items individuales que aparecen en el total de transacciones con una frecuencia por encima de un mínimo establecido por el usuario. continuación, se sigue una estrategia bottom-en la que se extienden los candidatos añadiendo un nuevo item y se eliminan aquellos que contienen un subconjunto infrecuente o que alcanzan el soporte mínimo. Este proceso se repite hasta que el algoritmo encuentra más ampliaciones exitosas de los itemsets previos o cuando se alcanza un tamaño máximo.Se procede identificar los itemsets frecuentes y, partir de ellos, crear reglas de asociación.Transacción{, B, C, D}{, B, D}{, B}{B, C, D}{B, C}{C, D}{B, D}Para este problema se considera que un item o itemset es frecuente si aparece en un mínimo de 3 transacciones, es decir, su soporte debe de ser igual o superior 3/7 = 0.43. Se inicia el algoritmo identificando todos los items individuales (itemsets de un único item) y calculando su soporte.Itemset (k=1) Ocurrencias SoporteTodos los itemsets de tamaño k = 1 tienen un soporte igual o superior al mínimo establecido, por lo que todos superan la fase de filtrado (poda).continuación, se generan todos los posibles itemsets de tamaño k = 2 que se pueden crear con los itemsets que han superado el paso anterior y se calcula su soporte.Itemset (k=2) Ocurrencias SoporteLos itemsets {, B}, {B, C}, {B, D} y {C, D} superan el límite de soporte, por lo que son frecuentes. Los itemsets {, C} y {, D} superan el soporte mínimo por lo que se descartan. Además, cualquier futuro itemset que los contenga también será descartado ya que puede ser frecuente por el hecho de que contiene un subconjunto infrecuente.Itemset (k=2) Ocurrencias SoporteSe repite el proceso, esta vez creando itemsets de tamaño k = 3.Itemset (k=3)Los itemsets {, B, C}, {, B, D} y {C, D, } contienen subconjuntos infrecuentes, por lo que son descartados. Para los restantes se calcula su soporte.Itemset (k=3) Ocurrencias SoporteEl items {B, C, D} supera el soporte mínimo por lo que se considera infrecuente. Al haber ningún nuevo itemset frecuente, se detiene el algoritmo.Como resultado de la búsqueda se han identificado los siguientes itemsets frecuentes:Itemset frecuentes{, B}{B, C}{B, D}{C, D}El siguiente paso es crear las reglas de asociación partir de cada uno de los itemsets frecuentes. De nuevo, se produce una explosión combinatoria de posibles reglas ya que, de cada itemset frecuente, se generan tantas reglas como posibles particiones binarias. En concreto, el proceso seguido es el siguiente:Por cada itemset frecuente , obtener todos los posibles subsets de .1.1 Para cada subset s de , crear la regla \\(“s => (-s)”\\)Descartar todas las reglas que superen un mínimo de confianza.Supóngase que se desean únicamente reglas con una confianza igual o superior 0.7, es decir, que la regla se cumpla un 70% de las veces. Tal y como se describió anteriormente, la confianza de una regla se calcula como el soporte del itemset formado por todos los items que participan en la regla, dividido por el soporte del itemset formado por los items del antecedente.","code":""},{"path":"patrones-de-asociación.html","id":"reglas-confianza-confianza","chapter":"Capítulo 8 Patrones de asociación","heading":"8.5 Reglas Confianza Confianza","text":"De todas las posibles reglas, únicamente {C} => {D} y {C} => {B} superan el límite de confianza.La principal desventaja de algoritmo Apriori es el número de veces que se tienen que escanear los datos en busca de los itemsets frecuentes, en concreto, el algoritmo escanea todas las transacciones un total de kmax\n+1, donde:kmax es el tamaño máximo de itemset permitido. Esto hace que el algoritmo Apriori pueda aplicarse en situaciones con millones de registros, sin embargo, se han desarrollado adaptaciones (FP-growth, Eclat, Hash-Based, partitioning, etc) que solucionan esta limitación.","code":""},{"path":"patrones-de-asociación.html","id":"fp-growth","chapter":"Capítulo 8 Patrones de asociación","heading":"8.6 FP-Growth","text":"Los investigadores Han et al. propusieron en el 2000 un nuevo algoritmo llamado FP-Growth que permite extraer reglas de asociación partir de itemsets frecuentes pero, diferencia del algoritmo Apriori, estos se identifican sin necesidad de generar candidatos para cada tamaño.En términos generales, el algoritmo emplea una estructura de árbol (Frequent Pattern Tree) donde almacena toda la información de las transacciones. Esta estructura permite comprimir la información de una base de datos de transacciones hasta 200 veces, haciendo posible que pueda ser cargada en memoria RAM. Una vez que la base de datos ha sido comprimida en una estructura FP-Tree, se divide en varias bases de datos condicionales, cada una asociada con un patrón frecuente. Finalmente, cada partición se analiza de forma separada y se concatenan los resultados obtenidos. En la mayoría de casos, FP-Growth es más rápido que Apriori.","code":""},{"path":"patrones-de-asociación.html","id":"eclat","chapter":"Capítulo 8 Patrones de asociación","heading":"8.7 Eclat","text":"En el 2000, Zaki propuso un nuevo algoritmo para encontrar patrones frecuentes (itemsets frecuentes) llamado Equivalence Class Transformation (Eclat). La principal diferencia entre este algoritmo y Apriori es la forma en que se escanean y analizan los datos. El algoritmo Apriori emplea transacciones almacenadas de forma horizontal, es decir, todos los elementos que forman una misma transacción están en la misma línea. El algoritmo Eclat, sin embargo, analiza las transacciones en formato vertical, donde cada línea contiene un item y las transacciones en las que aparece ese item.Para ilustrar el funcionamiento del algoritmo Eclat, se muestra un ejemplo simplificado. La siguiente tabla contiene la información de las transacciones en formato horizontal. El soporte mínimo para considerar un itemset frecuente es del 20%.En primer lugar, el algoritmo identifica los items que aparecen en el conjunto de todas las transacciones y los emplea para crear la primera columna de una nueva tabla. continuación, se añade cada item el identificador de las transacciones en las que aparece y se calcula su soporte. La siguiente tabla muestra el resultado de reestructurar los datos de formato horizontal vertical.Itemset (k = 1) Transacciones SoporteCalculando todas las posibles intersecciones de la columna Transacciones de la tabla k=1 se obtienen los itemsets de longitud k+1.Itemset (k = 2) Transacciones SoporteDe nuevo, con las intersecciones de las transacciones de la tabla Itemset (k = 2) se obtienen los itemsets k = 3. Gracias al principio downward closure, es necesario realizar la intersección de {i1,i5} e {i4,i5} ya que {i1,i4} es frecuente y por lo tanto tampoco lo es {i1,i4,i5}.Itemset (k = 3) Transacciones SoporteEl algoritmo finaliza cuando hay más itemsets frecuentes.Cabe destacar que, el algoritmo Eclat, permite la identificación de itemsets frecuentes pero genera reglas de asociación. pesar de ello, una vez identificados los itemsets frecuentes, se puede aplicar la segunda parte del algoritmo Apriori para obtenerlas.","code":""},{"path":"patrones-de-asociación.html","id":"arules-package-de-r-cran","chapter":"Capítulo 8 Patrones de asociación","heading":"8.8 Arules package de R-Cran","text":"El paquete de R arules implementa el algoritmo Apriori para la identificación de itemsets frecuentes y la creación de reglas de asociación través de la función apriori(). También implementa el algoritmo Eclat con la función eclat().Tanto \\(apriori()\\) como \\(eclat()\\) reciben como argumento un objeto transaction con los datos de las transacciones, un argumento parameter que determina las características de los itemsets o reglas generadas (por ejemplo, el soporte mínimo) y un argumento control que determina el comportamiento del algoritmo (por ejemplo, ordenación de los resultados). La función apriori() también se incluye el argumento aparence que impone restricciones sobre las reglas generadas, por ejemplo, crear solo reglas que contengan un determinado item. El resultado de ambas funciones es un objeto de tipo association que puede ser manipulado con toda una serie de funciones que ofrece arules. Entre las principales destacan:summary(): muestra un resumen de los resultados.inspect(): muestra los resultados.length(): número de elementos (reglas o itemsets) almacenados.items(): extrae los items que forman un itemset o una regla. En el caso de reglas, combina los items de antecedente (LHS) y del consecuente (RHS). .sort(): ordena los resultados.subset: filtrado de los resultados.","code":""},{"path":"patrones-de-asociación.html","id":"ejemplo-ticket-de-supermercado","chapter":"Capítulo 8 Patrones de asociación","heading":"8.9 Ejemplo ticket de supermercado","text":"Supóngase que se dispone del registro de todas las compras que se han realizado en un supermercado. El objetivo del análisis es identificar productos que tiendan comprarse de forma conjunta para así poder situarlos en posiciones cercanas dentro de la tienda y maximizar la probabilidad de que los clientes compren.Para este ejemplo se emplea el set de datos Groceries del paquete arules, que contiene un registro de todas las ventas realizadas por un supermercado durante 30 días. En total se dispone de 9835 transacciones formadas por combinaciones de 169 productos. El objeto Groceries almacena la información en un formato propio de este paquete (descrito más adelante). Para representar mejor lo que suele ocurrir en la práctica, se ha reestructurado la información en formato de tabla. Los datos pueden encontrarse en Github.Cada línea del archivo contiene la información de un item y el identificador de la transacción (compra) la que pertenece. Esta es la estructura en la que comúnmente se almacenan los datos dentro de una base de datos y que, en el ámbito de las transacciones, recibe el nombre de tabla larga o single. Otra forma en la que se pueden encontrar los datos de transacciones es en formato matriz o tabla ancha, en el que cada fila contiene todos los items que forman parte de una misma transacción, este formato recibe el nombre de cesta o basket.Tal y como se ha definido previamente, el concepto de transacción hace referencia al conjunto de items o eventos que ocurren de forma conjunta. Para este caso de estudio, compras de supermercado, cada transacción está formada por todos los productos que se compran la vez, es decir, el vínculo de unión es el cliente sino cada una de las “cestas de la compra”. Por ejemplo, la transacción con id_compra == 14 está formada por 3 items.","code":"\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ tidyr::expand() masks Matrix::expand()\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ✖ tidyr::pack()   masks Matrix::pack()\n#> ✖ dplyr::recode() masks arules::recode()\n#> ✖ tidyr::unpack() masks Matrix::unpack()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nlibrary(readr)\ndatos <- read_csv(\"https://ricardorpalma.github.io/R-Dataset/datos_groceries.csv\")\n#> Rows: 43367 Columns: 2\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (1): item\n#> dbl (1): id_compra\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# datos <- read_csv(file = \"datos_groceries.csv\", col_names = TRUE)\nhead(datos)\n#> # A tibble: 6 × 2\n#>   id_compra item               \n#>       <dbl> <chr>              \n#> 1         1 citrus fruit       \n#> 2         1 semi-finished bread\n#> 3         1 margarine          \n#> 4         1 ready soups        \n#> 5         2 tropical fruit     \n#> 6         2 yogurt\ndatos %>% filter(id_compra == 14) %>% pull(item)\n#> [1] \"frankfurter\" \"rolls/buns\"  \"soda\""},{"path":"patrones-de-asociación.html","id":"lectura-de-datos","chapter":"Capítulo 8 Patrones de asociación","heading":"8.9.1 Lectura de datos","text":"Con la función read.transactions() se pueden leer directamente los datos de archivos tipo texto y almacenarlos en un objeto de tipo transactions, que es la estructura de almacenamiento que emplea arules. Esta función tiene los siguientes argumentos:file: nombre del archivo que se quiere leer.format: estructura en la que se encuentran almacenados los datos, “basket” si cada línea del archivo es una transacción completa, o “single” si cada línea representa un item.sep: tipo de separación de los campos.cols: si el formato es de tipo “basket”, un entero que indica la columna que contiene el identificador de las transacciones. Para el formato “single”, es un vector con los nombres (o posiciones) de las dos columnas que identifican las transacciones y los items, respectivamente.rm.duplicates: valor lógico indicando si se eliminan los items duplicados en una misma transacción. Por lo general, es conveniente eliminarlos, el interés suele estar en qué items ocurren de forma conjunta, en qué cantidad.quote: carácter que se utiliza como comillas.skip: número de líneas que hay que saltar desde el comienzo del fichero.Es importante recordar que los objetos de tipo transactions solo trabajan con información booleana, es decir, con la presencia o de cada uno de los items en la transacción. Por lo tanto, si el set de datos contiene variables numéricas, estas deben de ser discretizadas en intervalos o niveles. Para discretizar una variable numérica, se puede emplear la función discretize() o bien otras alternativas como la función case_when() del paquete dplyr.Los objetos transactions, se almacenan internamente como un tipo de matriz binaria. Se trata de una matriz de valores 0/1, con una fila por cada transacción, en este caso cada compra, y una columna por cada posible item, en este caso productos. La posición de la matriz (,j) tiene el valor 1 si la transacción contiene el item j.También es posible convertir un objeto dataframe en uno de tipo transactions con la función (dataframe, “transactions”). Para lograrlo, primero hay que convertir el dataframe en una lista en la que cada elemento contiene los items de una transacción. Este proceso puede ser muy lento si el dataframe tiene muchos registros, por lo que suele ser mejor crear un archivo de texto con los datos e importarlo mediante read.transactions().Otra alternativa es convertir el dataframe en una matriz 0/1 en la que cada fila es una transacción y cada columna uno de los posibles items.Exploración de itemsUno de los primeros análisis que conviene realizar cuando se trabaja con transacciones es explorar su contenido y tamaño, es decir, el número de items que las forman y cuáles son. Mediante la función inspect() se muestran los items que forman cada transacción.También es posible mostrar los resultados en formato de dataframe con la función DATAFRAME() o con (transacciones, “dataframe”).Para extraer el tamaño de cada transacción se emplea la función size().CuantilesTamañosLa gran mayoría de clientes compra entre 3 y 4 productos y el 90% de ellos compra como máximo 9.El siguiente análisis básico consiste en identificar cuáles son los items más frecuentes (los que tienen mayor soporte) dentro del conjunto de todas las transacciones. Con la función itemFrequency() se puede extraer esta información de un objeto tipo transactions. El nombre de esta función puede causar confusión. Por “frecuencia” se hace referencia al soporte de cada item, que es la fracción de transacciones que contienen dicho item respecto al total de todas las transacciones. Esto es distinto la frecuencia de un item respecto al total de items, de ahí que la suma de todos los soportes sea 1.Si se indica el argumento type = “absolute”, la función itemFrequency() devuelve el número de transacciones en las que aparece cada item.El listado anterior muestra que los 5 productos que más se venden son: whole milk, vegetables, rolls/buns y soda.Es muy importante estudiar cómo se distribuye el soporte de los items individuales en un conjunto de transacciones antes identificar itemsets frecuentes o crear reglas de asociación, ya que, dependiendo del caso, tendrá sentido emplear un límite de soporte u otro. Por lo general, cuando el número de posibles items es muy grande (varios miles) prácticamente todos los artículos son raros, por lo que los soportes son muy bajos.","code":"\n# IMPORTACIÓN DIRECTA DE LOS DATOS A UN OBJETO TIPO TRANSACTION\n# ==============================================================================\nlibrary(arules)\ntransacciones <- read.transactions(file = \"datos_groceries.csv\",\n                                   format = \"single\",\n                                   sep = \",\",\n                                   header = TRUE,\n                                   cols = c(\"id_compra\", \"item\"),\n                                   rm.duplicates = TRUE)\ntransacciones\n#> transactions in sparse format with\n#>  9835 transactions (rows) and\n#>  169 items (columns)\n\ncolnames(transacciones)[1:5]\n#> [1] \"abrasive cleaner\" \"artif. sweetener\" \"baby cosmetics\"  \n#> [4] \"baby food\"        \"bags\"\n\nrownames(transacciones)[1:5]\n#> [1] \"1\"    \"10\"   \"100\"  \"1000\" \"1001\"\n# CONVERSIÓN DE UN DATAFRAME A UN OBJETO TIPO TRANSACTION\n# ==============================================================================\n# Se convierte el dataframe a una lista en la que cada elemento  contiene los\n# items de una transacción\ndatos_split <- split(x = datos$item, f = datos$id_compra)\ntransacciones <- as(datos_split, Class = \"transactions\")\ntransacciones\n#> transactions in sparse format with\n#>  9835 transactions (rows) and\n#>  169 items (columns)\n\n\n# CONVERSIÓN DE UNA MATRIZ A UN OBJETO TIPO TRANSACTION\n# ==============================================================================\ndatos_matriz <- datos %>%\n                as.data.frame() %>%\n                mutate(valor = 1) %>%\n                spread(key = item, value = valor, fill = 0) %>%\n                column_to_rownames(var = \"id_compra\") %>%\n                as.matrix()\ntransacciones <- as(datos_matriz, Class = \"transactions\")\ntransacciones\n#> transactions in sparse format with\n#>  9835 transactions (rows) and\n#>  169 items (columns)\ninspect(transacciones[1:5])\n#>     items                      transactionID\n#> [1] {citrus fruit,                          \n#>      margarine,                             \n#>      ready soups,                           \n#>      semi-finished bread}                  1\n#> [2] {coffee,                                \n#>      tropical fruit,                        \n#>      yogurt}                               2\n#> [3] {whole milk}                           3\n#> [4] {cream cheese,                          \n#>      meat spreads,                          \n#>      pip fruit,                             \n#>      yogurt}                               4\n#> [5] {condensed milk,                        \n#>      long life bakery product,              \n#>      other vegetables,                      \n#>      whole milk}                           5\ndf_transacciones <- as(transacciones, Class = \"data.frame\")\n# Para que el tamaño de la tabla se ajuste mejor, se convierte el dataframe a tibble\nas.tibble(df_transacciones) %>% head()\n#> Warning: `as.tibble()` was deprecated in tibble 2.0.0.\n#> ℹ Please use `as_tibble()` instead.\n#> ℹ The signature and semantics have changed, see\n#>   `?as_tibble`.\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_lifecycle_warnings()` to see where\n#> this warning was generated.\n#> # A tibble: 6 × 2\n#>   items                                        transactionID\n#>   <chr>                                        <chr>        \n#> 1 {citrus fruit,margarine,ready soups,semi-fi… 1            \n#> 2 {coffee,tropical fruit,yogurt}               2            \n#> 3 {whole milk}                                 3            \n#> 4 {cream cheese,meat spreads,pip fruit,yogurt} 4            \n#> 5 {condensed milk,long life bakery product,ot… 5            \n#> 6 {abrasive cleaner,butter,rice,whole milk,yo… 6\ntamanyos <- size(transacciones)\nsummary(tamanyos)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   1.000   2.000   3.000   4.409   6.000  32.000\nquantile(tamanyos, probs = seq(0,1,0.1))\n#>   0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n#>    1    1    1    2    3    3    4    5    7    9   32\ndata.frame(tamanyos) %>%\n  ggplot(aes(x = tamanyos)) +\n  geom_histogram() +\n  labs(title = \"Distribución del tamaño de las transacciones\",\n       x = \"Tamaño\") +\n  theme_bw()\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\nfrecuencia_items <- itemFrequency(x = transacciones, type = \"relative\")\nfrecuencia_items %>% sort(decreasing = TRUE) %>% head(5)\n#>       whole milk other vegetables       rolls/buns \n#>        0.2555160        0.1934926        0.1839349 \n#>             soda           yogurt \n#>        0.1743772        0.1395018\nfrecuencia_items <- itemFrequency(x = transacciones, type = \"absolute\")\nfrecuencia_items %>% sort(decreasing = TRUE) %>% head(5)\n#>       whole milk other vegetables       rolls/buns \n#>             2513             1903             1809 \n#>             soda           yogurt \n#>             1715             1372"},{"path":"patrones-de-asociación.html","id":"itemsets","chapter":"Capítulo 8 Patrones de asociación","heading":"8.9.2 Itemsets","text":"Con la función apriori() se puede aplicar el algoritmo Apriori un objeto de tipo transactions y extraer tanto itemsets frecuentes como reglas de asociación que superen un determinado soporte y confianza. Los argumentos de esta función son:data: un objeto del tipo transactions o un objeto que pueda ser convertido tipo transactions, por ejemplo un dataframe o una matriz binaria.parameter: lista en la que se indican los parámetros del algoritmo.support: soporte mínimo que debe tener un itemset para ser considerado frecuente. Por defecto es 0.1.minlen: número mínimo de items que debe tener un itemset para ser incluido en los resultados. Por defecto 1.maxlen: número máximo de items que puede tener un itemset para ser incluido en los resultados. Por defecto 10.target: tipo de resultados que debe de generar el algoritmo, pueden ser “frequent itemsets”, “maximally frequent itemsets”, “closed frequent itemsets”, “rules” o “hyperedgesets”.confidence: confianza mínima que debe de tener una regla para ser incluida en los resultados. Por defecto 0.8.maxtime: tiempo máximo que puede estar el algoritmo buscando subsets. Por defecto 5 segundos.appearance: lista que permite definir patrones para restringir el espacio de búsqueda, por ejemplo, especificando qué items pueden o pueden aparecer.control: lista que permite modificar aspectos internos de algoritmo como la ordenación de los itemsets, si se construye un árbol con las transacciones, aspectos relacionados con el uso de memoria, etc.Se procede extraer aquellos itemsets, incluidos los formados por un único item, que hayan sido comprados al menos 30 veces. En un caso real, este valor sería excesivamente bajo si se tiene en cuenta la cantidad total de transacciones, sin embargo, se emplea 30 para que en los resultados aparezcan un número suficiente de itemsets y reglas de asociación que permitan mostrar las posibilidades de análisis que ofrece el paquete arules.Se han encontrado un total de 2226 itemsets frecuentes que superan el soporte mínimo de 0.003908286, la mayoría de ellos (1140) formados por dos items. En el siguiente listado se muestran los 20 itemsets con mayor soporte que, como cabe esperar, son los formados por items individuales (los itemsets de menor tamaño).Se muestran los top 20 itemsets de mayor menor soporteSi se quieren excluir del análisis los itemsets formados únicamente por un solo item, se puede, o bien aplicar de nuevo la función apriori() especificando minlen = 2, o filtrar los resultados con la función size().\nSe muestran los 20 itemsets más frecuentes formados por más de un item.","code":"\nsoporte <- 30 / dim(transacciones)[1]\nitemsets <- apriori(data = transacciones,\n                    parameter = list(support = soporte,\n                                     minlen = 1,\n                                     maxlen = 20,\n                                     target = \"frequent itemset\"))\n#> Apriori\n#> \n#> Parameter specification:\n#>  confidence minval smax arem  aval originalSupport maxtime\n#>          NA    0.1    1 none FALSE            TRUE       5\n#>     support minlen maxlen            target  ext\n#>  0.00305033      1     20 frequent itemsets TRUE\n#> \n#> Algorithmic control:\n#>  filter tree heap memopt load sort verbose\n#>     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n#> \n#> Absolute minimum support count: 30 \n#> \n#> set item appearances ...[0 item(s)] done [0.00s].\n#> set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].\n#> sorting and recoding items ... [136 item(s)] done [0.00s].\n#> creating transaction tree ... done [0.00s].\n#> checking subsets of size 1 2 3 4 5 done [0.01s].\n#> sorting transactions ... done [0.00s].\n#> writing ... [2226 set(s)] done [0.00s].\n#> creating S4 object  ... done [0.00s].\nsummary(itemsets)\n#> set of 2226 itemsets\n#> \n#> most frequent items:\n#>       whole milk other vegetables           yogurt \n#>              556              468              316 \n#>  root vegetables       rolls/buns          (Other) \n#>              251              241             3536 \n#> \n#> element (itemset/transaction) length distribution:sizes\n#>    1    2    3    4    5 \n#>  136 1140  850   98    2 \n#> \n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   1.000   2.000   2.000   2.412   3.000   5.000 \n#> \n#> summary of quality measures:\n#>     support         transIdenticalToItemsets\n#>  Min.   :0.003050   Min.   :0.0000000       \n#>  1st Qu.:0.003660   1st Qu.:0.0000000       \n#>  Median :0.004779   Median :0.0000000       \n#>  Mean   :0.007879   Mean   :0.0001613       \n#>  3rd Qu.:0.007219   3rd Qu.:0.0001017       \n#>  Max.   :0.255516   Max.   :0.0265379       \n#>      count        \n#>  Min.   :  30.00  \n#>  1st Qu.:  36.00  \n#>  Median :  47.00  \n#>  Mean   :  77.49  \n#>  3rd Qu.:  71.00  \n#>  Max.   :2513.00  \n#> \n#> includes transaction ID lists: FALSE \n#> \n#> mining info:\n#>           data ntransactions    support confidence\n#>  transacciones          9835 0.00305033          1\ntop_20_itemsets <- sort(itemsets, by = \"support\", decreasing = TRUE)[1:20]\ninspect(top_20_itemsets)\n#>      items                         support   \n#> [1]  {whole milk}                  0.25551601\n#> [2]  {other vegetables}            0.19349263\n#> [3]  {rolls/buns}                  0.18393493\n#> [4]  {soda}                        0.17437722\n#> [5]  {yogurt}                      0.13950178\n#> [6]  {bottled water}               0.11052364\n#> [7]  {root vegetables}             0.10899847\n#> [8]  {tropical fruit}              0.10493137\n#> [9]  {shopping bags}               0.09852567\n#> [10] {sausage}                     0.09395018\n#> [11] {pastry}                      0.08896797\n#> [12] {citrus fruit}                0.08276563\n#> [13] {bottled beer}                0.08052872\n#> [14] {newspapers}                  0.07981698\n#> [15] {canned beer}                 0.07768175\n#> [16] {pip fruit}                   0.07564820\n#> [17] {other vegetables,whole milk} 0.07483477\n#> [18] {fruit/vegetable juice}       0.07229283\n#> [19] {whipped/sour cream}          0.07168277\n#> [20] {brown bread}                 0.06487036\n#>      transIdenticalToItemsets count\n#> [1]  0.0125063549             2513 \n#> [2]  0.0063040163             1903 \n#> [3]  0.0111845450             1809 \n#> [4]  0.0162684291             1715 \n#> [5]  0.0040671073             1372 \n#> [6]  0.0068124047             1087 \n#> [7]  0.0025419420             1072 \n#> [8]  0.0023385867             1032 \n#> [9]  0.0048805287              969 \n#> [10] 0.0024402644              924 \n#> [11] 0.0037620742              875 \n#> [12] 0.0017285206              814 \n#> [13] 0.0124046772              792 \n#> [14] 0.0055922725              785 \n#> [15] 0.0265378749              764 \n#> [16] 0.0028469751              744 \n#> [17] 0.0006100661              736 \n#> [18] 0.0022369090              711 \n#> [19] 0.0026436197              705 \n#> [20] 0.0021352313              638\nas(top_20_itemsets, Class = \"data.frame\") %>%\n  ggplot(aes(x = reorder(items, support), y = support)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Itemsets más frecuentes\", x = \"itemsets\") +\n  theme_bw()\ninspect(sort(itemsets[size(itemsets) > 1], decreasing = TRUE)[1:20])\n#>      items                              support   \n#> [1]  {other vegetables,whole milk}      0.07483477\n#> [2]  {rolls/buns,whole milk}            0.05663447\n#> [3]  {whole milk,yogurt}                0.05602440\n#> [4]  {root vegetables,whole milk}       0.04890696\n#> [5]  {other vegetables,root vegetables} 0.04738180\n#> [6]  {other vegetables,yogurt}          0.04341637\n#> [7]  {other vegetables,rolls/buns}      0.04260295\n#> [8]  {tropical fruit,whole milk}        0.04229792\n#> [9]  {soda,whole milk}                  0.04006101\n#> [10] {rolls/buns,soda}                  0.03833249\n#> [11] {other vegetables,tropical fruit}  0.03589222\n#> [12] {bottled water,whole milk}         0.03436706\n#> [13] {rolls/buns,yogurt}                0.03436706\n#> [14] {pastry,whole milk}                0.03324860\n#> [15] {other vegetables,soda}            0.03274021\n#> [16] {whipped/sour cream,whole milk}    0.03223183\n#> [17] {rolls/buns,sausage}               0.03060498\n#> [18] {citrus fruit,whole milk}          0.03050330\n#> [19] {pip fruit,whole milk}             0.03009659\n#> [20] {domestic eggs,whole milk}         0.02999492\n#>      transIdenticalToItemsets count\n#> [1]  0.0006100661             736  \n#> [2]  0.0013218099             557  \n#> [3]  0.0008134215             551  \n#> [4]  0.0003050330             481  \n#> [5]  0.0006100661             466  \n#> [6]  0.0004067107             427  \n#> [7]  0.0008134215             419  \n#> [8]  0.0008134215             416  \n#> [9]  0.0007117438             394  \n#> [10] 0.0023385867             377  \n#> [11] 0.0005083884             353  \n#> [12] 0.0007117438             338  \n#> [13] 0.0010167768             338  \n#> [14] 0.0012201322             327  \n#> [15] 0.0001016777             322  \n#> [16] 0.0008134215             317  \n#> [17] 0.0013218099             301  \n#> [18] 0.0003050330             300  \n#> [19] 0.0004067107             296  \n#> [20] 0.0003050330             295"},{"path":"patrones-de-asociación.html","id":"filtrado-de-itemsets","chapter":"Capítulo 8 Patrones de asociación","heading":"8.10 Filtrado de itemsets","text":"Una vez que los itemsets frecuentes han sido identificados mediante el algoritmo Apripori, pueden ser filtrados con la función subset(). Esta función recibe dos argumentos: un objeto itemset o rules y una condición lógica que tienen que cumplir las reglas/itemsets para ser seleccionados. La siguiente tabla muestra los operadores permitidos:Como esta función tiene el mismo nombre que una función del paquete básico de R, para evitar errores, es conveniente especificar el paquete donde se encuentra.Se procede identificar aquellos itemsets frecuentes que contienen el item newspapers.Se repite el proceso pero, esta vez, con aquellos itemsets que contienen newspapers y whole milk.Puede observarse que muchos itemsets están su vez contenidos en itemsets de orden superior, es decir, existen itemsets que son subsets de otros. Para identificar cuáles son, o cuales lo son, se puede emplear la función .subset(). Encontrar los itemsets que son subsets de otros itemsets implica comparar todos los pares de itemsets y determinar si uno está contenido en el otro. La función .subset() realiza comparaciones entre dos conjuntos de itemsets y devuelve una matriz lógica que determina si el itemset de la fila está contenido en cada itemset de las columnas.Para encontrar los subsets dentro de un conjunto de itemsets, se compara el\nconjunto de itemsets con sigo mismo.","code":"\nitemsets_filtrado <- arules::subset(itemsets,\n                                    subset = items %in% \"newspapers\")\nitemsets_filtrado\n#> set of 80 itemsets\ninspect(itemsets_filtrado[1:10])\n#>      items                                 support    \n#> [1]  {newspapers}                          0.079816980\n#> [2]  {meat,newspapers}                     0.003050330\n#> [3]  {newspapers,sliced cheese}            0.003152008\n#> [4]  {newspapers,UHT-milk}                 0.004270463\n#> [5]  {newspapers,oil}                      0.003152008\n#> [6]  {newspapers,onions}                   0.003152008\n#> [7]  {hygiene articles,newspapers}         0.003050330\n#> [8]  {newspapers,sugar}                    0.003152008\n#> [9]  {newspapers,waffles}                  0.004168785\n#> [10] {long life bakery product,newspapers} 0.003457041\n#>      transIdenticalToItemsets count\n#> [1]  0.0055922725             785  \n#> [2]  0.0001016777              30  \n#> [3]  0.0000000000              31  \n#> [4]  0.0001016777              42  \n#> [5]  0.0000000000              31  \n#> [6]  0.0000000000              31  \n#> [7]  0.0000000000              30  \n#> [8]  0.0000000000              31  \n#> [9]  0.0001016777              41  \n#> [10] 0.0002033554              34\nitemsets_filtrado <- arules::subset(itemsets,\n                                    subset = items %ain% c(\"newspapers\", \"whole milk\"))\nitemsets_filtrado\n#> set of 16 itemsets\n\ninspect(itemsets_filtrado[1:10])\n#>      items                                  support    \n#> [1]  {newspapers,whole milk}                0.027351296\n#> [2]  {chocolate,newspapers,whole milk}      0.003152008\n#> [3]  {brown bread,newspapers,whole milk}    0.004067107\n#> [4]  {margarine,newspapers,whole milk}      0.003152008\n#> [5]  {butter,newspapers,whole milk}         0.003152008\n#> [6]  {newspapers,pastry,whole milk}         0.003863752\n#> [7]  {citrus fruit,newspapers,whole milk}   0.003355363\n#> [8]  {newspapers,sausage,whole milk}        0.003050330\n#> [9]  {bottled water,newspapers,whole milk}  0.004067107\n#> [10] {newspapers,tropical fruit,whole milk} 0.005083884\n#>      transIdenticalToItemsets count\n#> [1]  0.0008134215             269  \n#> [2]  0.0000000000              31  \n#> [3]  0.0002033554              40  \n#> [4]  0.0000000000              31  \n#> [5]  0.0001016777              31  \n#> [6]  0.0003050330              38  \n#> [7]  0.0000000000              33  \n#> [8]  0.0000000000              30  \n#> [9]  0.0001016777              40  \n#> [10] 0.0001016777              50\nsubsets <- is.subset(x = itemsets, y = itemsets, sparse = FALSE)\n\n# Para conocer el total de itemsets que son subsets de otros itemsets se cuenta el número total de TRUE en la matriz resultante.\n\n# La suma de una matriz lógica devuelve el número de TRUEs\nsum(subsets)\n#> [1] 11038"},{"path":"patrones-de-asociación.html","id":"reglas-de-asociación","chapter":"Capítulo 8 Patrones de asociación","heading":"8.11 Reglas de asociación","text":"Para crear las reglas de asociación se sigue el mismo proceso que para obtener itemsets frecuentes pero, además de especificar un soporte mínimo, se tiene que establecer una confianza mínima para que una regla se incluya en los resultados. En este caso, se emplea una confianza mínima del 70%.Se han identificado un total de 19 reglas, la mayoría de ellas formadas por 4 items en el antecedente (parte izquierda de la regla).","code":"\nsoporte <- 30 / dim(transacciones)[1]\nreglas <- apriori(data = transacciones,\n                  parameter = list(support = soporte,\n                                   confidence = 0.70,\n                                   # Se especifica que se creen reglas\n                                   target = \"rules\"))\n#> Apriori\n#> \n#> Parameter specification:\n#>  confidence minval smax arem  aval originalSupport maxtime\n#>         0.7    0.1    1 none FALSE            TRUE       5\n#>     support minlen maxlen target  ext\n#>  0.00305033      1     10  rules TRUE\n#> \n#> Algorithmic control:\n#>  filter tree heap memopt load sort verbose\n#>     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n#> \n#> Absolute minimum support count: 30 \n#> \n#> set item appearances ...[0 item(s)] done [0.00s].\n#> set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].\n#> sorting and recoding items ... [136 item(s)] done [0.00s].\n#> creating transaction tree ... done [0.00s].\n#> checking subsets of size 1 2 3 4 5 done [0.00s].\n#> writing ... [19 rule(s)] done [0.00s].\n#> creating S4 object  ... done [0.00s].\n\nreglas\n#> set of 19 rules\nsummary(reglas)\n#> set of 19 rules\n#> \n#> rule length distribution (lhs + rhs):sizes\n#> 3 4 5 \n#> 7 9 3 \n#> \n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   3.000   3.000   4.000   3.789   4.000   5.000 \n#> \n#> summary of quality measures:\n#>     support           confidence        coverage       \n#>  Min.   :0.003050   Min.   :0.7000   Min.   :0.003559  \n#>  1st Qu.:0.003203   1st Qu.:0.7047   1st Qu.:0.004525  \n#>  Median :0.003559   Median :0.7164   Median :0.004982  \n#>  Mean   :0.003767   Mean   :0.7373   Mean   :0.005143  \n#>  3rd Qu.:0.004169   3rd Qu.:0.7500   3rd Qu.:0.005592  \n#>  Max.   :0.005694   Max.   :0.8857   Max.   :0.008134  \n#>       lift           count      \n#>  Min.   :2.740   Min.   :30.00  \n#>  1st Qu.:2.758   1st Qu.:31.50  \n#>  Median :2.804   Median :35.00  \n#>  Mean   :3.044   Mean   :37.05  \n#>  3rd Qu.:2.984   3rd Qu.:41.00  \n#>  Max.   :4.578   Max.   :56.00  \n#> \n#> mining info:\n#>           data ntransactions    support confidence\n#>  transacciones          9835 0.00305033        0.7\ninspect(sort(x = reglas, decreasing = TRUE, by = \"confidence\"))\n#>      lhs                     rhs                    support confidence    coverage     lift count\n#> [1]  {citrus fruit,                                                                              \n#>       root vegetables,                                                                           \n#>       tropical fruit,                                                                            \n#>       whole milk}         => {other vegetables} 0.003152008  0.8857143 0.003558719 4.577509    31\n#> [2]  {butter,                                                                                    \n#>       root vegetables,                                                                           \n#>       yogurt}             => {whole milk}       0.003050330  0.7894737 0.003863752 3.089723    30\n#> [3]  {citrus fruit,                                                                              \n#>       root vegetables,                                                                           \n#>       tropical fruit}     => {other vegetables} 0.004473818  0.7857143 0.005693950 4.060694    44\n#> [4]  {brown bread,                                                                               \n#>       other vegetables,                                                                          \n#>       root vegetables}    => {whole milk}       0.003152008  0.7750000 0.004067107 3.033078    31\n#> [5]  {butter,                                                                                    \n#>       onions}             => {whole milk}       0.003050330  0.7500000 0.004067107 2.935237    30\n#> [6]  {curd,                                                                                      \n#>       tropical fruit,                                                                            \n#>       yogurt}             => {whole milk}       0.003965430  0.7500000 0.005287239 2.935237    39\n#> [7]  {curd,                                                                                      \n#>       domestic eggs}      => {whole milk}       0.004778851  0.7343750 0.006507372 2.874086    47\n#> [8]  {butter,                                                                                    \n#>       tropical fruit,                                                                            \n#>       yogurt}             => {whole milk}       0.003355363  0.7333333 0.004575496 2.870009    33\n#> [9]  {root vegetables,                                                                           \n#>       tropical fruit,                                                                            \n#>       whipped/sour cream} => {other vegetables} 0.003355363  0.7333333 0.004575496 3.789981    33\n#> [10] {butter,                                                                                    \n#>       curd}               => {whole milk}       0.004880529  0.7164179 0.006812405 2.803808    48\n#> [11] {domestic eggs,                                                                             \n#>       sugar}              => {whole milk}       0.003558719  0.7142857 0.004982206 2.795464    35\n#> [12] {other vegetables,                                                                          \n#>       root vegetables,                                                                           \n#>       tropical fruit,                                                                            \n#>       yogurt}             => {whole milk}       0.003558719  0.7142857 0.004982206 2.795464    35\n#> [13] {baking powder,                                                                             \n#>       yogurt}             => {whole milk}       0.003253686  0.7111111 0.004575496 2.783039    32\n#> [14] {tropical fruit,                                                                            \n#>       whipped/sour cream,                                                                        \n#>       yogurt}             => {whole milk}       0.004372140  0.7049180 0.006202339 2.758802    43\n#> [15] {citrus fruit,                                                                              \n#>       other vegetables,                                                                          \n#>       root vegetables,                                                                           \n#>       tropical fruit}     => {whole milk}       0.003152008  0.7045455 0.004473818 2.757344    31\n#> [16] {butter,                                                                                    \n#>       pork}               => {whole milk}       0.003863752  0.7037037 0.005490595 2.754049    38\n#> [17] {butter,                                                                                    \n#>       coffee}             => {whole milk}       0.003355363  0.7021277 0.004778851 2.747881    33\n#> [18] {domestic eggs,                                                                             \n#>       other vegetables,                                                                          \n#>       whipped/sour cream} => {whole milk}       0.003558719  0.7000000 0.005083884 2.739554    35\n#> [19] {root vegetables,                                                                           \n#>       tropical fruit,                                                                            \n#>       yogurt}             => {whole milk}       0.005693950  0.7000000 0.008134215 2.739554    56"},{"path":"patrones-de-asociación.html","id":"evaluación-de-las-reglas","chapter":"Capítulo 8 Patrones de asociación","heading":"8.12 Evaluación de las reglas","text":"Además de la confianza y el soporte, existen otras métricas que permiten cuantificar la calidad de las reglas y la probabilidad de que reflejen relaciones reales. Algunas de las más empleadas son:Lift: el estadístico lift compara la frecuencia observada de una regla con la frecuencia esperada simplemente por azar (si la regla existe realmente). El valor lift de una regla “si X, entonces Y” se obtiene acorde la siguiente ecuación:\nsoporte(union(X,Y))soporte(X) * soporte(Y)\nCuanto más se aleje el valor de lift de 1, más evidencias de que la regla se debe un artefacto aleatorio, es decir, mayor la evidencia de que la regla representa un patrón real.Coverage: es el soporte de la parte izquierda de la regla (antecedente). Se interpreta como la frecuencia con la que el antecedente aparece en el conjunto de transacciones.Fisher exact test: devuelve el p-value asociado la probabilidad de observar la regla solo por azar.Con la función interestMeasure() se pueden calcular más de 20 métricas distintas para un conjunto de reglas creadas con la función apriori().Estas nuevas métricas pueden añadirse al objeto que contiene las reglas.","code":"\nmetricas <- interestMeasure(reglas, measure = c(\"coverage\", \"fishersExactTest\"),\n                            transactions = transacciones)\nmetricas\n#>       coverage fishersExactTest\n#> 1  0.004575496     1.775138e-10\n#> 2  0.004067107     7.502990e-11\n#> 3  0.004982206     1.990017e-11\n#> 4  0.004778851     1.582670e-10\n#> 5  0.006812405     2.857678e-15\n#> 6  0.006507372     1.142131e-15\n#> 7  0.005490595     5.673757e-12\n#> 8  0.005287239     1.003422e-13\n#> 9  0.004067107     8.092894e-12\n#> 10 0.004575496     2.336708e-11\n#> 11 0.003863752     7.584014e-12\n#> 12 0.005083884     5.006888e-11\n#> 13 0.004575496     5.680562e-15\n#> 14 0.006202339     2.037822e-13\n#> 15 0.005693950     1.301061e-21\n#> 16 0.008134215     7.433712e-17\n#> 17 0.004473818     5.003096e-10\n#> 18 0.003558719     1.459542e-18\n#> 19 0.004982206     1.990017e-11\ncoverage\n#> standardGeneric for \"coverage\" defined from package \"arules\"\n#> \n#> function (x, transactions = NULL, reuse = TRUE) \n#> standardGeneric(\"coverage\")\n#> <bytecode: 0x60981d3afd48>\n#> <environment: 0x60981d3aa020>\n#> Methods may be defined for arguments: x, transactions, reuse\n#> Use  showMethods(coverage)  for currently available ones.\nlibrary(arules)\nquality(reglas) <- cbind(quality(reglas), metricas)\n# inspect(sort(x = reglas, decreasing = TRUE, by = \"confidence\"))\ndf_reglas <- as(reglas, Class = \"data.frame\") \ndf_reglas %>% as.tibble() %>% arrange(desc(confidence)) %>% head()\n#> # A tibble: 6 × 8\n#>   rules   support confidence coverage  lift count coverage.1\n#>   <chr>     <dbl>      <dbl>    <dbl> <dbl> <int>      <dbl>\n#> 1 {citru… 0.00315      0.886  0.00356  4.58    31    0.00356\n#> 2 {butte… 0.00305      0.789  0.00386  3.09    30    0.00386\n#> 3 {citru… 0.00447      0.786  0.00569  4.06    44    0.00569\n#> 4 {brown… 0.00315      0.775  0.00407  3.03    31    0.00407\n#> 5 {butte… 0.00305      0.75   0.00407  2.94    30    0.00407\n#> 6 {curd,… 0.00397      0.75   0.00529  2.94    39    0.00529\n#> # ℹ 1 more variable: fishersExactTest <dbl>\n#rules"},{"path":"patrones-de-asociación.html","id":"filtrado-de-reglas","chapter":"Capítulo 8 Patrones de asociación","heading":"8.13 Filtrado de reglas","text":"Cuando se crean reglas de asociación, pueden ser interesantes únicamente aquellas que contienen un determinado conjunto de items en el antecedente o en el consecuente. Con arules existen varias formas de seleccionar solo determinadas reglas.","code":""},{"path":"patrones-de-asociación.html","id":"restringir-las-reglas-que-se-crean","chapter":"Capítulo 8 Patrones de asociación","heading":"8.13.1 Restringir las reglas que se crean","text":"Es posible restringir los items que aparecen en el lado izquierdo y/o derecho de la reglas la hora de crearlas, por ejemplo, supóngase que solo son de interés reglas que muestren productos que se vendan junto con vegetables. Esto significa que el item vegetables, debe aparecer en el lado derecho (rhs).Esto mismo puede hacerse con el lado izquierdo (lhs) o en ambos ().","code":"\nsoporte <- 30 / dim(transacciones)[1]\nreglas_vegetables <- apriori(data = transacciones,\n                             parameter = list(support = soporte,\n                                              confidence = 0.70,\n                                              # Se especifica que se creen reglas\n                                              target = \"rules\"),\n                             appearance = list(rhs = \"other vegetables\"))\n#> Apriori\n#> \n#> Parameter specification:\n#>  confidence minval smax arem  aval originalSupport maxtime\n#>         0.7    0.1    1 none FALSE            TRUE       5\n#>     support minlen maxlen target  ext\n#>  0.00305033      1     10  rules TRUE\n#> \n#> Algorithmic control:\n#>  filter tree heap memopt load sort verbose\n#>     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n#> \n#> Absolute minimum support count: 30 \n#> \n#> set item appearances ...[1 item(s)] done [0.00s].\n#> set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].\n#> sorting and recoding items ... [136 item(s)] done [0.00s].\n#> creating transaction tree ... done [0.00s].\n#> checking subsets of size 1 2 3 4 5 done [0.00s].\n#> writing ... [3 rule(s)] done [0.00s].\n#> creating S4 object  ... done [0.00s].\nsummary(reglas_vegetables)\n#> set of 3 rules\n#> \n#> rule length distribution (lhs + rhs):sizes\n#> 4 5 \n#> 2 1 \n#> \n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   4.000   4.000   4.000   4.333   4.500   5.000 \n#> \n#> summary of quality measures:\n#>     support           confidence        coverage       \n#>  Min.   :0.003152   Min.   :0.7333   Min.   :0.003559  \n#>  1st Qu.:0.003254   1st Qu.:0.7595   1st Qu.:0.004067  \n#>  Median :0.003355   Median :0.7857   Median :0.004575  \n#>  Mean   :0.003660   Mean   :0.8016   Mean   :0.004609  \n#>  3rd Qu.:0.003915   3rd Qu.:0.8357   3rd Qu.:0.005135  \n#>  Max.   :0.004474   Max.   :0.8857   Max.   :0.005694  \n#>       lift           count     \n#>  Min.   :3.790   Min.   :31.0  \n#>  1st Qu.:3.925   1st Qu.:32.0  \n#>  Median :4.061   Median :33.0  \n#>  Mean   :4.143   Mean   :36.0  \n#>  3rd Qu.:4.319   3rd Qu.:38.5  \n#>  Max.   :4.578   Max.   :44.0  \n#> \n#> mining info:\n#>           data ntransactions    support confidence\n#>  transacciones          9835 0.00305033        0.7\ninspect(reglas_vegetables)\n#>     lhs                     rhs                    support confidence    coverage     lift count\n#> [1] {root vegetables,                                                                           \n#>      tropical fruit,                                                                            \n#>      whipped/sour cream} => {other vegetables} 0.003355363  0.7333333 0.004575496 3.789981    33\n#> [2] {citrus fruit,                                                                              \n#>      root vegetables,                                                                           \n#>      tropical fruit}     => {other vegetables} 0.004473818  0.7857143 0.005693950 4.060694    44\n#> [3] {citrus fruit,                                                                              \n#>      root vegetables,                                                                           \n#>      tropical fruit,                                                                            \n#>      whole milk}         => {other vegetables} 0.003152008  0.8857143 0.003558719 4.577509    31"},{"path":"patrones-de-asociación.html","id":"filtrar-reglas-creadas","chapter":"Capítulo 8 Patrones de asociación","heading":"8.14 Filtrar reglas creadas","text":"También es posible filtrar las reglas una vez que han sido creadas. Por ejemplo, se procede filtrar aquellas reglas que contienen vegetables y citrus fruit en el antecedente.","code":"\nfiltrado_reglas <- subset(x = reglas,\n                          subset = lhs %ain% c(\"other vegetables\",\"citrus fruit\"))\ninspect(filtrado_reglas)\n#>     lhs                   rhs              support confidence    coverage     lift count    coverage fishersExactTest\n#> [1] {citrus fruit,                                                                                                   \n#>      other vegetables,                                                                                               \n#>      root vegetables,                                                                                                \n#>      tropical fruit}   => {whole milk} 0.003152008  0.7045455 0.004473818 2.757344    31 0.004473818     5.003096e-10"},{"path":"patrones-de-asociación.html","id":"reglas-maximales","chapter":"Capítulo 8 Patrones de asociación","heading":"8.15 Reglas maximales","text":"Un itemset es maximal si existe otro itemset que sea su superset. Una regla de asociación se define como regla maximal si está generada con un itemset maximal. Con la función .maximal() se pueden identificar las reglas maximales.","code":"\nreglas_maximales <- reglas[is.maximal(reglas)]\nreglas_maximales\n#> set of 17 rules\n\ninspect(reglas_maximales[1:10])\n#>      lhs                   rhs              support confidence    coverage     lift count    coverage fishersExactTest\n#> [1]  {baking powder,                                                                                                  \n#>       yogurt}           => {whole milk} 0.003253686  0.7111111 0.004575496 2.783039    32 0.004575496     1.775138e-10\n#> [2]  {butter,                                                                                                         \n#>       onions}           => {whole milk} 0.003050330  0.7500000 0.004067107 2.935237    30 0.004067107     7.502990e-11\n#> [3]  {domestic eggs,                                                                                                  \n#>       sugar}            => {whole milk} 0.003558719  0.7142857 0.004982206 2.795464    35 0.004982206     1.990017e-11\n#> [4]  {butter,                                                                                                         \n#>       coffee}           => {whole milk} 0.003355363  0.7021277 0.004778851 2.747881    33 0.004778851     1.582670e-10\n#> [5]  {butter,                                                                                                         \n#>       curd}             => {whole milk} 0.004880529  0.7164179 0.006812405 2.803808    48 0.006812405     2.857678e-15\n#> [6]  {curd,                                                                                                           \n#>       domestic eggs}    => {whole milk} 0.004778851  0.7343750 0.006507372 2.874086    47 0.006507372     1.142131e-15\n#> [7]  {butter,                                                                                                         \n#>       pork}             => {whole milk} 0.003863752  0.7037037 0.005490595 2.754049    38 0.005490595     5.673757e-12\n#> [8]  {curd,                                                                                                           \n#>       tropical fruit,                                                                                                 \n#>       yogurt}           => {whole milk} 0.003965430  0.7500000 0.005287239 2.935237    39 0.005287239     1.003422e-13\n#> [9]  {brown bread,                                                                                                    \n#>       other vegetables,                                                                                               \n#>       root vegetables}  => {whole milk} 0.003152008  0.7750000 0.004067107 3.033078    31 0.004067107     8.092894e-12\n#> [10] {butter,                                                                                                         \n#>       tropical fruit,                                                                                                 \n#>       yogurt}           => {whole milk} 0.003355363  0.7333333 0.004575496 2.870009    33 0.004575496     2.336708e-11"},{"path":"patrones-de-asociación.html","id":"reglas-redundantes","chapter":"Capítulo 8 Patrones de asociación","heading":"8.16 Reglas redundantes","text":"Dos reglas son idénticas si tienen el mismo antecedente (parte izquierda) y consecuente (parte derecha). Supóngase ahora que una de estas reglas tiene en su antecedente los mismos items que forman el antecedente de la otra, junto con algunos items más. La regla más genérica se considera redundante, ya que aporta información adicional. En concreto, se considera que una regla X => Y es redundante si existe un subset X’ tal que existe una regla X’ => Y cuyo soporte es mayor.\\(X => Y\\) es redundante si existe un subset X’ tal que: conf(X’ -> Y) >= conf(X -> Y)Para este ejemplo se detectan reglas redundantes.","code":"\nreglas_redundantes <- reglas[is.redundant(x = reglas, measure = \"confidence\")]\nreglas_redundantes\n#> set of 0 rules"},{"path":"patrones-de-asociación.html","id":"transacciones-que-verifican-una-determinada-regla","chapter":"Capítulo 8 Patrones de asociación","heading":"8.17 Transacciones que verifican una determinada regla","text":"Una vez identificada una determinada regla, puede ser interesante recuperar todas aquellas transacciones en las que se cumple. continuación, se recuperan aquellas transacciones para las que se cumple la regla con mayor confianza de entre todas las encontradas.Las transacciones que cumplen esta regla son todas aquellas que contienen los items: citrus fruit, root vegetables, tropical fruit, whole milk y vegetables.","code":"\n# Se identifica la regla con mayor confianza\nas(reglas, \"data.frame\") %>%\n  arrange(desc(confidence)) %>%\n  head(1) %>%\n  pull(rules)\n#> [1] \"{citrus fruit,root vegetables,tropical fruit,whole milk} => {other vegetables}\"\nfiltrado_transacciones <- subset(x = transacciones,\n                                 subset = items %ain% c(\"citrus fruit\",\n                                                        \"root vegetables\",\n                                                        \"tropical fruit\",\n                                                        \"whole milk\",\n                                                        \"other vegetables\"))\nfiltrado_transacciones\n#> transactions in sparse format with\n#>  31 transactions (rows) and\n#>  169 items (columns)\n\ninspect(filtrado_transacciones[1:3])\n#>     items                   transactionID\n#> [1] {berries,                            \n#>      bottled water,                      \n#>      butter,                             \n#>      citrus fruit,                       \n#>      hygiene articles,                   \n#>      napkins,                            \n#>      other vegetables,                   \n#>      root vegetables,                    \n#>      rubbing alcohol,                    \n#>      tropical fruit,                     \n#>      whole milk}                     596 \n#> [2] {bottled water,                      \n#>      citrus fruit,                       \n#>      curd,                               \n#>      dessert,                            \n#>      frozen meals,                       \n#>      frozen vegetables,                  \n#>      fruit/vegetable juice,              \n#>      grapes,                             \n#>      napkins,                            \n#>      other vegetables,                   \n#>      pip fruit,                          \n#>      root vegetables,                    \n#>      specialty chocolate,                \n#>      tropical fruit,                     \n#>      UHT-milk,                           \n#>      whipped/sour cream,                 \n#>      whole milk}                     1122\n#> [3] {beef,                               \n#>      beverages,                          \n#>      butter,                             \n#>      candles,                            \n#>      chicken,                            \n#>      citrus fruit,                       \n#>      cream cheese,                       \n#>      curd,                               \n#>      domestic eggs,                      \n#>      flour,                              \n#>      frankfurter,                        \n#>      ham,                                \n#>      hard cheese,                        \n#>      hygiene articles,                   \n#>      liver loaf,                         \n#>      margarine,                          \n#>      mayonnaise,                         \n#>      other vegetables,                   \n#>      pasta,                              \n#>      roll products,                      \n#>      rolls/buns,                         \n#>      root vegetables,                    \n#>      sausage,                            \n#>      skin care,                          \n#>      soft cheese,                        \n#>      soups,                              \n#>      specialty fat,                      \n#>      sugar,                              \n#>      tropical fruit,                     \n#>      whipped/sour cream,                 \n#>      whole milk,                         \n#>      yogurt}                         1217"},{"path":"patrones-de-asociación.html","id":"información-sesión","chapter":"Capítulo 8 Patrones de asociación","heading":"8.18 Información sesión","text":"Este comando te permite ver como estás de memoria y cpu, así como los paquetes que tienes cargados en la sesión.","code":"\nsesion_info <- devtools::session_info()\ndplyr::select(\n  tibble::as_tibble(sesion_info$packages),\n  c(package, loadedversion, source)\n)\n#> # A tibble: 93 × 3\n#>    package    loadedversion source        \n#>    <chr>      <chr>         <chr>         \n#>  1 arules     1.6-6         CRAN (R 4.3.3)\n#>  2 arulesViz  1.5.4         CRAN (R 4.3.3)\n#>  3 bit        4.0.5         CRAN (R 4.2.2)\n#>  4 bit64      4.0.5         CRAN (R 4.0.2)\n#>  5 bookdown   0.44          CRAN (R 4.3.3)\n#>  6 bslib      0.6.1         CRAN (R 4.3.2)\n#>  7 cachem     1.0.8         CRAN (R 4.3.0)\n#>  8 cli        3.6.2         CRAN (R 4.3.2)\n#>  9 colorspace 2.1-0         CRAN (R 4.2.2)\n#> 10 crayon     1.5.2         CRAN (R 4.2.2)\n#> # ℹ 83 more rows"},{"path":"patrones-de-asociación.html","id":"visualizacion-de-las-reglas","chapter":"Capítulo 8 Patrones de asociación","heading":"8.19 Visualizacion de las reglas","text":"para visualizar los graficos interactivos utiliza la funcionBasado en el texto de:Reglas de asociación y algoritmo Apriori con R Joaquín Amat Rodrigo, available Attribution 4.0 International (CC 4.0) https://www.cienciadedatos.net/documentos/43_reglas_de_asociacion","code":"library(arulesViz)\n\nplot(reglas)plot(reglas, method = \"graph\")"},{"path":"algoritmo-del-vecino-más-próximo.html","id":"algoritmo-del-vecino-más-próximo","chapter":"Capítulo 9 Algoritmo del vecino más próximo","heading":"Capítulo 9 Algoritmo del vecino más próximo","text":"kNN (k-nearest neighbors). El algoritmo kNN es un método de aprendizaje automático supervisado utilizado para la clasificación y la regresión. Se basa en la idea de que los puntos de datos similares tienden agruparse en grupos o regiones en el espacio de características.10En el caso de la clasificación, kNN asigna una etiqueta un punto de datos desconocido basándose en las etiquetas de sus vecinos más cercanos. La “k” en kNN se refiere al número de vecinos más cercanos que se toman en cuenta para tomar una decisión de clasificación. Por ejemplo, si k = 3, entonces el algoritmo considerará las etiquetas de los 3 puntos más cercanos y asignará al punto desconocido la etiqueta que sea más común entre esos 3 vecinos.","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"k-nearest-neighbor-knn","chapter":"Capítulo 9 Algoritmo del vecino más próximo","heading":"9.1 k Nearest Neighbor kNN","text":"En kNN la similaridad o distancia entre observaciones es utilizada para clasificar los elementos de una muestra o población.\nTipicamente usamos la distancia Euclídea en el espacio n dimensional. El modo más común de clasificación de un grupo es con el k-ésimo vecino(s) más próximo(s) en base datos etiquetados.11Se dice que kNN es una instanciación antes que un método basado en modelo, significando esto que en realidad se crea un modelo para clasificar.El proceso básico del algoritmo kNN es el siguiente:Calcula la distancia entre el punto de datos desconocido y todos los demás puntos de datos en el conjunto de entrenamiento. La distancia más comúnmente utilizada es la distancia euclidiana, pero también se pueden utilizar otras medidas de distancia.Calcula la distancia entre el punto de datos desconocido y todos los demás puntos de datos en el conjunto de entrenamiento. La distancia más comúnmente utilizada es la distancia euclidiana, pero también se pueden utilizar otras medidas de distancia.Selecciona los k puntos más cercanos al punto de datos desconocido en función de la distancia calculada en el paso anterior.Selecciona los k puntos más cercanos al punto de datos desconocido en función de la distancia calculada en el paso anterior.En el caso de clasificación, cuenta las etiquetas de los k vecinos más cercanos y asigna al punto desconocido la etiqueta más común entre ellos. En el caso de regresión, se puede tomar un promedio ponderado de los valores de los k vecinos más cercanos para obtener una estimación del valor desconocido.12En el caso de clasificación, cuenta las etiquetas de los k vecinos más cercanos y asigna al punto desconocido la etiqueta más común entre ellos. En el caso de regresión, se puede tomar un promedio ponderado de los valores de los k vecinos más cercanos para obtener una estimación del valor desconocido.12Es importante tener en cuenta que el valor de k en kNN es un parámetro ajustable y debe ser seleccionado cuidadosamente. Un valor pequeño de k puede llevar decisiones inestables y más susceptibles ruido, mientras que un valor grande de k puede suavizar las fronteras entre las clases y puede perder detalles más finos en los datos.Eligiendo un k grande, se reduce el impacto de la varianza causada por el “ruido” aleatorio de los datos, pero esto puede sesgar el aprendizaje de ignorar patrones pequeños, pero significativos.Se suele utilizar una regla de facto para elegir k, que es la raíz cuadrada del número de observaciones del set de entrenamiento redondeado hacia arriba\\[k =\\sqrt{n_{training}}\\]El algoritmo kNN es relativamente simple y fácil de implementar, pero puede ser computacionalmente costoso cuando se tienen conjuntos de datos grandes, ya que implica calcular distancias para cada punto en el conjunto de entrenamiento. Además, kNN captura patrones globales en los datos, ya que toma decisiones basadas únicamente en los puntos de datos más cercanos.","code":"\nlibrary(readr)\nlibrary(class) # knn\nperformance_scm <- read_delim(\"https://ricardorpalma.github.io/R-Dataset/scm_perform.csv\",     delim = \";\", escape_double = FALSE, col_types = cols(Performance = col_factor(levels = c(\"good\", \"fair\", \"poor\"))), locale = locale(decimal_mark = \",\",         grouping_mark = \".\"), trim_ws = TRUE)\nhead(performance_scm)\n#> # A tibble: 6 × 5\n#>   Inventory.Turnover Lead.Time Perfect.Order\n#>                <dbl>     <dbl>         <dbl>\n#> 1                5.1       3.5           1.4\n#> 2                4.9       3             1.4\n#> 3                4.7       3.2           1.3\n#> 4                4.6       3.1           1.5\n#> 5                5         3.6           1.4\n#> 6                5.4       3.9           1.7\n#> # ℹ 2 more variables: `Trnasport.Km/Ton` <dbl>,\n#> #   Performance <fct>"},{"path":"algoritmo-del-vecino-más-próximo.html","id":"que-relación-tiene-con-k-means","chapter":"Capítulo 9 Algoritmo del vecino más próximo","heading":"9.2 ¿Que relación tiene con k-means","text":"El algoritmo k-means es otro método utilizado en analítica de datos y big-data.Luego veremos en detalle que es k-means. Lo importante ahora es confundir estos conseptos que tienen nombres parecidos.El algoritmo k-means es otro método utilizado en aprendizaje automático supervisado para agrupar datos en conjuntos llamados “clusters”. Aunque k-means y k-nearest neighbors (kNN) tienen un nombre similar debido al uso de la letra “k”, son algoritmos diferentes con enfoques distintos.Mientras que kNN es un algoritmo de aprendizaje supervisado utilizado para clasificación y regresión, k-means es un algoritmo de aprendizaje supervisado utilizado para agrupamiento. Su objetivo principal es encontrar grupos o clusters en los datos sin tener información previa de las etiquetas o clases las que pertenecen los datos.La idea básica detrás de k-means es asignar cada punto de datos uno de los k clusters existentes, donde k es un número predeterminado definido por el usuario. El algoritmo itera para encontrar los centroides de los clusters de manera que se minimice la distancia total entre los puntos de datos y sus centroides correspondientes. En cada iteración, se recalculan los centroides y se reasignan los puntos de datos los clusters más cercanos.Una diferencia clave entre kNN y k-means es que kNN utiliza la información de etiquetas para tomar decisiones de clasificación o regresión, mientras que k-means agrupa los datos en base la proximidad espacial sin utilizar información de etiquetas. En k-means, los puntos de datos se agrupan en función de la similitud de sus características.En resumen, kNN es un algoritmo de aprendizaje supervisado utilizado para clasificación y regresión, mientras que k-means es un algoritmo de aprendizaje supervisado utilizado para agrupamiento. Ambos algoritmos tienen en común el uso del parámetro “k”, pero se aplican de manera diferente y tienen objetivos distintos en el análisis de datos.13","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"bibliografía-recomendada-1","chapter":"Capítulo 9 Algoritmo del vecino más próximo","heading":"9.3 Bibliografía recomendada","text":"","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"carga-de-dataset-y-exploración","chapter":"Capítulo 9 Algoritmo del vecino más próximo","heading":"9.4 Carga de dataset y exploración","text":"Carga de Bibliotecas Utilizadas","code":"\nlibrary(readr)\nperformance_scm <- read_delim(\"https://ricardorpalma.github.io/R-Dataset/scm_perform.csv\",     delim = \";\", escape_double = FALSE, col_types = cols(Performance = col_factor(levels = c(\"good\", \"fair\", \"poor\"))), locale = locale(decimal_mark = \",\",         grouping_mark = \".\"), trim_ws = TRUE)\nlibrary(e1071) # svm navie bayes\nlibrary(class) # knn\nlibrary(caret) # Construcción de Modelos\n#> Loading required package: ggplot2\n#> Loading required package: lattice\nlibrary(neuralnet) # Redes Neuronales\nlibrary(randomForest) # Random Forest\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\n#> \n#> Attaching package: 'randomForest'\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     margin\nlibrary(ggplot2) # Representacion del conocimiento\nlibrary(gridExtra) # Ploteo\n#> \n#> Attaching package: 'gridExtra'\n#> The following object is masked from 'package:randomForest':\n#> \n#>     combine"},{"path":"algoritmo-del-vecino-más-próximo.html","id":"entrenamiento-y-prueba","chapter":"Capítulo 9 Algoritmo del vecino más próximo","heading":"9.5 Entrenamiento y prueba","text":"Separaremos los datos en dos conjuntos aleatoriosDataset de EntrenamientoDataset de PruebaLos datos puede ser elegidos aleatoriamente con el comando sample.\nEn nuestro caso emplearemos la proporción \\(70/30\\)\n70% Para entrenar 30% para test (preuba). Esto debería dejaronos 105 observaciones en un conjunto y 45 en el otro.Para que podamos repetir el experimento con los mismos datos fijaremos la semilla y así obtener resultado parecidosAhora partiremos el dataset utilizando como índice inTrain.CPodemos ver la tabla de resultados contenidos con este comandoDel mismo modo podemos ver el dataset de pruebas","code":"\nset.seed(831)\ninTrain.C <- createDataPartition(performance_scm$Performance,p=.7,list = FALSE)\ntraining.C <- performance_scm[inTrain.C, ]\ntesting.C <- performance_scm[-inTrain.C, ]\ntable(training.C$Performance)\n#> \n#> good fair poor \n#>   35   35   35\ntable(testing.C$Performance)\n#> \n#> good fair poor \n#>   15   15   15"},{"path":"algoritmo-del-vecino-más-próximo.html","id":"ajuste-del-modelo","chapter":"Capítulo 9 Algoritmo del vecino más próximo","heading":"9.6 Ajuste del modelo","text":"Realizaremos una tabla de contingencia o matriz de confusión","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"mediciones-model-level","chapter":"Capítulo 9 Algoritmo del vecino más próximo","heading":"9.7 Mediciones Model-level","text":"","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"accuracy","chapter":"Capítulo 9 Algoritmo del vecino más próximo","heading":"9.7.1 Accuracy","text":"Mide la proporción de predicciones correctas y es extraño encontrarlo como porcentaje. Su rango va de 0 100% . valor más alto mejor preformance del modelo\\[Accuracy = \\frac {TP + TN}{TP+TN+FP+FN} \\]","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"error-rate","chapter":"Capítulo 9 Algoritmo del vecino más próximo","heading":"9.7.2 Error-Rate","text":"Mide la proporción de instancias mal clasificadas sobre el total de instancias. También es frecuente su uso como porcentaje en el rango 0% 100%. El valor más bajo es el más conveniente, al contrario que el de Acurracy\\[ErrorRate = \\frac{FP + FN}{TP+TN+FP+FN} = 1- Accuracy\\]","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"class-level-mesures","chapter":"Capítulo 9 Algoritmo del vecino más próximo","heading":"9.8 Class-Level Mesures","text":"","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"precisión","chapter":"Capítulo 9 Algoritmo del vecino más próximo","heading":"9.8.1 Precisión","text":"Mide cuantas predicciones hechas fueron correctas respecto del total de predicciones emitidas.\\[Precision = \\frac{CorrectPredic}{TotalPredic}\\]","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"recall-o-rememoración","chapter":"Capítulo 9 Algoritmo del vecino más próximo","heading":"9.8.2 Recall o rememoración","text":"Estima cuantas predicciones para una clase dada son correctas contra el total de casos predichos en la clase (entrenamiento y test)\\[Recall = \\frac{CorrectPredic}{TotalActual}\\]\n### Métrica-F\nEstima la bondad de clasificación para una clase dada. Se compara el total de casos correctamente clasificados contra el balance en la precisión y rememoración (recall). El valor máximo de F-Messure es igual 1\\[F1Messure = 2*\\frac{Precision - Recall}{Precision * Recall} \\]","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"subreentrenado-sobreentrenado","chapter":"Capítulo 9 Algoritmo del vecino más próximo","heading":"9.9 Subreentrenado Sobreentrenado","text":"","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"k-nearest-neighbor-knn-1","chapter":"Capítulo 9 Algoritmo del vecino más próximo","heading":"9.10 k Nearest Neighbor kNN","text":"En kNN la similaridd o distancia entre observaciones es utilizada para clasificar los elementos de una muestra o población.\nTipicamente usamos la distancia Euclídea en el espacio n dimensional. El modo más común de clasificación de un grupo es con el k-ésimo vecino(s) más próximo(s) en base datos etiquetados.Se dice que kNN es una instanciación antes que un método basado en modelo, significando esto que en realidad se crea un modelo para clasificar.Eligiendo un k grande, se reduce el impacto de la varianza causada por el “ruido” aleatorio de los datos, pero esto puede sesgar el aprendizaje de ignorar patrones pequeños, pero significativos.Se suele utilizar una regla de facto para elegir k, que es la raíz cuadrada del número de observaciones del set de entrenamiento redondeado hacia arriba\\[k =\\sqrt{n_{training}}\\]Para el dataset de performance logística se tieneUtilizar un número impar para \\(k\\) es una buena práctica, porque evita caer en empate al seleccionar la etiqueta dominante.Utilizaremos la función \\(knn()\\) del paquete \\(class\\) para construir lo que representaría el modelo.","code":"\nk.choice <- ceiling(sqrt(nrow(training.C)))\nk.choice\n#> [1] 11\nlibrary(class)\nknn.pred.C <- knn(train=training.C[ ,1:4] , test=testing.C[ , 1:4], cl=training.C$Performance, k=k.choice)"},{"path":"algoritmo-del-vecino-más-próximo.html","id":"testeo-de-performace-del-algoritmo-entrenado","chapter":"Capítulo 9 Algoritmo del vecino más próximo","heading":"9.10.1 Testeo de Performace del Algoritmo entrenado","text":"Dado que kNN precide un modelo podemos hacer una ponderación o medida de la bondad del ajuste en los datos de entrenamiento, por lo que debemos necesariamente aplicar el modelo los datos de prueba.","code":"\nknn.test.acc <- confusionMatrix(knn.pred.C,testing.C$Performance, mode=\"prec_recall\")\nknn.test.acc\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction good fair poor\n#>       good   15    0    0\n#>       fair    0   14    0\n#>       poor    0    1   15\n#> \n#> Overall Statistics\n#>                                           \n#>                Accuracy : 0.9778          \n#>                  95% CI : (0.8823, 0.9994)\n#>     No Information Rate : 0.3333          \n#>     P-Value [Acc > NIR] : < 2.2e-16       \n#>                                           \n#>                   Kappa : 0.9667          \n#>                                           \n#>  Mcnemar's Test P-Value : NA              \n#> \n#> Statistics by Class:\n#> \n#>                      Class: good Class: fair Class: poor\n#> Precision                 1.0000      1.0000      0.9375\n#> Recall                    1.0000      0.9333      1.0000\n#> F1                        1.0000      0.9655      0.9677\n#> Prevalence                0.3333      0.3333      0.3333\n#> Detection Rate            0.3333      0.3111      0.3333\n#> Detection Prevalence      0.3333      0.3111      0.3556\n#> Balanced Accuracy         1.0000      0.9667      0.9833"},{"path":"algoritmo-del-vecino-más-próximo.html","id":"vigilar-el-sobrenetrenamiento","chapter":"Capítulo 9 Algoritmo del vecino más próximo","heading":"9.11 Vigilar el sobrenetrenamiento","text":"Subreentrenado o Sobreentrenado","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"buenas-prácticas-en-knn","chapter":"Capítulo 9 Algoritmo del vecino más próximo","heading":"9.12 Buenas prácticas en kNN","text":"Nromalizar los valoresDado que kNN se basa en las distancias puede haber un error de influencia si una columna tiene magnitudes muy diferentes otra.Basados en el gráfico boxplot deberíamos transfomar el recorrido de las variables al rango \\([0,1]\\).\\[ \\frac{x-min(x)}{max(x)-min(x)}\\]\nNoramlizaremos los datosAhora podemos hacer el gráfico con los valores escaladosAlternativamente de puede usar z-scores, que están estandarizados con la media entorno 0 y el desvío estandard.\\[Z= \\frac{x-u}{\\sigma} \\]Vista del gráfico normalizado","code":"\nboxplot(performance_scm[ ,1:4])\nperf_escalado <-apply(performance_scm[ ,1:4], 2, function (x) (x-min(x))/(max(x)-min(x)) )\nperf_escalado <- data.frame(perf_escalado, performance_scm$Performance)\nboxplot(perf_escalado[ ,1:4], main=\"Gráfico de cajas Predictores Escalados\", las=2)\nperf_z <- apply(performance_scm[ ,1:4],2, function(x) (x-mean(x)) / (sd(x))  )\nperf_z <- data.frame(perf_z,performance_scm$Performance)\nboxplot(perf_z[ ,1:4])"},{"path":"algoritmo-del-vecino-más-próximo.html","id":"variando-k","chapter":"Capítulo 9 Algoritmo del vecino más próximo","heading":"9.12.1 Variando k","text":"Dado que kNN es un algoritmo de aprendizaje peresozo, pero eficiente la hora de hacer predicciones (pues en realidad hay aprendizaje formal), resulta interesante probar el efecto de variar los valores de k y ver cómo impacta en los resultados obtenidos y la performance de entrenamiento.","code":"\nsave <- list()\nfor (i in 1:25){\n  knn.pred.C.l <- knn(train = training.C[ ,1:4], test=testing.C[ ,1:4], cl=training.C$Performance, k=i)\n  save[[i]] <-confusionMatrix(knn.pred.C.l, testing.C$Performance, mode=\"prec_recall\")$overall[1] \n  \n}\nsave <-do.call(rbind,save)\nplot(1:25,save, xlab=\"k\", ylab=\"Accuracy\", type=\"o\")"},{"path":"redes-neuronales-artificiales.html","id":"redes-neuronales-artificiales","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"Capítulo 10 Redes Neuronales artificiales","text":"Las redes neuronales artificiales son modelos computacionales inspirados en el funcionamiento del cerebro humano. Están compuestas por una serie de unidades llamadas neuronas artificiales, que están conectadas entre sí través de conexiones ponderadas. Estas conexiones permiten las redes neuronales procesar información y realizar tareas de aprendizaje y predicción.En ingeniería industrial, las redes neuronales artificiales se utilizan en una amplia variedad de aplicaciones, como:Pronóstico de demanda: Las redes neuronales pueden utilizarse para predecir la demanda futura de productos o servicios en función de datos históricos, ayudando así la planificación de la producción y la gestión de inventarios.Pronóstico de demanda: Las redes neuronales pueden utilizarse para predecir la demanda futura de productos o servicios en función de datos históricos, ayudando así la planificación de la producción y la gestión de inventarios.Control de procesos: Las redes neuronales pueden utilizarse para controlar y optimizar procesos industriales. Pueden aprender de los datos recopilados en tiempo real y ajustar los parámetros del sistema para maximizar la eficiencia y minimizar los errores.Control de procesos: Las redes neuronales pueden utilizarse para controlar y optimizar procesos industriales. Pueden aprender de los datos recopilados en tiempo real y ajustar los parámetros del sistema para maximizar la eficiencia y minimizar los errores.Mantenimiento predictivo: Las redes neuronales pueden utilizarse para predecir fallos y realizar mantenimiento predictivo en maquinaria y equipos industriales. Al analizar datos de sensores y patrones de fallas anteriores, las redes neuronales pueden identificar señales de advertencia temprana y ayudar evitar costosas interrupciones en la producción.Mantenimiento predictivo: Las redes neuronales pueden utilizarse para predecir fallos y realizar mantenimiento predictivo en maquinaria y equipos industriales. Al analizar datos de sensores y patrones de fallas anteriores, las redes neuronales pueden identificar señales de advertencia temprana y ayudar evitar costosas interrupciones en la producción.Control de calidad: Las redes neuronales pueden utilizarse para realizar inspección y control de calidad en productos manufacturados. Pueden identificar defectos o anomalías en imágenes, señales o datos sensoriales, ayudando mejorar la calidad del producto final.Control de calidad: Las redes neuronales pueden utilizarse para realizar inspección y control de calidad en productos manufacturados. Pueden identificar defectos o anomalías en imágenes, señales o datos sensoriales, ayudando mejorar la calidad del producto final.Optimización de la cadena de suministro: Las redes neuronales pueden utilizarse para optimizar la gestión de la cadena de suministro, analizando grandes volúmenes de datos y tomando decisiones óptimas en tiempo real sobre inventario, distribución y logística.Optimización de la cadena de suministro: Las redes neuronales pueden utilizarse para optimizar la gestión de la cadena de suministro, analizando grandes volúmenes de datos y tomando decisiones óptimas en tiempo real sobre inventario, distribución y logística.La ventaja de las redes neuronales artificiales en ingeniería industrial radica en su capacidad para modelar relaciones complejas y lineales entre variables, así como en su capacidad de aprendizaje partir de datos. Sin embargo, es importante tener en cuenta que el uso de redes neuronales artificiales requiere una adecuada preparación y procesamiento de los datos, así como una validación y ajuste adecuados de los modelos para garantizar resultados confiables y precisos.14Si la ANN (Artificial Neural Net) tiene sólo una neurona de salida actua como clasificador dicotómico de la entrada.Si la ANN (Artificial Neural Net) tiene sólo una neurona de salida actua como clasificador dicotómico de la entrada.Si tiene varios niveles o categorías del tipo binario puede actuar como clasificador categóricoSi tiene varios niveles o categorías del tipo binario puede actuar como clasificador categóricoSi tiene una salida de doble precisión puede actuar como predictor (clustering)Si tiene una salida de doble precisión puede actuar como predictor (clustering)","code":""},{"path":"redes-neuronales-artificiales.html","id":"bibliografía-recomendada-2","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.1 Bibliografía recomendada","text":"","code":""},{"path":"redes-neuronales-artificiales.html","id":"redes-neuronales-en-r-cran","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.2 Redes Neuronales en R-Cran","text":"En R-Cran, puedes implementar redes neuronales artificiales utilizando la biblioteca “neuralnet”. Esta biblioteca proporciona funciones para construir y entrenar redes neuronales de una o varias capas en R.Aquí hay un ejemplo básico de cómo construir y entrenar una red neuronal artificial utilizando la biblioteca “neuralnet” en R-Cran:Instalación del paquete “neuralnet”:\ninstall.packages(“neuralnet”)Instalación del paquete “neuralnet”:install.packages(“neuralnet”)Carga del paquete “neuralnet”:\nlibrary(neuralnet)Carga del paquete “neuralnet”:library(neuralnet)Preparación de los datos de entrenamiento y prueba:Preparación de los datos de entrenamiento y prueba:Supongamos que tienes un conjunto de datos con variables de entrada (predictoras) llamadas “x1” y “x2” y una variable de salida (objetivo) llamada “y”. Debes dividir tus datos en conjuntos de entrenamiento y prueba. Aquí hay un ejemplo de cómo puedes hacerlo:Construcción de la red neuronal:Aquí puedes definir la arquitectura de tu red neuronal especificando el número de nodos en cada capa oculta. Por ejemplo, una red con una capa oculta de 5 nodos se puede definir de la siguiente manera:Crear la red neuronalEn este ejemplo, “formula” especifica la relación entre las variables de entrada y salida, y “hidden” indica el número de nodos en la capa oculta.Entrenamiento de la red neuronal:Utiliza la función “train” para entrenar la red neuronal con los datos de entrenamiento:Entrenar la red neuronalPredicción con la red neuronal entrenada:Utiliza la función “compute” para realizar predicciones con la red neuronal entrenada:Realizar predicciones con la red neuronalEn este ejemplo, “test_data[, c(”x1”, ”x2”)]” son las variables de entrada para las cuales deseas realizar predicciones.Este es solo un ejemplo básico para construir y entrenar una red neuronal artificial en R-Cran utilizando la biblioteca “neuralnet”. Puedes consultar la documentación de la biblioteca para obtener más información sobre las funciones y opciones disponibles.15","code":"set.seed(123)  # Establecer una semilla aleatoria para reproducibilidad\n\n  # Dividir datos en conjuntos de entrenamiento y prueba\n\ntrain_indices <- sample(1:nrow(tus_datos), nrow(tus_datos)*0.8)  # 80% de los datos para entrenamiento\ntrain_data <- tus_datos[train_indices, ]\ntest_data <- tus_datos[-train_indices, ]# Definir la fórmula de la red neuronal\nformula <- y ~ x1 + x2nn <- neuralnet(formula, data = train_data, hidden = c(5))trained_nn <- train(nn)predictions <- compute(trained_nn, test_data[, c(\"x1\", \"x2\")])"},{"path":"redes-neuronales-artificiales.html","id":"caso-de-estudio-cargar-de-bibliotecas","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.3 Caso de estudio: Cargar de bibliotecas","text":"","code":"\nlibrary(neuralnet)  # regression\n\nlibrary(nnet) # classification \n\nlibrary(NeuralNetTools)\n\nlibrary(plyr)"},{"path":"redes-neuronales-artificiales.html","id":"carga-de-datos","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.4 Carga de Datos","text":"La columna supervivencia de la tabla Categoric tiene etiquetas que identifican la situación en la que terminaron los emprendimientos, saber:SpinOf La empresa se separó de una principal y logró su independencia económicaSpinOf La empresa se separó de una principal y logró su independencia económicaRevEq La empresa alcanzó su punto de equilibrio canceló su deuda con el banco y fue comprada por la competencia.RevEq La empresa alcanzó su punto de equilibrio canceló su deuda con el banco y fue comprada por la competencia.BankR La empresa prosperó y el banco se quedó con los activos que vendidos compensaron el preśtamo inicial (Banca Rota).BankR La empresa prosperó y el banco se quedó con los activos que vendidos compensaron el preśtamo inicial (Banca Rota).","code":"\nlibrary(readr)\nStartups <- read_csv(\"https://ricardorpalma.github.io/R-Dataset/50_Startups_LAC.csv\")\n#> Rows: 50 Columns: 6\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (1): Pais\n#> dbl (5): R_D_Spend, POM, Logist_Market, Profit, Superviv...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nCategoric <- read_csv(\"https://ricardorpalma.github.io/R-Dataset//50_Startups_Categoric_LAC.csv\")\n#> Rows: 50 Columns: 6\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (2): Pais, Supervivencia\n#> dbl (4): R_D_Spend, POM, Logist_Market, Profit\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"redes-neuronales-artificiales.html","id":"tratamiento-de-variables-categóricas","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.5 Tratamiento de variables categóricas","text":"","code":"\ntabla1 <- table(Categoric$Pais)\ntabla2 <- table(Categoric$Supervivencia)\ntabla3 <- table(Categoric$Pais,Categoric$Supervivencia)\nplot(tabla1, col=c(\"red\",\"green\",\"blue\"))\nplot(tabla2, col=c(\"red\",\"green\",\"blue\"))\nplot(tabla3, col=c(\"red\",\"green\",\"blue\"))"},{"path":"redes-neuronales-artificiales.html","id":"histogramas-superpuestos","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.6 Histogramas superpuestos","text":"","code":"\nind_1 <- which(Categoric$Pais==\"Colombia\")\np1 <- as.matrix(Categoric[ind_1,5])\n\nind_2 <- which(Categoric$Pais==\"Ecuador\")\np2 <- as.matrix(Categoric[ind_2,5])\n\nind_3 <- which(Categoric$Pais==\"Chile\")\np3 <- as.matrix(Categoric[ind_3,5])\nhp1 <- hist(p1)\nhp2 <- hist(p2)\nhp3 <- hist(p3)\n\npar(mfrow=c(3,1))\nplot( hp1, col=rgb(0,0,1,1/4), xlim=c(30000,200000),ylim=c(0,5),main=\"Ecuador\")\nplot( hp2, col=rgb(1,0,0,1/4),xlim=c(30000,200000),ylim=c(0,10),main=\"Colombia\") \nplot( hp3, col=rgb(1,0,0,1/4),xlim=c(30000,200000),ylim=c(0,10),main=\"Chile\") \npar(mfrow=c(1,3))\nplot( hp1, col=rgb(0,0,1,1/4), xlim=c(30000,200000),ylim=c(0,5),main=\"Ecuador\")\nplot( hp2, col=rgb(1,0,0,1/4),xlim=c(30000,200000),ylim=c(0,10),main=\"Colombia\") \nplot( hp3, col=rgb(1,0,0,1/4),xlim=c(30000,200000),ylim=c(0,10),main=\"Chile\") \npairs(Categoric[ ,1:3])\nboxplot(Categoric[ ,1:3])\nCategoric$Pais <- as.numeric(revalue(Categoric$Pais,\n                          c(\"Colombia\"=\"0\", \"Ecuador\"=\"1\",\n                            \"Chile\"=\"2\")))\n#> Warning: NAs introducidos por coerción"},{"path":"redes-neuronales-artificiales.html","id":"cuadro-de-campos-categóricos","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.7 Cuadro de campos categóricos","text":"","code":"\nCategoric$Supervivencia <- as.numeric(revalue(Categoric$Supervivencia,\n                          c(\"BankR\"=\"0\", \"RevEq\"=\"1\",\n                            \"SpinOff\"=\"2\")))"},{"path":"redes-neuronales-artificiales.html","id":"profit-versus-país","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.8 Profit versus País","text":"","code":"\nplot(Categoric$Pais, Categoric$Profit)"},{"path":"redes-neuronales-artificiales.html","id":"visualización-de-tablas","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.9 Visualización de Tablas","text":"Tabla TextualTabla Simple\nTable 10.1: Tablas estilo Tesis Di3\n","code":"\n\nlibrary(kableExtra)\nkable(head(Categoric), \"pipe\",  booktabs = TRUE)%>% kable_styling(font_size = 8)\n#> Warning: 'xfun::attr()' is deprecated.\n#> Use 'xfun::attr2()' instead.\n#> See help(\"Deprecated\")\n\n#> Warning: 'xfun::attr()' is deprecated.\n#> Use 'xfun::attr2()' instead.\n#> See help(\"Deprecated\")\nkable(head(Categoric), \"simple\")\n#> Warning: 'xfun::attr()' is deprecated.\n#> Use 'xfun::attr2()' instead.\n#> See help(\"Deprecated\")\n\n#> Warning: 'xfun::attr()' is deprecated.\n#> Use 'xfun::attr2()' instead.\n#> See help(\"Deprecated\")\nkbl(Categoric)  %>%\n  kable_paper(\"hover\", full_width = F)\n#> Warning: 'xfun::attr()' is deprecated.\n#> Use 'xfun::attr2()' instead.\n#> See help(\"Deprecated\")\nna.omit(Categoric) %>%\n  kbl(caption = \"Tablas estilo Tesis Di3\") %>%\n  kable_classic(full_width = F, html_font = \"Cambria\")\n#> Warning: 'xfun::attr()' is deprecated.\n#> Use 'xfun::attr2()' instead.\n#> See help(\"Deprecated\")"},{"path":"redes-neuronales-artificiales.html","id":"normailización","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.10 Normailización","text":"Eliminaremos los datos que tienen NADatos Originales y Datos normalizadosMuestreo para entrenamento","code":"\nCategoric <- na.omit(Categoric)\nnormalize<-function(x){\n  return ( (x-min(x))/(max(x)-min(x)))\n}\n\nStartups_norm<-as.data.frame(lapply(Categoric,FUN=normalize))\nsummary(Startups_norm$Profit)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.0000  0.4116  0.5259  0.5581  0.7374  1.0000\nhead(Categoric$Profit)\n#> [1] 192261.8 191792.1 191050.4 182902.0 166187.9 156991.1\nhead(Startups_norm)\n#>   R_D_Spend       POM Logist_Market Pais    Profit\n#> 1 1.0000000 0.6517439     1.0000000  0.0 1.0000000\n#> 2 0.9833595 0.7619717     0.9408934  0.5 0.9973546\n#> 3 0.9279846 0.3795790     0.8646636  1.0 0.9931781\n#> 4 0.8731364 0.5129984     0.8122351  0.0 0.9472924\n#> 5 0.8594377 0.3053280     0.7761356  1.0 0.8531714\n#> 6 0.7975660 0.3694479     0.7691259  0.0 0.8013818\n#>   Supervivencia\n#> 1             1\n#> 2             1\n#> 3             1\n#> 4             1\n#> 5             1\n#> 6             1\nindice <- sample(2, nrow(Startups_norm), replace = TRUE, prob = c(0.7,0.3))\nstartups_train <- Startups_norm[indice==1,]\nstartups_test  <- Startups_norm[indice==2,]"},{"path":"redes-neuronales-artificiales.html","id":"modelo-de-neural-net","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.11 Modelo de Neural Net","text":"","code":"\nlibrary(neuralnet)\nattach(Categoric)\n\nstartups_model <- neuralnet(Profit ~ R_D_Spend+ POM + Logist_Market + Pais , data = startups_train)\n\nstr(startups_model)\n#> List of 14\n#>  $ call               : language neuralnet(formula = Profit ~ R_D_Spend + POM + Logist_Market +      Pais, data = startups_train)\n#>  $ response           : num [1:30, 1] 0.997 0.993 0.801 0.796 0.794 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:30] \"2\" \"3\" \"6\" \"7\" ...\n#>   .. ..$ : chr \"Profit\"\n#>  $ covariate          : num [1:30, 1:4] 0.983 0.928 0.798 0.814 0.788 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:30] \"2\" \"3\" \"6\" \"7\" ...\n#>   .. ..$ : chr [1:4] \"R_D_Spend\" \"POM\" \"Logist_Market\" \"Pais\"\n#>  $ model.list         :List of 2\n#>   ..$ response : chr \"Profit\"\n#>   ..$ variables: chr [1:4] \"R_D_Spend\" \"POM\" \"Logist_Market\" \"Pais\"\n#>  $ err.fct            :function (x, y)  \n#>   ..- attr(*, \"type\")= chr \"sse\"\n#>  $ act.fct            :function (x)  \n#>   ..- attr(*, \"type\")= chr \"logistic\"\n#>  $ linear.output      : logi TRUE\n#>  $ data               :'data.frame': 30 obs. of  6 variables:\n#>   ..$ R_D_Spend    : num [1:30] 0.983 0.928 0.798 0.814 0.788 ...\n#>   ..$ POM          : num [1:30] 0.762 0.38 0.369 0.73 0.717 ...\n#>   ..$ Logist_Market: num [1:30] 0.941 0.865 0.769 0.271 0.686 ...\n#>   ..$ Pais         : num [1:30] 0.5 1 0 0.5 1 0 0.5 0.5 1 0 ...\n#>   ..$ Profit       : num [1:30] 0.997 0.993 0.801 0.796 0.794 ...\n#>   ..$ Supervivencia: num [1:30] 1 1 1 1 1 1 0.5 0.5 0.5 0.5 ...\n#>  $ exclude            : NULL\n#>  $ net.result         :List of 1\n#>   ..$ : num [1:30, 1] 0.911 0.9 0.832 0.825 0.836 ...\n#>   .. ..- attr(*, \"dimnames\")=List of 2\n#>   .. .. ..$ : chr [1:30] \"2\" \"3\" \"6\" \"7\" ...\n#>   .. .. ..$ : NULL\n#>  $ weights            :List of 1\n#>   ..$ :List of 2\n#>   .. ..$ : num [1:5, 1] -1.8371 4.1142 -0.0806 0.3251 0.1266\n#>   .. ..$ : num [1:2, 1] 0.0516 0.9286\n#>  $ generalized.weights:List of 1\n#>   ..$ : num [1:30, 1:4] 3.25 3.35 3.67 3.68 3.65 ...\n#>   .. ..- attr(*, \"dimnames\")=List of 2\n#>   .. .. ..$ : chr [1:30] \"2\" \"3\" \"6\" \"7\" ...\n#>   .. .. ..$ : NULL\n#>  $ startweights       :List of 1\n#>   ..$ :List of 2\n#>   .. ..$ : num [1:5, 1] 0.124 -0.661 0.343 -1.603 0.438\n#>   .. ..$ : num [1:2, 1] -0.685 -0.509\n#>  $ result.matrix      : num [1:10, 1] 0.05496 0.00932 180 -1.83714 4.11423 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:10] \"error\" \"reached.threshold\" \"steps\" \"Intercept.to.1layhid1\" ...\n#>   .. ..$ : NULL\n#>  - attr(*, \"class\")= chr \"nn\""},{"path":"redes-neuronales-artificiales.html","id":"ploteo-de-la-red-neuronal","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.12 Ploteo de la red Neuronal","text":"","code":"\nplot(startups_model, rep = \"best\")"},{"path":"redes-neuronales-artificiales.html","id":"ploteo-de-la-red-proporcional","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.13 Ploteo de la red proporcional","text":"Esto indica cuales son los KPI","code":"\npar(mar = numeric(4), family = 'serif')\nplotnet(startups_model, alpha = 0.6)"},{"path":"redes-neuronales-artificiales.html","id":"evaluación-de-la-performance-del-modelo","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.13.1 Evaluación de la performance del modelo","text":"","code":"\nmodel_results <- compute(startups_model,startups_test[1:4])\npredicted_profit <- model_results$net.result"},{"path":"redes-neuronales-artificiales.html","id":"predicted-profit-vs-actual-profit-of-test-data.","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.14 Predicted profit Vs Actual profit of test data.","text":"","code":"\ncor(predicted_profit,startups_test$Profit)\n#>           [,1]\n#> [1,] 0.9628344"},{"path":"redes-neuronales-artificiales.html","id":"desnormalización-de-los-resultados","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.15 Desnormalización de los resultados","text":"Dado que hicimos la predicciones con los datos normalizados, ahora deberemos des-normalizarlos16","code":"\n\nstr_max <- max(Startups$Profit)\nstr_min <- min(Startups$Profit)\n\nunnormalize <- function(x, min, max) { \n  return( (max - min)*x + min )\n}\n\nActualProfit_pred <- unnormalize(predicted_profit,str_min,str_max)\nhead(ActualProfit_pred)\n#>        [,1]\n#> 1  176806.7\n#> 4  168710.2\n#> 5  169998.1\n#> 13 155767.9\n#> 15 118832.5\n#> 24 103573.9"},{"path":"redes-neuronales-artificiales.html","id":"mejoramiento-de-la-performance-del-modelo","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.16 Mejoramiento de la performance del modelo","text":"Es posible mejorar la performance con el agregado de más capas ocultas.","code":"\nStartups_model2 <- neuralnet(Profit~R_D_Spend+ POM + Logist_Market + Pais , data = startups_train, hidden = c(2,4))\n\nplot(Startups_model2 ,rep = \"best\")"},{"path":"redes-neuronales-artificiales.html","id":"performance-del-modelo-mejorado","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.17 Performance del modelo mejorado","text":"","code":"\nmodel_results2<-compute(Startups_model2,startups_test[1:4])\npredicted_Profit2<-model_results2$net.result\ncor(predicted_Profit2,startups_test$Profit)\n#>           [,1]\n#> [1,] 0.9609024"},{"path":"redes-neuronales-artificiales.html","id":"modelo-mejorado-kpi","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.18 Modelo Mejorado KPI","text":"","code":"\npar(mar = numeric(4), family = 'serif')\nplotnet(Startups_model2, alpha = 0.6)"},{"path":"redes-neuronales-artificiales.html","id":"neural-net-clasificación","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.19 Neural Net Clasificación","text":"Armamos el dataset de datos clasificar","code":"\nlibrary(nnet)\n\nsupervivencia <- as.factor(Categoric$Supervivencia)\nR_D_Spend <- as.matrix(Categoric$R_D_Spend)\nPOM <- as.matrix(Categoric$POM)\nLogist_Market <- as.matrix(Categoric$Logist_Market)\nClasificar <- data.frame (supervivencia,R_D_Spend,POM,Logist_Market)"},{"path":"redes-neuronales-artificiales.html","id":"muestreo","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.20 Muestreo","text":"Entrenamiento de nnet como clasificador","code":"\nindice <- sample(2, nrow(Clasificar), replace = TRUE, prob = c(0.7,0.3))\n\nclasificar_train <- Startups_norm[indice==1,]\nclasificar_test  <- Startups_norm[indice==2,]\n\nsupervivientes_clasificados <- factor(clasificar_train$Supervivencia)\nsupervivientes_train<-nnet(supervivientes_clasificados~clasificar_train$R_D_Spend + clasificar_train$POM+ clasificar_train$Logist_Market ,data=clasificar_train,size=5, decay=5e-4, maxit=2000)\n#> # weights:  38\n#> initial  value 30.618845 \n#> iter  10 value 9.920325\n#> iter  20 value 4.196626\n#> iter  30 value 2.889551\n#> iter  40 value 2.073058\n#> iter  50 value 1.613938\n#> iter  60 value 1.539713\n#> iter  70 value 1.432790\n#> iter  80 value 1.375105\n#> iter  90 value 1.325779\n#> iter 100 value 1.231268\n#> iter 110 value 1.190554\n#> iter 120 value 1.179915\n#> iter 130 value 1.173316\n#> iter 140 value 1.165143\n#> iter 150 value 1.150997\n#> iter 160 value 1.145478\n#> iter 170 value 1.142106\n#> iter 180 value 1.138962\n#> iter 190 value 1.137330\n#> iter 200 value 1.136549\n#> iter 210 value 1.135992\n#> iter 220 value 1.135597\n#> iter 230 value 1.134803\n#> iter 240 value 1.130649\n#> iter 250 value 1.127304\n#> iter 260 value 1.121489\n#> iter 270 value 1.118144\n#> iter 280 value 1.113116\n#> iter 290 value 1.109477\n#> iter 300 value 1.104146\n#> iter 310 value 1.096284\n#> iter 320 value 1.093635\n#> iter 330 value 1.091332\n#> iter 340 value 1.090065\n#> iter 350 value 1.089239\n#> iter 360 value 1.088488\n#> iter 370 value 1.088024\n#> iter 380 value 1.087684\n#> iter 390 value 1.087383\n#> iter 400 value 1.087240\n#> iter 410 value 1.087149\n#> iter 420 value 1.087081\n#> iter 430 value 1.087052\n#> iter 440 value 1.087023\n#> iter 450 value 1.087006\n#> iter 460 value 1.086995\n#> iter 470 value 1.086992\n#> iter 480 value 1.086989\n#> iter 490 value 1.086983\n#> iter 500 value 1.086981\n#> iter 510 value 1.086980\n#> iter 520 value 1.086978\n#> final  value 1.086978 \n#> converged"},{"path":"redes-neuronales-artificiales.html","id":"visualización-del-clasificador","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.21 Visualización del clasificador","text":"","code":"\nlibrary(NeuralNetTools)\nplotnet(supervivientes_train, alpha = 0.6)"},{"path":"redes-neuronales-artificiales.html","id":"lecturas-complementarias","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.22 Lecturas complementarias","text":"","code":""},{"path":"redes-neuronales-artificiales.html","id":"estilos-de-tablas","chapter":"Capítulo 10 Redes Neuronales artificiales","heading":"10.22.1 Estilos de Tablas","text":"https://cran-r--project-org.translate.goog/web/packages/kableExtra/vignettes/awesome_table_in_html.html?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=tc#Table_Styles","code":""},{"path":"clusterización.html","id":"clusterización","chapter":"Capítulo 11 Clusterización","heading":"Capítulo 11 Clusterización","text":"Una de las cosas interesantes sobre la posibilidad de hacer Inteligencia de Negocios (BI por sus siglas en ingles – Business Intelligence) es que nos permite descubrir patrones visibles en primera instancia, pero que ayudan preguntarnos ¿por qué sucede esto?.\nEn esta serie de ejercicios teóricos, además de familiarizarnos con el lenguaje R-Cran y su uso en Clusters, trataremos de ver como podemos valernos de las bondades del entorno de trabajo para responder esas preguntas.Hasta ahora tenemos claro temas referidos al concepto estadísticos clásicos (monovariados), y hemos insinuado algo sobre las estructuras de los entornos multivariados.","code":""},{"path":"clusterización.html","id":"bibliografía-recomendada-3","chapter":"Capítulo 11 Clusterización","heading":"11.1 Bibliografía recomendada","text":"Teniendo en cuenta lo sensible que este mérodo es el tratamiento previo de los datos (sobre todo la normalización) es recomendable revisar esta publicación.Tanto sea para empresas industriales o de servicios en las que sus sistemas de información abarquen los campos de medición y administración de la estrategia o el aspecto operativo; es de crucial importancia determinar los niveles en los que las variables que representan las perspectivas tengan claramente establecidas sus franjas de flotación. En otras palabras debemos sentar las bases para construir un Cuadro de mando o tablero de control, que es sólo hacer que luces rojas amarillas y verdes se enciendan en un semáforo, sino tener relevancia para la toma de decisión sobre lo que los datos nos aportan.Utilizaremos una base de datos ficticia (cualquier semejanza con la realidad es pura coincidencia).https://ricardorpalma.github.io/R-Dataset/BSC_proveedores.csvEn ella se ha realizado un estudio sobre varias empresas que son contratistas o proveedores de servicios de alto valor agregado de grandes compañías constructora. Se ha indagado el desempeño de unos 150 contratistas según cuatro dimensiones o indicadores del BSC.Estas dimensiones están relacionadas con:TecnologíaNormasCapitalEquipamientoLos datos han sido normalizados tode vez que deberemos comparar unidades de medidas muy disimiles. Por ejemplo la calidad de los recursos humanos tiene dimensión y el capital estaba expresado en dolares.","code":""},{"path":"clusterización.html","id":"carga-de-datos-desde-archivos-externos","chapter":"Capítulo 11 Clusterización","heading":"11.1.1 Carga de datos desde archivos externos","text":"Si bien sería posible cargar datos mano en R-CRAN, sería práctico hacer lo así.\nTodos los datos que se utilizan en este entorno de trabajo se manejan internamente como matrices. Estas matrices de datos (semejantes bases de datos) se llaman data-frames.Un data frame podría ser una matriz de una fila por una columna así por ejemploIntente carga dos dataframes y b con valores numéricos y realizamos la suma de ambos como se muestra en el código.Esto nos permitiría usar R-CRAN como si se tratase de una calculadora.","code":"  > X <-200 ;\n  > Y= 20 ;\n\n  > Y= X+Z"},{"path":"clusterización.html","id":"captura-de-datos","chapter":"Capítulo 11 Clusterización","heading":"11.2 Captura de datos","text":"Cargando datos desde la línea de comandoPara finalizar la carga tipee dos veces .","code":"Ejecute los comandos \"scan()\" y luego tipee los números separados por <Enter>."},{"path":"clusterización.html","id":"desde-dónde-podemos-cargar-datos","chapter":"Capítulo 11 Clusterización","heading":"11.3 ¿Desde dónde podemos cargar datos?","text":"Uno de los requisitos para poder cargar datos es que sepamos en que carpeta estamos trabajando.\nEn este ejercicio veremos como hacer para saber donde estamos parados.Puedes cambiar tu carpeta (directorio) de trabajo con el comando setwd()Debes poner entre comillas lo que va dentro del paréntesisPrepare un archivo en excel y guardelo con formato csv\n“comma separated values” para capturarlo desde R-CRAN.","code":"\ngetwd()\n#> [1] \"/media/rpalma/Datos/2025/Posgrado/Analitica de datos industriales/R-Cran/Libro_25/ADI_2nd_Edition\""},{"path":"clusterización.html","id":"revisión-de-datos-ejemplo","chapter":"Capítulo 11 Clusterización","heading":"11.3.1 Revisión de Datos Ejemplo","text":"Con los siguientes comando importaremos una planilla generada con una hoja de cálculo que contiene las 150 respuestas de las encuestas que se realizaronPodemos ver el contenido del dataframe BSC_Proveedores que se llama igual que el archivo de texto separado por comas. Para ello ejecutamos el siguiente comando.","code":"\nlibrary(readr)\n#BSC_proveedores <- read_csv(\"BSC_proveedores.csv\")\nBSC_proveedores <- read.csv(\"https://ricardorpalma.github.io/R-Dataset/BSC_proveedores.csv\")\nBSC_proveedores [c(1:5 ,70:73, 126:129) ,]\n#>       X Tecnologia Normas Capital Equipo       Empresa\n#> 1     1        5.1    3.5     1.4    0.2   CNB-Cerveza\n#> 2     2        4.9    3.0     1.4    0.2   CNB-Cerveza\n#> 3     3        4.7    3.2     1.3    0.2   CNB-Cerveza\n#> 4     4        4.6    3.1     1.5    0.2   CNB-Cerveza\n#> 5     5        5.0    3.6     1.4    0.2   CNB-Cerveza\n#> 70   70        5.6    2.5     3.9    1.1     FARMACORP\n#> 71   71        5.9    3.2     4.8    1.8     FARMACORP\n#> 72   72        6.1    2.8     4.0    1.3     FARMACORP\n#> 73   73        6.3    2.5     4.9    1.5     FARMACORP\n#> 126 126        7.2    3.2     6.0    1.8 SOFIA-Avicola\n#> 127 127        6.2    2.8     4.8    1.8 SOFIA-Avicola\n#> 128 128        6.1    3.0     4.9    1.8 SOFIA-Avicola\n#> 129 129        6.4    2.8     5.6    2.1 SOFIA-Avicola"},{"path":"clusterización.html","id":"análisis-exploratorio-1","chapter":"Capítulo 11 Clusterización","heading":"11.4 Análisis Exploratorio","text":"Si quisiésemos ver el desempeño respecto la variable TECNOLOGÍA tendríamos que interponer entre el nombre del dataset el signo pesos y luego el nombre de la columnaRepita todo el prceso con el resto de la dimensiones NORMA, CAPITAL, EQUIPO, EMPRESA.","code":"\nBSC_proveedores [c(1:5 ,70:73, 126:129) ,]\n#>       X Tecnologia Normas Capital Equipo       Empresa\n#> 1     1        5.1    3.5     1.4    0.2   CNB-Cerveza\n#> 2     2        4.9    3.0     1.4    0.2   CNB-Cerveza\n#> 3     3        4.7    3.2     1.3    0.2   CNB-Cerveza\n#> 4     4        4.6    3.1     1.5    0.2   CNB-Cerveza\n#> 5     5        5.0    3.6     1.4    0.2   CNB-Cerveza\n#> 70   70        5.6    2.5     3.9    1.1     FARMACORP\n#> 71   71        5.9    3.2     4.8    1.8     FARMACORP\n#> 72   72        6.1    2.8     4.0    1.3     FARMACORP\n#> 73   73        6.3    2.5     4.9    1.5     FARMACORP\n#> 126 126        7.2    3.2     6.0    1.8 SOFIA-Avicola\n#> 127 127        6.2    2.8     4.8    1.8 SOFIA-Avicola\n#> 128 128        6.1    3.0     4.9    1.8 SOFIA-Avicola\n#> 129 129        6.4    2.8     5.6    2.1 SOFIA-Avicola"},{"path":"clusterización.html","id":"historgramas-análisis-de-histograma","chapter":"Capítulo 11 Clusterización","heading":"11.5 Historgramas Análisis de Histograma","text":"Veremos como se comportan las muestras (contratistar) utilizando el histogramaEl gráfico nos muestra que hay dos grupos de contratistas (casi 29 ocurrencias en cada uno) con un desempeño de 6 en la variable TECNOLOGÍA . La escala original era de 1 10.Utilizaremos el comando par() que permite dividir el área de ploteo en una matriz especificada por el comando Numero de Columna (nmfrow)Haga las interpretaciones de estos gráficos.** Particionado del área de impresión","code":"\nhist(BSC_proveedores$Tecnologia)\npar(mfrow=c(2,2))\nhist(BSC_proveedores$Tecnologia)\nhist(BSC_proveedores$Tecnologia)\nhist(BSC_proveedores$Capital)\nhist(BSC_proveedores$Equipo)"},{"path":"clusterización.html","id":"gráficos-de-densidad-1","chapter":"Capítulo 11 Clusterización","heading":"11.6 Gráficos de Densidad","text":"Algunas personas prefieren utilizar la envolvente del histograma que es el gráfico de densidad.17<<Density,fig=TRUE>>=Algunas de estas gráficas ya nos muestran que existen ciertas diferencias entre las contratistas, es como si hubiese diferentes campanas de Gauss que agrupan las diferentes muestras.","code":"\n\npar(mfrow=c(1,1))\nplot(density(BSC_proveedores$Equipo))\npar(mfrow=c(2,2))\nplot(density(BSC_proveedores$Tecnologia))\nplot(density(BSC_proveedores$Normas))\nplot(density(BSC_proveedores$Capital))\nplot(density(BSC_proveedores$Equipo))"},{"path":"clusterización.html","id":"gráficas-ralas-y-análisis-multivariado","chapter":"Capítulo 11 Clusterización","heading":"11.7 Gráficas Ralas y Análisis Multivariado","text":"Este tipo de análisis multivariado nos permite construir una matriz de gráficas que en la diagonal principal nos muestra los ya conocidos gráficos de densidad.\nLuego cada una de las otras intersecciones nos señala si existe algún tipo de correlación montónica (creciente o decreciente) entre las variables analizadas.\nEsto es importante, pues priori sabemos si las dimensiones que estamos usando tienen o relación entre ellas. En otras palabras si las dimensiones o metas tienen correlación quiere decir que podríamos prescindir de una de ellas.\nAsí capital y equipo parecen priori tener alta linealidad en su correlación.18Notar también las líneas de puntos que nos marcan el intervalo de confianza que podríamos tener sobre esa variable. Es justamente ese margen el que nos perite establecer la franja de flotación que motiva el paso de verde amarillo. Si se desplazase tres veces la varianza estaríamos en rojo.","code":"\nlibrary(car)\n#> Loading required package: carData\nBSC_Rawdata <- BSC_proveedores[ ,c(2,3,4,5)]\nBSC_Rawdata\n#>     Tecnologia Normas Capital Equipo\n#> 1          5.1    3.5     1.4    0.2\n#> 2          4.9    3.0     1.4    0.2\n#> 3          4.7    3.2     1.3    0.2\n#> 4          4.6    3.1     1.5    0.2\n#> 5          5.0    3.6     1.4    0.2\n#> 6          5.4    3.9     1.7    0.4\n#> 7          4.6    3.4     1.4    0.3\n#> 8          5.0    3.4     1.5    0.2\n#> 9          4.4    2.9     1.4    0.2\n#> 10         4.9    3.1     1.5    0.1\n#> 11         5.4    3.7     1.5    0.2\n#> 12         4.8    3.4     1.6    0.2\n#> 13         4.8    3.0     1.4    0.1\n#> 14         4.3    3.0     1.1    0.1\n#> 15         5.8    4.0     1.2    0.2\n#> 16         5.7    4.4     1.5    0.4\n#> 17         5.4    3.9     1.3    0.4\n#> 18         5.1    3.5     1.4    0.3\n#> 19         5.7    3.8     1.7    0.3\n#> 20         5.1    3.8     1.5    0.3\n#> 21         5.4    3.4     1.7    0.2\n#> 22         5.1    3.7     1.5    0.4\n#> 23         4.6    3.6     1.0    0.2\n#> 24         5.1    3.3     1.7    0.5\n#> 25         4.8    3.4     1.9    0.2\n#> 26         5.0    3.0     1.6    0.2\n#> 27         5.0    3.4     1.6    0.4\n#> 28         5.2    3.5     1.5    0.2\n#> 29         5.2    3.4     1.4    0.2\n#> 30         4.7    3.2     1.6    0.2\n#> 31         4.8    3.1     1.6    0.2\n#> 32         5.4    3.4     1.5    0.4\n#> 33         5.2    4.1     1.5    0.1\n#> 34         5.5    4.2     1.4    0.2\n#> 35         4.9    3.1     1.5    0.2\n#> 36         5.0    3.2     1.2    0.2\n#> 37         5.5    3.5     1.3    0.2\n#> 38         4.9    3.6     1.4    0.1\n#> 39         4.4    3.0     1.3    0.2\n#> 40         5.1    3.4     1.5    0.2\n#> 41         5.0    3.5     1.3    0.3\n#> 42         4.5    2.3     1.3    0.3\n#> 43         4.4    3.2     1.3    0.2\n#> 44         5.0    3.5     1.6    0.6\n#> 45         5.1    3.8     1.9    0.4\n#> 46         4.8    3.0     1.4    0.3\n#> 47         5.1    3.8     1.6    0.2\n#> 48         4.6    3.2     1.4    0.2\n#> 49         5.3    3.7     1.5    0.2\n#> 50         5.0    3.3     1.4    0.2\n#> 51         7.0    3.2     4.7    1.4\n#> 52         6.4    3.2     4.5    1.5\n#> 53         6.9    3.1     4.9    1.5\n#> 54         5.5    2.3     4.0    1.3\n#> 55         6.5    2.8     4.6    1.5\n#> 56         5.7    2.8     4.5    1.3\n#> 57         6.3    3.3     4.7    1.6\n#> 58         4.9    2.4     3.3    1.0\n#> 59         6.6    2.9     4.6    1.3\n#> 60         5.2    2.7     3.9    1.4\n#> 61         5.0    2.0     3.5    1.0\n#> 62         5.9    3.0     4.2    1.5\n#> 63         6.0    2.2     4.0    1.0\n#> 64         6.1    2.9     4.7    1.4\n#> 65         5.6    2.9     3.6    1.3\n#> 66         6.7    3.1     4.4    1.4\n#> 67         5.6    3.0     4.5    1.5\n#> 68         5.8    2.7     4.1    1.0\n#> 69         6.2    2.2     4.5    1.5\n#> 70         5.6    2.5     3.9    1.1\n#> 71         5.9    3.2     4.8    1.8\n#> 72         6.1    2.8     4.0    1.3\n#> 73         6.3    2.5     4.9    1.5\n#> 74         6.1    2.8     4.7    1.2\n#> 75         6.4    2.9     4.3    1.3\n#> 76         6.6    3.0     4.4    1.4\n#> 77         6.8    2.8     4.8    1.4\n#> 78         6.7    3.0     5.0    1.7\n#> 79         6.0    2.9     4.5    1.5\n#> 80         5.7    2.6     3.5    1.0\n#> 81         5.5    2.4     3.8    1.1\n#> 82         5.5    2.4     3.7    1.0\n#> 83         5.8    2.7     3.9    1.2\n#> 84         6.0    2.7     5.1    1.6\n#> 85         5.4    3.0     4.5    1.5\n#> 86         6.0    3.4     4.5    1.6\n#> 87         6.7    3.1     4.7    1.5\n#> 88         6.3    2.3     4.4    1.3\n#> 89         5.6    3.0     4.1    1.3\n#> 90         5.5    2.5     4.0    1.3\n#> 91         5.5    2.6     4.4    1.2\n#> 92         6.1    3.0     4.6    1.4\n#> 93         5.8    2.6     4.0    1.2\n#> 94         5.0    2.3     3.3    1.0\n#> 95         5.6    2.7     4.2    1.3\n#> 96         5.7    3.0     4.2    1.2\n#> 97         5.7    2.9     4.2    1.3\n#> 98         6.2    2.9     4.3    1.3\n#> 99         5.1    2.5     3.0    1.1\n#> 100        5.7    2.8     4.1    1.3\n#> 101        6.3    3.3     6.0    2.5\n#> 102        5.8    2.7     5.1    1.9\n#> 103        7.1    3.0     5.9    2.1\n#> 104        6.3    2.9     5.6    1.8\n#> 105        6.5    3.0     5.8    2.2\n#> 106        7.6    3.0     6.6    2.1\n#> 107        4.9    2.5     4.5    1.7\n#> 108        7.3    2.9     6.3    1.8\n#> 109        6.7    2.5     5.8    1.8\n#> 110        7.2    3.6     6.1    2.5\n#> 111        6.5    3.2     5.1    2.0\n#> 112        6.4    2.7     5.3    1.9\n#> 113        6.8    3.0     5.5    2.1\n#> 114        5.7    2.5     5.0    2.0\n#> 115        5.8    2.8     5.1    2.4\n#> 116        6.4    3.2     5.3    2.3\n#> 117        6.5    3.0     5.5    1.8\n#> 118        7.7    3.8     6.7    2.2\n#> 119        7.7    2.6     6.9    2.3\n#> 120        6.0    2.2     5.0    1.5\n#> 121        6.9    3.2     5.7    2.3\n#> 122        5.6    2.8     4.9    2.0\n#> 123        7.7    2.8     6.7    2.0\n#> 124        6.3    2.7     4.9    1.8\n#> 125        6.7    3.3     5.7    2.1\n#> 126        7.2    3.2     6.0    1.8\n#> 127        6.2    2.8     4.8    1.8\n#> 128        6.1    3.0     4.9    1.8\n#> 129        6.4    2.8     5.6    2.1\n#> 130        7.2    3.0     5.8    1.6\n#> 131        7.4    2.8     6.1    1.9\n#> 132        7.9    3.8     6.4    2.0\n#> 133        6.4    2.8     5.6    2.2\n#> 134        6.3    2.8     5.1    1.5\n#> 135        6.1    2.6     5.6    1.4\n#> 136        7.7    3.0     6.1    2.3\n#> 137        6.3    3.4     5.6    2.4\n#> 138        6.4    3.1     5.5    1.8\n#> 139        6.0    3.0     4.8    1.8\n#> 140        6.9    3.1     5.4    2.1\n#> 141        6.7    3.1     5.6    2.4\n#> 142        6.9    3.1     5.1    2.3\n#> 143        5.8    2.7     5.1    1.9\n#> 144        6.8    3.2     5.9    2.3\n#> 145        6.7    3.3     5.7    2.5\n#> 146        6.7    3.0     5.2    2.3\n#> 147        6.3    2.5     5.0    1.9\n#> 148        6.5    3.0     5.2    2.0\n#> 149        6.2    3.4     5.4    2.3\n#> 150        5.9    3.0     5.1    1.8\nscatterplotMatrix(BSC_Rawdata)"},{"path":"clusterización.html","id":"mínimo-numero-de-dimensiones","chapter":"Capítulo 11 Clusterización","heading":"11.8 Mínimo numero de dimensiones","text":"Cuándo nos enfrentamos situaciones como esta, suele ocurrir que al definir los indicadores nos encontramos con el dilema del gran volumen de datos. Esto es un problema que provenga tan solo del número de casos que estudiamos con el objeto de conocer el recorrido de una variable, sino más bien por la gran cantidad de variables o calificadores con los que los definimos o estudiamos.\nYa vimos en el caso anterior como dimensiones o variables que tienen distinto nombre son en realidad más que la misma cosa.\nEn el ejemplo anterior la pregunta era si podríamos prescindir de una variable. En este ejercicio trataremos de ver cuantas podemos eliminar. La consigna es Mientras menos variables mejor, y la restricción que impondremos será la de perder variables siempre que podamos seguir describiendo con alto nivel de confianza el comportamiento de todos los casos. Otra mirada sobre el problema podría enunciarse así. “Como puedo saber que valores o recorrido le impondría la mínima cantidad de variables para calificar como candidato interesante en la nómina de contratistas de las grandes empresas constructoras”.19Para auxilio en este problema utilizaremos el Método de Análisis de Componentes Principales. En este caso y al igual que en el caso anterior usaré un subconjunto de datos (sólo los numéricos) y en especial la matriz de correlación. Esta matriz está armada con las pendientes de las aproximaciones lineales de las rectas del gráfico de densidades.Las técnicas que usaremos pretenden desde sus diferentes enfoques abrodar el problema de simplificar la interpretación del comportamiento individual y colectivo de los casos (empresas constructoras y contratista) y como podemos valernos del proceso de ingeniería inversa para mover los controles de nuestra “nave” en el tablero de comando con el que fijaremos la altura de la vara del tablero de control.","code":""},{"path":"clusterización.html","id":"análisis-de-componentes-principales","chapter":"Capítulo 11 Clusterización","heading":"11.9 Análisis de Componentes Principales","text":"Crearemos un objeto nuevo que se llamará PC1 (por Principal Component 1) y la instrucción con el que crearemos la matriz de correlaciones es princomp.20En el ploteo podemos ver que uno de los componentes principales aporta casi el 4 veces más de la información referida al comportamiento de la varianza de todos los casos. Este componente es el que más incluye en la clasificación o posible identificación del comportamiento de cada individuo de la muestra.<<sumario_pc1,echo=TRUE>>=Si observamos bien el reporte que nos entrega el comando summary nos podemos dar cuenta que con los dos primeros componentes podríamos explicar 97.768521% del comportamiento de las muestras de la población. En nuestro caso del total de empresas contratistas analizadas.¿Qué pasaría si representamos las empresas en un gŕafico en el que las variables de los ejes sean los dos componentes principales? , pues tendríamos un primer indicio de la bondad de las dimensiones o variables para agrupar las muestras\nEsto lo podemos realizar con el comando bilot.21Los ńumeros que aparecen el el diagrama son el caso de estudio (renglón en que se encuentra la empresa contratista).\nsimple vista observamos que hay como tres tipos distintos empresas (tres nubes claramente diferenciadas). Aquí nos queda claro que el principal componente que ordena o divide estas colonias es indistintamente el CAPITAL o el EQUIPAMIENTO con que cuentas.También podemos ver que hay empresas como la 15, 16, 132, 118, 61, 107 sobre las que el gráfico nos recomienda estudiarlas más pues es capaz de clasificarlas bien (son casos extremos o anómalos). Tal vez con poco capital o sin equipo pueden llegar ser competitivas o interesantes para las grandes constructoras.22Por último la dimensión referida la cetificación de NORMAS es la dimensión que menos valor aporta. Esto implica que certificar sea poco importante, sino que probablemente sea una pregunta irrelevante si todos contestaron que SI certificaron ISO 9000.","code":"\nPC1 <- princomp(BSC_Rawdata)\nPC1\n#> Call:\n#> princomp(x = BSC_Rawdata)\n#> \n#> Standard deviations:\n#>    Comp.1    Comp.2    Comp.3    Comp.4 \n#> 2.0494032 0.4909714 0.2787259 0.1538707 \n#> \n#>  4  variables and  150 observations.\nplot(PC1)\nsummary(PC1)\n#> Importance of components:\n#>                           Comp.1     Comp.2     Comp.3\n#> Standard deviation     2.0494032 0.49097143 0.27872586\n#> Proportion of Variance 0.9246187 0.05306648 0.01710261\n#> Cumulative Proportion  0.9246187 0.97768521 0.99478782\n#>                             Comp.4\n#> Standard deviation     0.153870700\n#> Proportion of Variance 0.005212184\n#> Cumulative Proportion  1.000000000\nbiplot(PC1)"},{"path":"clusterización.html","id":"scores","chapter":"Capítulo 11 Clusterización","heading":"11.10 Scores","text":"Si el comportamiento del componente va hacia el lado positivo, se debe interpretar como que mayor desempeño mejor resultado o calificación. Si algún componente apunta para el lado negativo tendremos que pensar que mayor calificación en esa dimensión pero sería el desempeño.\nLa variable PC1 que usamos tiene mucha información valiosa.\nRevise todo el contenido, voy mostrar una dimensión que es el score que indica como se comportarían todos los individuos si sólo los analizásemos con los componentes 1 y 2.Voy realizar el mismo score pero ahora solo con los componentes 1 y 2Aquí ya podemos ver más claramente la división que se produce entre distintos clusters. Para poder diferencias aún más recurriremos un nuevo tipo de análisis diferenciado que se llama análisis de clusters","code":"\nacp1 <- PC1$scores\nacp1 [1:10 , ]\n#>          Comp.1      Comp.2      Comp.3       Comp.4\n#>  [1,] -2.684126  0.31939725  0.02791483  0.002262437\n#>  [2,] -2.714142 -0.17700123  0.21046427  0.099026550\n#>  [3,] -2.888991 -0.14494943 -0.01790026  0.019968390\n#>  [4,] -2.745343 -0.31829898 -0.03155937 -0.075575817\n#>  [5,] -2.728717  0.32675451 -0.09007924 -0.061258593\n#>  [6,] -2.280860  0.74133045 -0.16867766 -0.024200858\n#>  [7,] -2.820538 -0.08946138 -0.25789216 -0.048143106\n#>  [8,] -2.626145  0.16338496  0.02187932 -0.045297871\n#>  [9,] -2.886383 -0.57831175 -0.02075957 -0.026744736\n#> [10,] -2.672756 -0.11377425  0.19763272 -0.056295401\nacp2 <-PC1$scores[ ,1:2]\nplot(acp2)"},{"path":"clusterización.html","id":"análisis-de-clusters-o-conglomerados","chapter":"Capítulo 11 Clusterización","heading":"11.11 Análisis de Clusters o Conglomerados","text":"Para realizar este análisis recurriremos cargar la biblioteca clustersEn el análisis de conglomerados existen dos formas clásicas de estudio. Ambas recurren las distancias euclídeas entre las muestras. Tenemos aproximaciones Jerárquicas y Jerárquicas\nAGNES, CLARA, DIANA, MORA, PAM son nombres de las técnicas que la biblioteca Clusters usa. Todas las técnicas se caracterizan por ser un acrónimo de la combinación de aproximaciones que usan (Single Linkage, Complete Linkage, Average Linkage) .Todas tienen nombre de mujer, pero esto quiere necesariamente decir que se trate de una técnica con complicaciones inesperadas, sino más bien que si quieres lo mejor de una de ellas es mejor que la entiendas e indagues en la página del manual.Con la clasificacion terminada procederemos ver gráficamente el resultado.Pasa asignar las muestras grupos usaré el comendo cuttree que permite valerme de las franjas blancas de corte del los gráficos para armar los clusters","code":"\nlibrary(cluster)\nagp1 = agnes(acp2,method=\"single\")\nagp2 = agnes(acp2,method=\"complete\")\nagp3 = agnes(acp2,method=\"average\")\npar(mfrow=c(2,2))\nplot(acp2)\nplot(agp1)\nplot(agp2)\nplot(agp3)\nagpcut <- cutree(agp3,3)\npar(mfrow=c(1,1))\nplot(acp2,col=agpcut)"},{"path":"clusterización.html","id":"otros-gráficos-de-agrupamiento","chapter":"Capítulo 11 Clusterización","heading":"11.11.1 Otros gráficos de agrupamiento","text":"Método Clara","code":"\nplot(clara(BSC_Rawdata,5))"},{"path":"clusterización.html","id":"método-melisa","chapter":"Capítulo 11 Clusterización","heading":"11.12 Método Melisa","text":"","code":"\nplot(diana(BSC_Rawdata),ask = FALSE)"},{"path":"árboles-de-decisión.html","id":"árboles-de-decisión","chapter":"Capítulo 12 Árboles de decisión","heading":"Capítulo 12 Árboles de decisión","text":"Los árboles de decisión son modelos de aprendizaje automático que se utilizan para resolver problemas de clasificación y regresión. Se basan en una estructura similar un árbol, donde cada nodo representa una pregunta o una condición sobre una característica de los datos. medida que se desciende por el árbol, se toman decisiones basadas en las respuestas las preguntas, hasta llegar una hoja que representa la predicción final.En un árbol de decisión, los datos se dividen en diferentes ramas en función de las características y los valores de las mismas. Estas divisiones se realizan de manera que se maximice la pureza o la homogeneidad de los subconjuntos resultantes. En el caso de problemas de clasificación, los subconjuntos se dividen de forma que se agrupen las instancias con la misma clase, mientras que en problemas de regresión, se busca reducir la varianza de los valores objetivo.Se trata de un proceso de aprendizaje partir de datos y se dice que es supervisado. Necesita datos con etiquetas.El proceso de construcción de un árbol de decisión implica varios pasos, que incluyen la selección de características, la definición de criterios de división y la poda del árbol para evitar el sobreajuste. Algunos algoritmos populares para la construcción de árboles de decisión son el algoritmo ID3, C4.5, CART y Random Forest.Los árboles de decisión tienen varias ventajas, como la interpretabilidad y la capacidad de manejar tanto datos numéricos como categóricos. Además, pueden capturar relaciones lineales entre las características y la variable objetivo. Sin embargo, también tienen limitaciones, como la tendencia al sobreajuste y la sensibilidad pequeñas variaciones en los datos de entrenamiento.En resumen, los árboles de decisión son modelos versátiles y ampliamente utilizados en machine learning debido su capacidad para resolver problemas de clasificación y regresión. Su estructura jerárquica basada en preguntas y decisiones los hace fáciles de interpretar y explicar, lo que los convierte en una herramienta valiosa para tomar decisiones basadas en datos en diferentes dominios.En esta parte veremos como trabajar con el entrenamiento de árboles para luego comprender mejor el algoritmo ramdom-forest.","code":""},{"path":"árboles-de-decisión.html","id":"bibliografía-recomendada-4","chapter":"Capítulo 12 Árboles de decisión","heading":"12.1 Bibliografía recomendada","text":"","code":""},{"path":"árboles-de-decisión.html","id":"carga-de-datos-desde-la-web","chapter":"Capítulo 12 Árboles de decisión","heading":"12.2 Carga de datos desde la web","text":"","code":"\nlibrary(car)\n#> Loading required package: carData\nlibrary(readr)\npartners <- read_csv(\"https://ricardorpalma.github.io/R-Dataset/BSC_proveedores.csv\", \n    col_types = cols(Empresa = col_factor(levels = c(\"CNB-Cerveza\", \n        \"FARMACORP\", \"SOFIA-Avicola\"))))\n#> New names:\n#> • `` -> `...1`\n# partners <- read.table(\"BSC_proveedores.csv\",header=TRUE,sep=\",\")\nsummary(partners)\n#>       ...1          Tecnologia        Normas     \n#>  Min.   :  1.00   Min.   :4.300   Min.   :2.000  \n#>  1st Qu.: 38.25   1st Qu.:5.100   1st Qu.:2.800  \n#>  Median : 75.50   Median :5.800   Median :3.000  \n#>  Mean   : 75.50   Mean   :5.843   Mean   :3.057  \n#>  3rd Qu.:112.75   3rd Qu.:6.400   3rd Qu.:3.300  \n#>  Max.   :150.00   Max.   :7.900   Max.   :4.400  \n#>     Capital          Equipo               Empresa  \n#>  Min.   :1.000   Min.   :0.100   CNB-Cerveza  :50  \n#>  1st Qu.:1.600   1st Qu.:0.300   FARMACORP    :50  \n#>  Median :4.350   Median :1.300   SOFIA-Avicola:50  \n#>  Mean   :3.758   Mean   :1.199                     \n#>  3rd Qu.:5.100   3rd Qu.:1.800                     \n#>  Max.   :6.900   Max.   :2.500"},{"path":"árboles-de-decisión.html","id":"matriz-de-covarianza","chapter":"Capítulo 12 Árboles de decisión","heading":"12.3 Matriz de Covarianza","text":"Como vemos la columna 1, PK, (primary key) es parte de los datos. Se trata de un número secuencial que está relacionado con la muestra.","code":"\nlibrary(scatterPlotMatrix)\nscatterPlotMatrix(partners)"},{"path":"árboles-de-decisión.html","id":"entrenamiento-de-árbol-de-decisión","chapter":"Capítulo 12 Árboles de decisión","heading":"12.4 Entrenamiento de árbol de decisión","text":"Esta técnica utiliza un set de datos representativos de una situaci'y utilizando recursivamente el teoréma de Bayes puede armar un pronosticador o clasificador de datos. Es una t'ecnica parecida la de clustering, pero m'refinada, pues se basa en reglas sino en parendizaje del set de datos usado como entrenamento. En el paquete party existen dos funciones ctree que se utiliza para entrenar y predict que se usa para pronosticar o generar la regla de decici'que debemos usar.Impresión del Árbol de Decisión","code":"\nlibrary(party)\n#> Loading required package: grid\n#> Loading required package: mvtnorm\n#> Loading required package: modeltools\n#> Loading required package: stats4\n#> \n#> Attaching package: 'modeltools'\n#> The following object is masked from 'package:car':\n#> \n#>     Predict\n#> Loading required package: strucchange\n#> Loading required package: zoo\n#> \n#> Attaching package: 'zoo'\n#> The following objects are masked from 'package:base':\n#> \n#>     as.Date, as.Date.numeric\n#> Loading required package: sandwich\nattach(partners)\nstr(partners)    \n#> spc_tbl_ [150 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#>  $ ...1      : num [1:150] 1 2 3 4 5 6 7 8 9 10 ...\n#>  $ Tecnologia: num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n#>  $ Normas    : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n#>  $ Capital   : num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n#>  $ Equipo    : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n#>  $ Empresa   : Factor w/ 3 levels \"CNB-Cerveza\",..: 1 1 1 1 1 1 1 1 1 1 ...\n#>  - attr(*, \"spec\")=\n#>   .. cols(\n#>   ..   ...1 = col_double(),\n#>   ..   Tecnologia = col_double(),\n#>   ..   Normas = col_double(),\n#>   ..   Capital = col_double(),\n#>   ..   Equipo = col_double(),\n#>   ..   Empresa = col_factor(levels = c(\"CNB-Cerveza\", \"FARMACORP\", \"SOFIA-Avicola\"), ordered = FALSE, include_na = FALSE)\n#>   .. )\n#>  - attr(*, \"problems\")=<externalptr>\n# describe al objeto transit y muestras las columna que tiene\n\nind <- sample(2, nrow(partners), replace=TRUE, prob=c(0.7, 0.3))  \n# toma una muestra  \nind  \n#>   [1] 1 2 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 2 1 2 1 1\n#>  [28] 1 2 1 1 2 2 1 1 2 1 1 2 1 1 1 1 1 1 2 1 1 2 1 2 1 1 1\n#>  [55] 1 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 2\n#>  [82] 1 2 1 2 1 1 1 1 2 1 1 1 2 1 1 1 1 2 1 2 2 1 2 1 1 1 1\n#> [109] 1 1 1 2 1 1 2 1 1 2 1 1 2 1 2 2 2 1 1 1 2 1 1 1 1 2 1\n#> [136] 1 1 2 1 1 1 1 1 2 2 2 1 1 1 2\n# nos imprime la muestra tomada.\ntrainData <- partners [ind==1,]    \n# genero un set de entrenamiento \ntestData <- partners [ind==2,]    \n# genero un set de datos de prueba\nmyFormula <- Empresa ~ Normas + Tecnologia + Capital + Equipo \ntransit_ctree <- ctree(myFormula, data=trainData)    \n# creo el motor de entrenamiento\n# Verificar las prediciones \ntable(predict(transit_ctree), trainData$Empresa) \n#>                \n#>                 CNB-Cerveza FARMACORP SOFIA-Avicola\n#>   CNB-Cerveza            36         0             0\n#>   FARMACORP               0        37             3\n#>   SOFIA-Avicola           0         1            30\nprint(transit_ctree) \n#> \n#>   Conditional inference tree with 4 terminal nodes\n#> \n#> Response:  Empresa \n#> Inputs:  Normas, Tecnologia, Capital, Equipo \n#> Number of observations:  107 \n#> \n#> 1) Capital <= 1.9; criterion = 1, statistic = 100.115\n#>   2)*  weights = 36 \n#> 1) Capital > 1.9\n#>   3) Equipo <= 1.6; criterion = 1, statistic = 48.21\n#>     4) Capital <= 4.6; criterion = 1, statistic = 14.72\n#>       5)*  weights = 31 \n#>     4) Capital > 4.6\n#>       6)*  weights = 9 \n#>   3) Equipo > 1.6\n#>     7)*  weights = 31\nlibrary(party)\nattach(partners)\n#> The following objects are masked from partners (pos = 3):\n#> \n#>     ...1, Capital, Empresa, Equipo, Normas,\n#>     Tecnologia\nstr(partners)    \n#> spc_tbl_ [150 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#>  $ ...1      : num [1:150] 1 2 3 4 5 6 7 8 9 10 ...\n#>  $ Tecnologia: num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n#>  $ Normas    : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n#>  $ Capital   : num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n#>  $ Equipo    : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n#>  $ Empresa   : Factor w/ 3 levels \"CNB-Cerveza\",..: 1 1 1 1 1 1 1 1 1 1 ...\n#>  - attr(*, \"spec\")=\n#>   .. cols(\n#>   ..   ...1 = col_double(),\n#>   ..   Tecnologia = col_double(),\n#>   ..   Normas = col_double(),\n#>   ..   Capital = col_double(),\n#>   ..   Equipo = col_double(),\n#>   ..   Empresa = col_factor(levels = c(\"CNB-Cerveza\", \"FARMACORP\", \"SOFIA-Avicola\"), ordered = FALSE, include_na = FALSE)\n#>   .. )\n#>  - attr(*, \"problems\")=<externalptr>\n# describe al objeto transit\nind <- sample(2, nrow(partners), replace=TRUE, prob=c(0.7, 0.3))  \n# toma una muestra \nind  \n#>   [1] 2 1 1 1 2 2 2 1 1 1 1 1 1 2 2 2 1 2 1 2 2 1 1 1 1 1 2\n#>  [28] 1 1 1 1 1 1 1 2 1 1 2 1 1 1 1 2 1 2 1 1 2 1 1 2 1 2 2\n#>  [55] 1 1 1 1 2 1 1 1 1 1 2 1 1 2 2 2 1 2 1 1 1 1 1 1 1 2 1\n#>  [82] 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 2 1 1 1\n#> [109] 2 1 1 2 1 1 1 1 1 1 1 2 1 2 1 1 1 1 2 1 1 1 2 1 1 2 2\n#> [136] 1 1 1 2 1 1 2 2 1 1 1 1 1 1 1\n# nos imprime la muestra tomada.\ntable(predict(transit_ctree), trainData$Empresa) \n#>                \n#>                 CNB-Cerveza FARMACORP SOFIA-Avicola\n#>   CNB-Cerveza            36         0             0\n#>   FARMACORP               0        37             3\n#>   SOFIA-Avicola           0         1            30\nplot(transit_ctree, las=2)\nsummary(trainData$Empresa)\n#>   CNB-Cerveza     FARMACORP SOFIA-Avicola \n#>            36            38            33"},{"path":"random-forest.html","id":"random-forest","chapter":"Capítulo 13 Random Forest","heading":"Capítulo 13 Random Forest","text":"Es una técnica que utiliza un conjunto de árboles predictores.\nCada árbol depende de un vector randómicamiente generado y con árboles que tienen entre sí como factor común una misma estructura (Breiman 2001.1). Su uso implica construir múltiples árboles de decisión que conforman un bosque o colección de árboles. Cada bosque su vez utiliza un técnica de votación para hacer la clasificación y la solución más botada es la que se toma por cierta o verdadera.Utilizaremos la función ramdonForest() de la biblioteca que lleva el mismo nombre para construir el modelo del bosque. Especificaremos que en el bosque hay 500 árboles que serán informados en el parámetro ntree que le pasaremos la función.Nota: La salida de la función nos proporcionará una muestra “--bag” más conocida como OOB como resultado, que es una muestra reforzada aproximada del conjunto de datos de entrenamiento.23","code":""},{"path":"random-forest.html","id":"bibliografía-recomendada-5","chapter":"Capítulo 13 Random Forest","heading":"13.1 Bibliografía recomendada","text":"Creación del conjunto de datosSeparación de índicesPodemos observar una representación multidimensional escalada de la proximidad de los conjuntos clasificados según la muestra o MDS.Podemos también ver las variables que más peso tienen en la clasificación del randmoforest construyendo un gráfica de “promedio de exactitud decreciente”\no Average Decrease Accuracy (ADC) y otro gráfico semejante denominado “promedio decreciente de nodo de impuridad” o Mean Decrease Node Impurity  esta técnica coincide con el análisis del índice Gini. Ambos gráficos deben producir","code":"\nlibrary(randomForest)\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\nlibrary(caret) # Tienen la función de partición \n#> Loading required package: ggplot2\n#> \n#> Attaching package: 'ggplot2'\n#> The following object is masked from 'package:randomForest':\n#> \n#>     margin\n#> Loading required package: lattice\nin.Train.C <- createDataPartition (iris$Species, p=0.7 , list=FALSE)\ntraining <- iris[in.Train.C, ]\ntest <- iris[-in.Train.C, ]\nset.seed(831)\nrf.mod.C <- randomForest(Species~., data= training, importance=TRUE, proximity=TRUE, ntree=500 )\nrf.mod.C\n#> \n#> Call:\n#>  randomForest(formula = Species ~ ., data = training, importance = TRUE,      proximity = TRUE, ntree = 500) \n#>                Type of random forest: classification\n#>                      Number of trees: 500\n#> No. of variables tried at each split: 2\n#> \n#>         OOB estimate of  error rate: 3.81%\n#> Confusion matrix:\n#>            setosa versicolor virginica class.error\n#> setosa         35          0         0  0.00000000\n#> versicolor      0         33         2  0.05714286\n#> virginica       0          2        33  0.05714286\nMDSplot(rf.mod.C, training$Species, xlab=\"CP1\", ylab=\"CP2\", main=\"MDS basada en randmo forest\") \nlegend(\"bottomleft\", title = \"Especies\", c(\"setosa\",\"versicolor\",\"virginica\"), fill=c(\"red\",\"green\",\"blue\"), horiz=TRUE, cex=0.45)\nvarImpPlot(rf.mod.C, main=\"Grafico de Importancia Relativa\")"},{"path":"random-forest.html","id":"medición-de-la-preformance","chapter":"Capítulo 13 Random Forest","heading":"13.2 Medición de la preformance","text":"Cada vez que implementamos un método lo indicado es proceder evaluar que tan bueno son los resultados que podemos obtener y esto podemos hacerlo con el ser de entrenamiento o con el de verificación.24","code":"\nrf.train <- predict(rf.mod.C, training[ ,1:4], type = \"class\")\nrf.train.confmat <- confusionMatrix(rf.train, training$Species, mode=\"prec_recall\")\nrf.train.confmat\n#> Confusion Matrix and Statistics\n#> \n#>             Reference\n#> Prediction   setosa versicolor virginica\n#>   setosa         35          0         0\n#>   versicolor      0         35         0\n#>   virginica       0          0        35\n#> \n#> Overall Statistics\n#>                                      \n#>                Accuracy : 1          \n#>                  95% CI : (0.9655, 1)\n#>     No Information Rate : 0.3333     \n#>     P-Value [Acc > NIR] : < 2.2e-16  \n#>                                      \n#>                   Kappa : 1          \n#>                                      \n#>  Mcnemar's Test P-Value : NA         \n#> \n#> Statistics by Class:\n#> \n#>                      Class: setosa Class: versicolor\n#> Precision                   1.0000            1.0000\n#> Recall                      1.0000            1.0000\n#> F1                          1.0000            1.0000\n#> Prevalence                  0.3333            0.3333\n#> Detection Rate              0.3333            0.3333\n#> Detection Prevalence        0.3333            0.3333\n#> Balanced Accuracy           1.0000            1.0000\n#>                      Class: virginica\n#> Precision                      1.0000\n#> Recall                         1.0000\n#> F1                             1.0000\n#> Prevalence                     0.3333\n#> Detection Rate                 0.3333\n#> Detection Prevalence           0.3333\n#> Balanced Accuracy              1.0000"},{"path":"random-forest.html","id":"prueba-de-performance-utilizando-dataset-de-prueba","chapter":"Capítulo 13 Random Forest","heading":"13.3 Prueba de performance utilizando dataset de prueba","text":"Al utilizar el dataset de prueba y tener las columna previamente etiquetadas podemos medir la eficacia del algoritmo utilizado","code":"\nrf.test <- predict(rf.mod.C, test[ ,1:4], type = \"class\")\nrf.test.confmat <- confusionMatrix(rf.test, test$Species, mode=\"prec_recall\")\nrf.test.confmat\n#> Confusion Matrix and Statistics\n#> \n#>             Reference\n#> Prediction   setosa versicolor virginica\n#>   setosa         15          0         0\n#>   versicolor      0         14         3\n#>   virginica       0          1        12\n#> \n#> Overall Statistics\n#>                                           \n#>                Accuracy : 0.9111          \n#>                  95% CI : (0.7878, 0.9752)\n#>     No Information Rate : 0.3333          \n#>     P-Value [Acc > NIR] : 8.467e-16       \n#>                                           \n#>                   Kappa : 0.8667          \n#>                                           \n#>  Mcnemar's Test P-Value : NA              \n#> \n#> Statistics by Class:\n#> \n#>                      Class: setosa Class: versicolor\n#> Precision                   1.0000            0.8235\n#> Recall                      1.0000            0.9333\n#> F1                          1.0000            0.8750\n#> Prevalence                  0.3333            0.3333\n#> Detection Rate              0.3333            0.3111\n#> Detection Prevalence        0.3333            0.3778\n#> Balanced Accuracy           1.0000            0.9167\n#>                      Class: virginica\n#> Precision                      0.9231\n#> Recall                         0.8000\n#> F1                             0.8571\n#> Prevalence                     0.3333\n#> Detection Rate                 0.2667\n#> Detection Prevalence           0.2889\n#> Balanced Accuracy              0.8833"},{"path":"random-forest.html","id":"ajuste-del-modelo-1","chapter":"Capítulo 13 Random Forest","heading":"13.4 Ajuste del modelo","text":"Comparando los resultados anteriores podemos estimar la exactitud del modelo (Accuracy).Como vemos la exactitud con el set de entrenamiento es del 100% y con el de prueba es bastante aceptable 97%.25","code":"\nrbind(rf.train.confmat$overall[1], rf.test.confmat$overall[1])\n#>       Accuracy\n#> [1,] 1.0000000\n#> [2,] 0.9111111"},{"path":"random-forest.html","id":"buenas-prácticas-en-randomforest","chapter":"Capítulo 13 Random Forest","heading":"13.5 Buenas prácticas en RandomForest","text":"","code":""},{"path":"random-forest.html","id":"ajuste-del-número-de-árboles-en-el-bosque","chapter":"Capítulo 13 Random Forest","heading":"13.5.1 Ajuste del número de árboles en el bosque","text":"Deberían existir suficiente árboles como para garantizar la estabilidad de la soluciones, pero tanto como para que el costo computacional crezca más allá de la capacidad de la computadora que utilizas. Te darás cuenta si estás utilizando muchos árboles porque llegarás al resultado y tendrás la pantalla azul que aparece cuando Windows se cuelga.\nPuede ayudarte el gráfico de abajo que muestre la nivel de clase de error, que se estabiliza para este ejemplo con 100 árboles. Luego cualquier modelo semejante que tenga resultados mejores debería utilizar más árboles que estos.26\\[ mtry = \\sqrt {n_{features}} \\]Por defecto **mtry* tomará el valor entero más próximo la raíz cuadrada de la cantidad de columnas que tenga el dataset.","code":"    * number of trees **(ntree)**\nplot(rf.mod.C, main = \"Class-Leve Error vs Número de Árboles\")  * Ajuste de variables aleatoriamente muestreadas \nset.seed(831)\ntuneR <- tuneRF(x=training[ ,1:4], y=training$Species, ntreeTry = 500, stepFactor = 1.5)\n#> mtry = 2  OOB error = 4.76% \n#> Searching left ...\n#> Searching right ...\n#> mtry = 3     OOB error = 3.81% \n#> 0.2 0.05 \n#> mtry = 4     OOB error = 3.81% \n#> 0 0.05"},{"path":"bibliografía.html","id":"bibliografía","chapter":"Bibliografía","heading":"Bibliografía","text":"nocite: ‘27’","code":""}]
